<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Instant Papers</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="Customize your instant paper reading experience">
<meta property="og:type" content="website">
<meta property="og:title" content="Instant Papers">
<meta property="og:url" content="https://fan-jj24.github.io/">
<meta property="og:site_name" content="Instant Papers">
<meta property="og:description" content="Customize your instant paper reading experience">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Pop Fan">
<meta property="article:tag" content="Papers, Instant, Customize, Reading">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Instant Papers" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/fork-awesome@1.2.0/css/fork-awesome.min.css">

<meta name="generator" content="Hexo 8.0.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Instant Papers</a>
      </h1>
      
        <h2 id="subtitle-wrap">
          <a href="/" id="subtitle">Publishing Academic Papers as Newspapers</a>
        </h2>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"><span class="fa fa-bars"></span></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
        
          <a class="nav-icon" href="/atom.xml" title="RSS Feed"><span class="fa fa-rss"></span></a>
        
        <a class="nav-icon nav-search-btn" title="Search"><span class="fa fa-search"></span></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://fan-jj24.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-2025-10-22" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2025/10/23/2025-10-22/" class="article-date">
  <time class="dt-published" datetime="2025-10-23T09:11:58.725Z" itemprop="datePublished">2025-10-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2025/10/23/2025-10-22/">2025-10-22</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Total Papers Searched: 202<br>Total Accepted Papers: 69</p>
<p>Please note that I only want to read the papers in the specified category. If you need other categories, please contact me.<br>Papers that do not meet the requirements will be rejected and displayed at the end of the text.</p>
<h2 id="Alternative-category"><a href="#Alternative-category" class="headerlink" title="Alternative category"></a>Alternative category</h2><p>Reinforcement Learning, Robotics, Vision-Language Models, World Models</p>
<h2 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h2><h3 id="Scaf-GRPO-Scaffolded-Group-Relative-Policy-Optimization-for-Enhancing-LLM-Reasoning"><a href="#Scaf-GRPO-Scaffolded-Group-Relative-Policy-Optimization-for-Enhancing-LLM-Reasoning" class="headerlink" title="Scaf-GRPO: Scaffolded Group Relative Policy Optimization for Enhancing LLM Reasoning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19807v1">Scaf-GRPO: Scaffolded Group Relative Policy Optimization for Enhancing LLM Reasoning</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>基于可验证奖励的强化学习已成为增强大型语言模型（LLM）复杂推理能力的一种强大技术。然而，这些方法本质上受到“学习悬崖”现象的限制：当面临远超当前能力范围的问题时，模型会持续失败，产生持续的零奖励信号。在像GRPO这样的策略优化算法中，这会使得优势计算坍缩为零，使这些困难问题在学习梯度中变得不可见，从而阻碍了训练进展。为了解决这一问题，我们引入了Scaf-GRPO（分层引导组相对策略优化），这是一种渐进式训练框架，只有在模型独立学习达到瓶颈时，才战略性地提供最小限度的引导。该框架首先诊断学习停滞，然后通过注入从抽象概念到具体步骤的多层次提示信息进行干预，使模型能够自行构建有效的解决方案。在具有挑战性的数学基准测试中进行的大量实验表明，Scaf-GRPO是有效的。它在AIME24基准测试中将Qwen2.5-Math-7B模型的pass@1分数比普通GRPO基线提高了44.3%。这一结果表明，我们的框架为解锁模型解决此前无法处理的问题的能力提供了一种稳健且有效的方法，这是拓展LLM自主推理能力前沿的关键一步。</p>
<h3 id="Provably-Efficient-Reward-Transfer-in-Reinforcement-Learning-with-Discrete-Markov-Decision-Processes"><a href="#Provably-Efficient-Reward-Transfer-in-Reinforcement-Learning-with-Discrete-Markov-Decision-Processes" class="headerlink" title="Provably Efficient Reward Transfer in Reinforcement Learning with Discrete Markov Decision Processes"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2503.13414v3">Provably Efficient Reward Transfer in Reinforcement Learning with Discrete Markov Decision Processes</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>在本文中，我们提出了一种新的奖励适应（RA）方法，用于强化学习。该方法中，智能体根据在相同领域动态下但不同奖励函数下预先学习到的一个或多个源行为，适应于目标奖励函数。虽然从头开始学习目标行为是可能的，但考虑到已有源行为，这种方法通常效率低下。我们的工作通过操作Q函数引入了一种新的RA方法。假设目标奖励函数是源奖励函数的已知函数，我们计算Q函数的界限，并提出一种迭代过程（类似于价值迭代），以收紧这些界限。这些界限使得在学习开始前就能在目标领域中进行动作剪枝。我们将这种方法称为“Q操作”（Q-M）。迭代过程假设可以访问一个轻量级模型，这种模型易于提供或学习。我们正式证明了在离散领域中，Q-M不会影响返回策略的最优性，并且在概率意义上，它在样本复杂度方面具有可证明的高效性。为了展示Q-M的有效性、通用性和实用性，我们在多种合成和模拟领域中对其进行了评估。</p>
<h3 id="QoQ-Med-Building-Multimodal-Clinical-Foundation-Models-with-Domain-Aware-GRPO-Training"><a href="#QoQ-Med-Building-Multimodal-Clinical-Foundation-Models-with-Domain-Aware-GRPO-Training" class="headerlink" title="QoQ-Med: Building Multimodal Clinical Foundation Models with Domain-Aware GRPO Training"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2506.00711v2">QoQ-Med: Building Multimodal Clinical Foundation Models with Domain-Aware GRPO Training</a></h3><p><strong>Categories:</strong> Reinforcement Learning, Vision-Language Models</p>
<p>临床决策通常需要对异构数据进行推理，然而现有的多模态语言模型（MLLMs）仍主要以视觉为中心，难以跨临床专科进行泛化。为弥合这一差距，我们引入了QoQ-Med-7B&#x2F;32B，这是首个开放的通用型临床基础模型，能够联合推理医学图像、时间序列信号和文本报告。QoQ-Med采用了一种新颖的强化学习目标——领域感知相对策略优化（DRPO），该方法根据领域稀缺性和模态难度分层地对归一化奖励进行缩放，从而缓解因临床数据分布不均衡导致的性能失衡问题。在涵盖9个临床领域的261万条指令微调数据对上进行训练，我们证明DRPO训练在所有视觉领域上的平均宏F1值比其他无批评者训练方法（如GRPO）的诊断性能提高了43%。此外，借助于QoQ-Med在密集分割数据上的训练，该模型能够突出与诊断相关的显著区域，其IoU比开源模型高10倍，同时达到OpenAI o4-mini的性能水平。为促进可重复性和下游研究，我们发布了以下内容：(i)完整的模型权重，(ii)模块化训练流程，以及(iii)所有中间推理轨迹，详见<a target="_blank" rel="noopener" href="https://github.com/DDVD233/QoQ_Med%E3%80%82">https://github.com/DDVD233/QoQ_Med。</a></p>
<h3 id="Breaking-the-Exploration-Bottleneck-Rubric-Scaffolded-Reinforcement-Learning-for-General-LLM-Reasoning"><a href="#Breaking-the-Exploration-Bottleneck-Rubric-Scaffolded-Reinforcement-Learning-for-General-LLM-Reasoning" class="headerlink" title="Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16949v5">Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>近年来，大语言模型（LLMs）的进展凸显了强化学习（RL）在促进推理能力发展方面的潜力。尽管取得了令人鼓舞的成果，但一个根本性的困境依然存在：RL的改进依赖于从高质量样本中学习，然而，对这类样本的探索仍受到LLMs固有局限性的制约。这实际上形成了一个不理想的循环：无法探索的内容也无法被学习。在本工作中，我们提出了一个新颖的指导性脚手架框架——Rubric-Scaffolded Reinforcement Learning（RuscaRL），旨在突破通用LLM推理的探索瓶颈。</p>
<p>具体而言，RuscaRL引入了清单式评分标准（rubrics）作为（1）在生成轨迹（rollout）过程中进行探索的明确脚手架，通过在任务指令中提供不同评分标准作为外部指导，引导生成多样化的高质量回答；这种指导会随时间逐渐减弱，鼓励模型内化潜在的推理模式；（2）在模型训练过程中用于探索的可验证奖励，我们可以通过评分标准作为参考获得稳健的“LLM作为裁判”评分，从而在通用推理任务上实现有效的强化学习。</p>
<p>大量实验结果表明，RuscaRL在各种基准测试中表现出色，有效拓展了在Best-of-N评估下的推理边界。值得注意的是，RuscaRL将Qwen2.5-7B-Instruct在HealthBench-500上的表现从23.6提升至50.3，超过了GPT-4.1。此外，我们对Qwen3-30B-A3B-Instruct进行微调后，其在HealthBench-500上的表现达到了61.1，优于包括OpenAI-o3在内的主流大语言模型。我们的代码已开源，地址为：<a target="_blank" rel="noopener" href="https://github.com/IANNXANG/RuscaRL%E3%80%82">https://github.com/IANNXANG/RuscaRL。</a></p>
<h3 id="Misalignment-Bounty-Crowdsourcing-AI-Agent-Misbehavior"><a href="#Misalignment-Bounty-Crowdsourcing-AI-Agent-Misbehavior" class="headerlink" title="Misalignment Bounty: Crowdsourcing AI Agent Misbehavior"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19738v1">Misalignment Bounty: Crowdsourcing AI Agent Misbehavior</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>先进的AI系统有时会以与人类意图不同的方式行事。为了收集清晰且可重复的案例，我们启动了“对齐偏差赏金”（Misalignment Bounty）项目，这是一个众包项目，旨在收集代理（AI系统）追求未预期或不安全目标的案例。该项目共收到295份提交，其中九份获得了奖励。</p>
<p>本报告将解释该计划的动机和评估标准，并逐步分析这九份获奖提交。</p>
<h3 id="Memo-Training-Memory-Efficient-Embodied-Agents-with-Reinforcement-Learning"><a href="#Memo-Training-Memory-Efficient-Embodied-Agents-with-Reinforcement-Learning" class="headerlink" title="Memo: Training Memory-Efficient Embodied Agents with Reinforcement Learning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19732v1">Memo: Training Memory-Efficient Embodied Agents with Reinforcement Learning</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>为了使具身智能体在长时间范围内有效运作，开发能够形成并访问记忆的模型至关重要，以便使其保持对环境的语境感知。在当前基于Transformer的具身序列决策任务的训练范式中，视觉输入往往会超出Transformer的上下文限制，而人类则能够维持并利用压缩为记忆的终身经验。原则上，大量输入信息是无关的，可以进行显著的压缩与抽象。然而，现有的方法主要集中在两种类型上：一种是具有固定大小记忆的循环模型，另一种是依赖完整上下文的Transformer模型。在本研究中，我们提出了Memo，这是一种基于Transformer的架构和训练方案，用于在需要大量记忆和长时间跨度的任务中进行强化学习（RL）。Memo通过在训练过程中将周期性摘要标记与模型输入交错，实现记忆的创建与检索。我们在网格世界元强化学习基准和照片级逼真的室内多目标导航任务中验证了Memo的有效性。Memo在计算和存储效率方面优于传统的长上下文Transformer基线模型，同时在推理过程中对更长的上下文具有更好的泛化能力，并且在流式处理场景中保持鲁棒性，即使需要截断历史上下文以适应推理限制。</p>
<h3 id="ARM-FM-Automated-Reward-Machines-via-Foundation-Models-for-Compositional-Reinforcement-Learning"><a href="#ARM-FM-Automated-Reward-Machines-via-Foundation-Models-for-Compositional-Reinforcement-Learning" class="headerlink" title="ARM-FM: Automated Reward Machines via Foundation Models for Compositional Reinforcement Learning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.14176v2">ARM-FM: Automated Reward Machines via Foundation Models for Compositional Reinforcement Learning</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>强化学习（RL）算法对奖励函数的定义非常敏感，这仍然是限制其广泛应用的核心挑战。我们提出了ARM-FM：基于基础模型的自动奖励机器（Automated Reward Machines via Foundation Models），这是一个用于强化学习中自动、组合式奖励设计的框架，利用了基础模型（FMs）的高级推理能力。奖励机器（RMs）——一种基于自动机的形式化奖励定义方法——被用作RL目标定义的机制，并通过使用基础模型自动构建。奖励机器的结构化形式化方法可以实现有效的任务分解，而基础模型的使用则使得目标定义可以使用自然语言进行表达。具体而言，我们实现了以下三点：（i）使用基础模型从自然语言规范中自动生成奖励机器；（ii）为每个奖励机器自动机状态关联语言嵌入，以实现跨任务的泛化；（iii）在一系列具有挑战性的环境中提供了ARM-FM有效性的实证证据，包括零样本泛化的证据。</p>
<h3 id="Masked-Generative-Priors-Improve-World-Models-Sequence-Modelling-Capabilities"><a href="#Masked-Generative-Priors-Improve-World-Models-Sequence-Modelling-Capabilities" class="headerlink" title="Masked Generative Priors Improve World Models Sequence Modelling Capabilities"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2410.07836v6">Masked Generative Priors Improve World Models Sequence Modelling Capabilities</a></h3><p><strong>Categories:</strong> Reinforcement Learning, World Models</p>
<p>深度强化学习（RL）已成为在复杂环境中创建人工代理的主流方法。基于模型的方法，即包含世界模型（能够预测环境动态）的强化学习方法，是提高数据效率最有前景的研究方向之一，是弥合研究与实际部署之间差距的关键步骤。特别是，世界模型通过在想象中进行学习，提高了样本效率，这种方法涉及以自监督的方式训练环境的生成序列模型。最近，掩码生成建模（Masked Generative Modelling）作为一种更为高效且优越的归纳偏差（inductive bias），在建模和生成标记序列方面崭露头角。基于高效的随机Transformer世界模型（STORM）架构，我们用掩码生成先验（如MaskGIT先验）替代传统的MLP先验，并引入了GIT-STORM。我们在两个下游任务上评估了我们的模型：强化学习和视频预测。在Atari 100k基准测试中，GIT-STORM在强化学习任务中表现出显著的性能提升。此外，我们首次将基于Transformer的世界模型应用于连续动作环境，弥补了先前研究中的一个重要空白。为了实现这一目标，我们采用了一种状态混合函数，将潜在状态表示与动作进行整合，从而使我们的模型能够处理连续控制任务。我们在DeepMind控制套件上通过定性和定量分析验证了这一方法，展示了基于Transformer的世界模型在这一新领域中的有效性。我们的结果突显了掩码生成动力学先验（MaskGIT）的多功能性和有效性，为更精确的世界模型和有效的强化学习策略铺平了道路。</p>
<h3 id="Optimizing-the-Unknown-Black-Box-Bayesian-Optimization-with-Energy-Based-Model-and-Reinforcement-Learning"><a href="#Optimizing-the-Unknown-Black-Box-Bayesian-Optimization-with-Energy-Based-Model-and-Reinforcement-Learning" class="headerlink" title="Optimizing the Unknown: Black Box Bayesian Optimization with Energy-Based Model and Reinforcement Learning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19530v1">Optimizing the Unknown: Black Box Bayesian Optimization with Energy-Based Model and Reinforcement Learning</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>现有的贝叶斯优化（BO）方法通常通过在探索与利用之间进行权衡来优化代价高昂的目标函数。然而，这些方法往往受到显著的一步偏差的影响，可能导致收敛到局部最优解，并在复杂或高维任务中表现不佳。近年来，黑箱优化（BBO）在多个科学和工程领域取得了成功，尤其是在函数评估成本高昂且梯度不可用的情况下。受此启发，我们提出了用于贝叶斯优化的强化能量模型（REBMBO），该方法结合高斯过程（GP）进行局部引导，并利用能量模型（EBM）来捕捉全局结构信息。值得注意的是，我们将每一次贝叶斯优化迭代定义为马尔可夫决策过程（MDP），并使用近端策略优化（PPO）进行自适应多步前瞻，动态调整探索的深度和方向，从而有效克服传统BO方法的局限性。我们在合成数据和现实世界基准数据上进行了大量实验，验证了REBMBO的优越性能。此外，针对不同GP配置的进一步分析进一步突显了其适应性和鲁棒性。</p>
<h3 id="VideoAgentTrek-Computer-Use-Pretraining-from-Unlabeled-Videos"><a href="#VideoAgentTrek-Computer-Use-Pretraining-from-Unlabeled-Videos" class="headerlink" title="VideoAgentTrek: Computer Use Pretraining from Unlabeled Videos"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19488v1">VideoAgentTrek: Computer Use Pretraining from Unlabeled Videos</a></h3><p><strong>Categories:</strong> Reinforcement Learning,Vision-Language Models</p>
<p>训练计算机使用代理需要大量的GUI交互数据，但大规模手动标注操作轨迹成本极高。我们提出了VideoAgentTrek，这是一个可扩展的流程，能够从公开的屏幕录制视频中自动挖掘训练数据，规模达到网络级别，从而消除了对人工标注的依赖。我们的方法解决了关键挑战：原始视频包含隐式的示范，但缺乏明确的动作标签。为了解决这一问题，我们开发了Video2Action，这是一个逆动力学模块（IDM），包含两个组成部分：（1）一个视频定位模型，可以检测并定位GUI操作，精确地确定其时间边界和上下文；（2）一个动作内容识别器，能够以高保真度提取结构化参数，如点击坐标和输入的文本。将我们的流程应用于39,000个YouTube教程视频，可以自动生成152万个交互步骤。我们通过持续的预训练和监督微调来利用这些数据。在OSWorld-Verified数据集上，我们的方法将任务成功率从仅使用SFT基础模型的9.3%提升至15.8%，相对提升达70%。在AgentNetBench数据集上，步骤准确率从64.1%提升至69.3%。我们的结果表明，被动的互联网视频可以转化为高质量的监督信号，为计算机使用代理提供一种可扩展的替代方案，以昂贵的人工标注。</p>
<h3 id="Universal-Quantitative-Abstraction-Categorical-Duality-and-Logical-Completeness-for-Probabilistic-Systems"><a href="#Universal-Quantitative-Abstraction-Categorical-Duality-and-Logical-Completeness-for-Probabilistic-Systems" class="headerlink" title="Universal Quantitative Abstraction: Categorical Duality and Logical Completeness for Probabilistic Systems"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19444v1">Universal Quantitative Abstraction: Categorical Duality and Logical Completeness for Probabilistic Systems</a></h3><p><strong>Categories:</strong> Reinforcement Learning, World Models</p>
<p>提出了一种定量抽象的统一理论，用于概率系统，将范畴论、最优传输和定量模态逻辑联系起来。该理论的核心是一个具有普遍性质的规范 $ \varepsilon $-商：在所有 $ \varepsilon $-抽象中，它是最能保留价值损失限制的抽象。这一构造在抽象与实现函子之间建立了伴随关系 $ (Q_{\varepsilon} \dashv R_{\varepsilon}) $，通过特殊伴随函子定理建立，揭示了度量结构与逻辑语义之间的范畴对偶性。行为伪度量被表征为一种贝尔曼风格算子的唯一不动点，并在协代数框架中证明了其收缩性和利普希茨性质。引入了一种定量模态 $ \mu $-演算，并证明其在逻辑可表示系统中具有表达完备性，因此行为距离与最大逻辑偏差一致。分析了接口细化下的组合性，阐明了抽象在系统边界间的相互作用。在有限马尔可夫决策过程中进行的精确验证套件证实了收缩性质、价值损失界限、对扰动的稳定性、对抗区分性和可扩展性，展示了该框架的鲁棒性和计算可行性。由此得到的框架为状态聚合和表示学习提供了原理性的目标，并在随机域中对价值函数近似提供了数学精确的保证。</p>
<h3 id="ColorAgent-Building-A-Robust-Personalized-and-Interactive-OS-Agent"><a href="#ColorAgent-Building-A-Robust-Personalized-and-Interactive-OS-Agent" class="headerlink" title="ColorAgent: Building A Robust, Personalized, and Interactive OS Agent"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19386v1">ColorAgent: Building A Robust, Personalized, and Interactive OS Agent</a></h3><p><strong>Categories:</strong> Reinforcement Learning, World Models</p>
<p>随着硬件、软件和大型语言模型技术的进步，人与操作系统之间的交互方式已从命令行界面演进到迅速兴起的AI代理交互。构建一个能够执行用户指令并忠实遵循用户意愿的操作系统（OS）代理正变得越来越现实。在本技术报告中，我们介绍了ColorAgent，这是一种设计用于与环境进行长期、稳健交互的操作系统代理，同时还能实现个性化和主动性的用户交互。为了实现与环境的长期交互，我们通过分步强化学习和自我演进训练增强了模型的能力，同时开发了一种定制的多代理框架，以确保其通用性、一致性和鲁棒性。在用户交互方面，我们探索了个性化用户意图识别和主动性参与，将操作系统代理不仅仅定位为一个自动化工具，而是作为一位温暖、协作的伙伴。我们在AndroidWorld和AndroidLab基准测试中评估ColorAgent，分别取得了77.2%和50.7%的成功率，从而建立了新的行业标杆。然而，我们也注意到当前的基准测试对于全面评估操作系统代理仍显不足，并在未来的进一步研究方向中提出了建议，特别是在评估范式、代理协作和安全性等方面。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/MadeAgents/mobile-use%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/MadeAgents/mobile-use上获取。</a></p>
<h3 id="Coordinated-Strategies-in-Realistic-Air-Combat-by-Hierarchical-Multi-Agent-Reinforcement-Learning"><a href="#Coordinated-Strategies-in-Realistic-Air-Combat-by-Hierarchical-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Coordinated Strategies in Realistic Air Combat by Hierarchical Multi-Agent Reinforcement Learning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.11474v2">Coordinated Strategies in Realistic Air Combat by Hierarchical Multi-Agent Reinforcement Learning</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>在对空战进行现实模拟的环境中实现任务目标极具挑战性，原因在于情境感知存在不完美以及飞行动力学的非线性特性。在本文中，我们引入了一种新颖的三维多智能体空战环境和一种分层多智能体强化学习框架，以应对这些挑战。我们的方法结合了异构智能体动力学、课程学习、联赛对抗训练以及一种新适应的训练算法。为此，决策过程被组织为两个抽象层级：低层级策略学习精确的控制机动动作，而高层策略则根据任务目标发布战术指令。实验结果表明，我们的分层方法在复杂的空战场景中提高了学习效率和战斗性能。</p>
<h3 id="Learning-To-Defer-To-A-Population-With-Limited-Demonstrations"><a href="#Learning-To-Defer-To-A-Population-With-Limited-Demonstrations" class="headerlink" title="Learning To Defer To A Population With Limited Demonstrations"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19351v1">Learning To Defer To A Population With Limited Demonstrations</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>本文探讨了阻碍学习延迟（L2D）系统在人群中实际部署的关键数据稀缺问题。我们提出了一种上下文感知的半监督框架，利用元学习仅通过少量演示生成专家特定的嵌入表示。我们展示了一种双用途机制的有效性：首先，这些嵌入用于生成大量伪标签以进行训练；随后，这些嵌入能够在测试时实现对新专家的实时适应。在三个不同数据集上的实验结果表明，通过这些合成标签训练的模型可以迅速接近oracle级别的性能，验证了我们方法的数据效率。通过解决一个关键的训练瓶颈，这项工作使自适应L2D系统更加实用和可扩展，为现实环境中人与AI的合作铺平了道路。为促进可重复性并解决主文中未涉及的实现细节，我们提供了源代码和训练配置，网址为：<a target="_blank" rel="noopener" href="https://github.com/nil123532/learning-to-defer-to-a-population-with-limited-demonstrations%E3%80%82">https://github.com/nil123532/learning-to-defer-to-a-population-with-limited-demonstrations。</a></p>
<h3 id="Every-Attention-Matters-An-Efficient-Hybrid-Architecture-for-Long-Context-Reasoning"><a href="#Every-Attention-Matters-An-Efficient-Hybrid-Architecture-for-Long-Context-Reasoning" class="headerlink" title="Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19338v1">Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>在本技术报告中，我们介绍了环形线性模型系列，具体包括Ring-mini-linear-2.0和Ring-flash-linear-2.0。Ring-mini-linear-2.0包含160亿个参数和95.7亿个激活值，而Ring-flash-linear-2.0则包含1040亿个参数和61亿个激活值。这两个模型均采用了一种混合架构，有效结合了线性注意力机制和softmax注意力机制，显著降低了长上下文推理场景下的I&#x2F;O和计算开销。与一个320亿参数的密集型模型相比，该系列模型将推理成本降低了10倍；与原始的Ring系列模型相比，成本也降低了超过50%。此外，通过系统性地探索混合架构中不同注意力机制之间的比例，我们已经确定了当前最优的模型结构。同时，通过利用我们自主研发的高性能FP8操作库linghe，整体训练效率提升了50%。得益于训练和推理引擎操作符之间的高度对齐，这些模型在强化学习阶段能够进行长期、稳定且高效的优化，在多个具有挑战性的复杂推理基准测试中持续保持最先进（SOTA）的性能。</p>
<h3 id="Balancing-Rewards-in-Text-Summarization-Multi-Objective-Reinforcement-Learning-via-HyperVolume-Optimization"><a href="#Balancing-Rewards-in-Text-Summarization-Multi-Objective-Reinforcement-Learning-via-HyperVolume-Optimization" class="headerlink" title="Balancing Rewards in Text Summarization: Multi-Objective Reinforcement Learning via HyperVolume Optimization"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19325v1">Balancing Rewards in Text Summarization: Multi-Objective Reinforcement Learning via HyperVolume Optimization</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>文本摘要是一项关键任务，需要同时优化多个目标，包括一致性、连贯性、相关性和流畅性，这带来了相当大的挑战。尽管大型语言模型（LLMs）通过强化学习（RL）的增强已展现出卓越的性能，但目前仍很少有研究专注于基于LLMs的强化学习来优化摘要的多目标问题。在本文中，我们引入了一种新颖的优化策略——超体积优化（HVO），该方法通过使用超体积方法，在强化学习的奖励过程中动态调整各组之间的得分。这种方法引导模型优化逐步逼近帕累托前沿，从而在多个目标上生成均衡的摘要。在多个代表性摘要数据集上的实验结果表明，我们的方法在总体得分上优于组相对策略优化（GRPO），并且在不同维度上表现出更均衡的性能。此外，通过HVO增强的70亿参数基础模型在摘要任务中的表现与GPT-4相当，同时保持了更短的生成长度。我们的代码已公开在<a target="_blank" rel="noopener" href="https://github.com/ai4business-LiAuto/HVO.git%E3%80%82">https://github.com/ai4business-LiAuto/HVO.git。</a></p>
<h3 id="Continual-Knowledge-Adaptation-for-Reinforcement-Learning"><a href="#Continual-Knowledge-Adaptation-for-Reinforcement-Learning" class="headerlink" title="Continual Knowledge Adaptation for Reinforcement Learning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19314v1">Continual Knowledge Adaptation for Reinforcement Learning</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>强化学习使智能体能够通过与环境的交互来学习最优行为。然而，现实世界中的环境通常是非平稳的，要求智能体持续适应新的任务和变化的条件。尽管持续强化学习（Continual Reinforcement Learning）可以在多个任务之间进行学习，但现有方法往往存在灾难性遗忘和知识利用效率低的问题。为了解决这些挑战，我们提出了持续知识适应强化学习（Continual Knowledge Adaptation for Reinforcement Learning，CKA-RL），它能够积累并有效利用历史知识。具体而言，我们引入了一种持续知识适应策略，该策略包括维护一个任务特定的知识向量池，并动态使用历史知识以适应新任务。这一过程通过保留和适应关键模型参数，减轻了灾难性遗忘问题，并实现了任务间的高效知识迁移。此外，我们还提出了一种自适应知识融合机制，通过合并相似的知识向量来应对可扩展性挑战，从而在减少内存需求的同时确保关键知识的保留。在三个基准测试上的实验表明，所提出的CKA-RL方法优于当前最先进的方法，在整体性能上提升了4.20%，在前向迁移上提升了8.02%。源代码可在<a target="_blank" rel="noopener" href="https://github.com/Fhujinwu/CKA-RL%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Fhujinwu/CKA-RL获取。</a></p>
<h3 id="Traffic-R1-Reinforced-LLMs-Bring-Human-Like-Reasoning-to-Traffic-Signal-Control-Systems"><a href="#Traffic-R1-Reinforced-LLMs-Bring-Human-Like-Reasoning-to-Traffic-Signal-Control-Systems" class="headerlink" title="Traffic-R1: Reinforced LLMs Bring Human-Like Reasoning to Traffic Signal Control Systems"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.02344v2">Traffic-R1: Reinforced LLMs Bring Human-Like Reasoning to Traffic Signal Control Systems</a></h3><p><strong>Categories:</strong> Reinforcement Learning, Robotics</p>
<p>我们介绍了Traffic-R1，这是一个拥有30亿参数的基准模型，具备类似人类的推理能力，用于交通信号控制（TSC）。该模型是在模拟交通环境中，通过专家指导下的大语言模型（LLM）自我探索和迭代强化训练而开发的。与传统的强化学习方法和近期基于LLM的方法相比，Traffic-R1具有三大优势：一是零样本泛化能力，能够通过利用内部交通控制策略和推理，直接迁移到新的道路网络和分布外事件中；二是紧凑的30亿参数设计，支持在移动级芯片上进行实时推理，便于边缘部署；三是可解释的TSC过程，通过通信和异步通信网络实现多路口协调。大量基准测试表明，Traffic-R1在性能上优于强大的基线模型和训练密集型的强化学习控制器。在实际应用中，该模型目前每天管理影响超过55,000名驾驶员的信号，平均排队长度减少超过5%，并将操作人员的工作量减半。我们的模型可在<a target="_blank" rel="noopener" href="https://huggingface.co/Season998/Traffic-R1%E8%8E%B7%E5%8F%96%E3%80%82">https://huggingface.co/Season998/Traffic-R1获取。</a></p>
<h3 id="Learning-to-Make-Friends-Coaching-LLM-Agents-toward-Emergent-Social-Ties"><a href="#Learning-to-Make-Friends-Coaching-LLM-Agents-toward-Emergent-Social-Ties" class="headerlink" title="Learning to Make Friends: Coaching LLM Agents toward Emergent Social Ties"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19299v1">Learning to Make Friends: Coaching LLM Agents toward Emergent Social Ties</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>大型语言模型（LLM）代理能否再现人类在线行为中所体现的复杂社会动态——这些动态由同质性、互惠性和社会认同等因素塑造？哪些记忆和学习机制能够使这些动态产生？我们提出了一种多代理LLM模拟框架，在该框架中，代理会反复互动、相互评估，并通过由教练信号加速的上下文学习机制来调整自身行为。为了模拟人类社会行为，我们设计了行为奖励函数，以捕捉在线参与的核心驱动力，包括社交互动、信息获取、自我呈现、协调合作和情感支持。这些奖励机制使代理的目标与实证观察到的用户动机相一致，从而能够研究网络结构和群体形成如何从个体决策中涌现。我们的实验表明，经过教练的LLM代理能够发展出稳定的互动模式，并形成涌现的社会关系，从而产生与真实在线社区相似的网络结构。通过结合行为奖励与上下文适应机制，我们的框架为研究LLM群体中的集体动态提供了一个原理性的实验平台，并揭示了人工代理如何模拟或偏离人类式的社会行为。</p>
<h3 id="Improved-Exploration-in-GFlownets-via-Enhanced-Epistemic-Neural-Networks"><a href="#Improved-Exploration-in-GFlownets-via-Enhanced-Epistemic-Neural-Networks" class="headerlink" title="Improved Exploration in GFlownets via Enhanced Epistemic Neural Networks"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2506.16313v2">Improved Exploration in GFlownets via Enhanced Epistemic Neural Networks</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>在GFlowNets中，高效识别合适的训练轨迹仍是一个开放性问题。为了解决这一问题，必须优先在奖励分布尚未充分学习的状态空间区域进行探索。这需要以不确定性驱动的探索方式，换句话说，智能体应当意识到自己所不知道的内容。这种属性可以通过联合预测来衡量，尤其在组合和序列决策问题中尤为重要。在本研究中，我们将知识性神经网络（ENN）与传统的GFlowNets架构相结合，以实现更高效的联合预测和更好的不确定性量化，从而提升探索效率并优化轨迹识别。我们提出的算法ENN-GFN-Enhanced与GFlowNets中的基线方法进行了比较，并在网格环境和多种设置下的结构化序列生成任务中进行了评估，验证了其有效性和高效性。</p>
<h3 id="PARCO-Parallel-AutoRegressive-Models-for-Multi-Agent-Combinatorial-Optimization"><a href="#PARCO-Parallel-AutoRegressive-Models-for-Multi-Agent-Combinatorial-Optimization" class="headerlink" title="PARCO: Parallel AutoRegressive Models for Multi-Agent Combinatorial Optimization"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2409.03811v3">PARCO: Parallel AutoRegressive Models for Multi-Agent Combinatorial Optimization</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>涉及多个智能体的组合优化问题由于其NP难的性质以及对有效智能体协调的必要性而极具挑战性。尽管在基于学习的方法上取得了进展，现有的方法仍然面临关键的局限性，包括次优的智能体协调、泛化能力差以及高计算延迟。为了解决这些问题，我们提出了PARCO（并行自回归组合优化），这是一种通用的强化学习框架，旨在高效地构建多智能体组合任务的高质量解决方案。为此，PARCO集成了三个关键的新组件：（1）基于Transformer的通信层，用于在并行解构造过程中实现有效的智能体协作；（2）多指针机制，用于实现低延迟的并行智能体决策；（3）基于优先级的冲突解决器，通过学习到的优先级来解决决策冲突。我们在多智能体车辆路径和调度问题中评估了PARCO，我们的方法在性能上优于现有的学习方法，展示了强大的泛化能力和显著的计算效率。我们公开了源代码以促进未来的研究：<a target="_blank" rel="noopener" href="https://github.com/ai4co/parco%E3%80%82">https://github.com/ai4co/parco。</a></p>
<h3 id="SPOT-Scalable-Policy-Optimization-with-Trees-for-Markov-Decision-Processes"><a href="#SPOT-Scalable-Policy-Optimization-with-Trees-for-Markov-Decision-Processes" class="headerlink" title="SPOT: Scalable Policy Optimization with Trees for Markov Decision Processes"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19241v1">SPOT: Scalable Policy Optimization with Trees for Markov Decision Processes</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>可解释的强化学习策略对于高风险决策至关重要，但在马尔可夫决策过程（MDPs）中优化决策树策略仍然具有挑战性。我们提出了一种名为SPOT的新方法，用于计算决策树策略，该方法将优化问题建模为混合整数线性规划（MILP）问题。为了提高效率，我们采用了一种缩减空间的分支定界方法，将MDP动态与树结构约束分离，从而实现高效的并行搜索。这显著提高了运行速度和可扩展性，相比以往方法具有明显优势。我们的方法确保每次迭代都能得到最优的决策树策略。在标准基准测试上的实验结果表明，SPOT实现了显著的速度提升，并能扩展到具有更多状态的较大MDP。所得到的决策树策略具有可解释性和紧凑性，保持了透明性而不会牺牲性能。这些结果表明，我们的方法同时实现了可解释性和可扩展性，比现有方法快了一个数量级，从而提供了高质量的策略。</p>
<h3 id="Horizon-Reduction-Makes-RL-Scalable"><a href="#Horizon-Reduction-Makes-RL-Scalable" class="headerlink" title="Horizon Reduction Makes RL Scalable"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2506.04168v3">Horizon Reduction Makes RL Scalable</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>在本工作中，我们研究了离线强化学习（RL）算法的可扩展性。原则上，一个真正具有可扩展性的离线RL算法应该能够解决任何给定的问题，无论其复杂程度如何，只要提供足够的数据、计算资源和模型容量。我们使用比典型离线RL数据集大1000倍的数据集，研究当前的离线RL算法在多样化、具有挑战性且之前未被解决的任务上是否能够实现这一承诺。我们观察到，尽管增加了数据量，许多现有的离线RL算法表现出较差的可扩展性，性能远未达到最大值。我们假设时间步长（horizon）是导致离线RL可扩展性差的主要原因。我们通过一系列分析实验验证了这一假设，结果显示，长的时间步长确实构成了离线RL扩展的一个根本性障碍。随后，我们展示了各种减少时间步长的技术在挑战性任务上显著提升了可扩展性。基于我们的洞察，我们还提出了一种简单但具有可扩展性的方法，名为SHARSA，该方法有效地减少了时间步长。SHARSA在我们的评估方法中实现了最佳的渐近性能和可扩展性，表明显式地减少时间步长可以释放离线RL的可扩展性。代码：<a target="_blank" rel="noopener" href="https://github.com/seohongpark/horizon-reduction">https://github.com/seohongpark/horizon-reduction</a></p>
<h3 id="WebGraphEval-Multi-Turn-Trajectory-Evaluation-for-Web-Agents-using-Graph-Representation"><a href="#WebGraphEval-Multi-Turn-Trajectory-Evaluation-for-Web-Agents-using-Graph-Representation" class="headerlink" title="WebGraphEval: Multi-Turn Trajectory Evaluation for Web Agents using Graph Representation"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19205v1">WebGraphEval: Multi-Turn Trajectory Evaluation for Web Agents using Graph Representation</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>目前对网络代理的评估大多简化为二元成功指标或对单一参考轨迹的符合度，忽略了基准数据集中存在的结构多样性。我们提出了WebGraphEval框架，该框架将多个代理的轨迹抽象为一个统一的加权动作图。这种表示方式可以直接与WebArena等基准进行兼容，利用排行榜运行和新收集的轨迹，而无需修改环境。该框架标准编码动作，合并重复行为，并应用包括奖励传播和基于成功度的边统计在内的结构分析。对六个网络代理的数千条轨迹进行评估表明，图抽象能够捕捉跨模型的规律性，突出冗余和低效部分，并识别基于结果指标所忽略的关键决策点。通过将网络交互建模为图结构数据，WebGraphEval建立了一种适用于多路径、跨代理和效率感知的网络代理评估通用方法。</p>
<h3 id="ROTATE-Regret-driven-Open-ended-Training-for-Ad-Hoc-Teamwork"><a href="#ROTATE-Regret-driven-Open-ended-Training-for-Ad-Hoc-Teamwork" class="headerlink" title="ROTATE: Regret-driven Open-ended Training for Ad Hoc Teamwork"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.23686v2">ROTATE: Regret-driven Open-ended Training for Ad Hoc Teamwork</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>在多智能体学习中，学会与之前未见过的合作伙伴进行协作，是一个根本性的泛化挑战，被称为“临时团队协作”（Ad Hoc Teamwork, AHT）。现有的AHT方法通常采用两阶段流程：首先，生成一个固定数量的队友，其想法是这些队友应能代表部署时将遇到的队友；其次，训练一个AHT智能体，使其能与该群体中的智能体良好协作。截至目前，研究社区主要专注于为每个阶段设计独立的算法。这种分离导致了生成的队友行为覆盖范围有限，并且忽略了这些生成的队友是否便于AHT智能体学习。此外，训练AHT智能体的算法通常将训练队友集合视为静态的，因此试图在不假设对训练队友集合有任何控制的情况下，泛化到之前未见过的合作伙伴智能体。本文通过将问题重新表述为AHT智能体与对抗性队友生成器之间的开放性学习过程，提出了一种统一的AHT框架。我们引入了ROTATE，这是一种基于遗憾驱动的、开放式的训练算法，它在提升AHT智能体和生成探测其缺陷的队友之间交替进行。在多种双人环境中的实验表明，ROTATE在泛化到未见过的评估队友集合时显著优于基线方法，从而确立了稳健且可泛化的团队合作的新标准。</p>
<h3 id="Imbalanced-Gradients-in-RL-Post-Training-of-Multi-Task-LLMs"><a href="#Imbalanced-Gradients-in-RL-Post-Training-of-Multi-Task-LLMs" class="headerlink" title="Imbalanced Gradients in RL Post-Training of Multi-Task LLMs"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19178v1">Imbalanced Gradients in RL Post-Training of Multi-Task LLMs</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>大型语言模型（LLMs）的多任务后训练通常通过混合来自不同任务的数据集并联合优化来实现。这种方法隐含地假设所有任务产生的梯度幅度相似；当这一假设不成立时，优化过程会偏向于梯度较大的任务。然而，在本文中，我们表明在强化学习（RL）的后训练中，这一假设并不成立：某些任务会产生显著更大的梯度，从而导致更新偏向这些任务。这种梯度不平衡只有在更大的梯度意味着更大的学习收益（即更大的性能提升）时才合理，但我们的研究发现这并非如此。梯度较大的任务可能获得与梯度较小的任务相当甚至更低的学习收益。进一步的分析表明，这些梯度不平衡无法用典型的训练统计量（如训练奖励或优势）来解释，这表明它们源于任务之间固有的差异。这警示我们不能简单地混合数据集，并呼吁未来对LLMs进行基于原理的梯度级修正研究。</p>
<h3 id="News-Aware-Direct-Reinforcement-Trading-for-Financial-Markets"><a href="#News-Aware-Direct-Reinforcement-Trading-for-Financial-Markets" class="headerlink" title="News-Aware Direct Reinforcement Trading for Financial Markets"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19173v1">News-Aware Direct Reinforcement Trading for Financial Markets</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>金融市场以对新闻高度敏感而著称。因此，有效将新闻数据纳入量化交易仍然是一个重要的挑战。现有的方法通常依赖于人工设计的规则和&#x2F;或手工构建的特征。在本研究中，我们直接使用由大型语言模型得出的新闻情感得分，结合原始价格和成交量数据，作为强化学习的可观测输入。这些输入通过序列模型（如循环神经网络或Transformer）进行处理，以实现端到端的交易决策。我们以加密货币市场为例进行实验，并评估了两种具有代表性的强化学习算法，即双重深度Q网络（DDQN）和群体相对策略优化（GRPO）。实验结果表明，我们的基于新闻的模型方法不依赖于手工构建的特征或人工设计的规则，其表现优于市场基准。我们进一步强调了时间序列信息在这一过程中的关键作用。</p>
<h3 id="X-Ego-Acquiring-Team-Level-Tactical-Situational-Awareness-via-Cross-Egocentric-Contrastive-Video-Representation-Learning"><a href="#X-Ego-Acquiring-Team-Level-Tactical-Situational-Awareness-via-Cross-Egocentric-Contrastive-Video-Representation-Learning" class="headerlink" title="X-Ego: Acquiring Team-Level Tactical Situational Awareness via Cross-Egocentric Contrastive Video Representation Learning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19150v1">X-Ego: Acquiring Team-Level Tactical Situational Awareness via Cross-Egocentric Contrastive Video Representation Learning</a></h3><p><strong>Categories:</strong> Reinforcement Learning, Robotics</p>
<p>人类团队战术源于每位球员的个体视角及其预测、解读和适应队友意图的能力。尽管视频理解技术的进步提高了体育运动中团队互动建模的水平，但大多数现有研究依赖于第三人称广播视角，忽视了多智能体学习中同步、以自我为中心的特性。我们引入了X-Ego-CS，这是一个基准数据集，包含《反恐精英2》（Counter-Strike 2）这一热门电子竞技游戏中45场专业级比赛的124小时游戏画面，旨在促进复杂三维环境中多智能体决策研究。X-Ego-CS提供了跨自我中心视角的视频流，同步捕捉所有玩家的第一人称视角以及状态-动作轨迹。基于这一资源，我们提出了交叉自我中心对比学习（Cross-Ego Contrastive Learning, CECL），通过对齐队友的自我中心视觉流，从个体视角促进团队层面的战术情境感知。我们在队友与对手位置预测任务上评估了CECL，证明其在利用最先进的视频编码器，从单一第一人称视角推断队友和对手位置方面具有有效性。总体而言，X-Ego-CS和CECL为电子竞技中的跨自我中心多智能体基准测试奠定了基础。更广泛地说，我们的工作将游戏理解定位为多智能体建模和战术学习的实验平台，对虚拟和现实世界中时空推理以及人机团队协作具有重要启示。代码和数据集可在 <a target="_blank" rel="noopener" href="https://github.com/HATS-ICT/x-ego">https://github.com/HATS-ICT/x-ego</a> 获取。</p>
<h3 id="Hummer-Towards-Limited-Competitive-Preference-Dataset"><a href="#Hummer-Towards-Limited-Competitive-Preference-Dataset" class="headerlink" title="Hummer: Towards Limited Competitive Preference Dataset"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2405.11647v4">Hummer: Towards Limited Competitive Preference Dataset</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>偏好数据集对于将人类偏好融入预训练语言模型至关重要，并在基于人类反馈的强化学习中发挥着关键作用。然而，这些数据集通常表现出相互冲突的对齐目标，导致模型更容易受到 jailbreak 攻击，并在将下游任务适配到优先考虑特定对齐目标时面临挑战，而不会对其他目标产生负面影响。在本工作中，我们引入了一种新的统计度量指标——对齐维度冲突（Alignment Dimension Conflict），用于量化偏好数据集中冲突的程度。随后，我们提出了创新的成对偏好数据集 \texttt{Hummer} 及其细粒度变体 \texttt{Hummer-F}，这些数据集具有降低冲突的对齐目标。\texttt{Hummer} 是基于 UltraFeedback 构建的，并通过 GPT-4 的 AI 反馈进行了增强，是首个旨在减少对齐目标之间竞争的偏好数据集。此外，我们开发了奖励模型 HummerRM 和 HummerRM-F，它们采用混合采样方法，有效平衡了多样化的对齐目标。这种采样方法使 HummerRM 成为特定领域进一步微调的理想模型，并有助于降低对攻击的脆弱性。</p>
<h2 id="Robotics"><a href="#Robotics" class="headerlink" title="Robotics"></a>Robotics</h2><h3 id="Semantic-World-Models"><a href="#Semantic-World-Models" class="headerlink" title="Semantic World Models"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19818v1">Semantic World Models</a></h3><p><strong>Categories:</strong> Robotics,World Models,Vision-Language Models</p>
<p>使用世界模型进行规划为机器人控制提供了一种强大的范式。传统方法通常训练模型根据当前帧和动作预测未来的帧，从而用于规划。然而，预测未来像素的目标往往与实际的规划目标相冲突；强大的像素重建并不总是与良好的规划决策相关。本文认为，与其重建未来帧中的像素，世界模型只需预测未来任务相关的语义信息即可。为此，本文将世界建模视为一个关于未来帧中语义信息的视觉问答问题。这种视角使得我们可以使用视觉语言模型底层的工具来处理世界建模。因此，可以通过在图像-动作-文本数据上进行监督微调，将视觉语言模型训练为“语义”世界模型，从而在继承预训练视觉-语言模型的泛化能力和鲁棒性的同时，支持用于决策制定的规划。本文展示了如何利用这种语义世界模型在开放式的机器人任务中进行策略改进，从而在基于重建的行动条件世界建模的典型范式之上实现显著的泛化性能提升。相关网站：<a target="_blank" rel="noopener" href="https://weirdlabuw.github.io/swm">https://weirdlabuw.github.io/swm</a></p>
<h3 id="Learning-Affordances-at-Inference-Time-for-Vision-Language-Action-Models"><a href="#Learning-Affordances-at-Inference-Time-for-Vision-Language-Action-Models" class="headerlink" title="Learning Affordances at Inference-Time for Vision-Language-Action Models"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19752v1">Learning Affordances at Inference-Time for Vision-Language-Action Models</a></h3><p><strong>Categories:</strong> Robotics,Vision-Language Models</p>
<p>解决复杂的现实世界控制任务通常需要多次尝试：如果第一次尝试失败，我们会反思哪里出了问题，并相应地调整策略，以避免重复同样的错误。在机器人领域，视觉-语言-动作模型（Vision-Language-Action models, VLAs）为解决复杂控制任务提供了一条有前景的途径，但它们在任务未成功时缺乏根据具体情境动态调整行为的能力。在本工作中，我们引入了一种名为“推理时执行学习”（Learning from Inference-Time Execution, LITEN）的方法，该方法将VLA的低级策略与一个基于过往经验的高级视觉语言模型（VLM）连接起来，通过将过往经验包含在上下文中进行条件建模，从而让模型能够学习低级VLA的可用性和能力。我们的方法在推理阶段生成并执行低级VLA的计划，而在评估阶段则对执行结果进行反思，从中得出有用的结论并将其纳入未来的推理上下文中。与非机器人领域的自优化方法不同，LITEN必须对无结构的现实世界机器人轨迹（如原始视频）进行反思，这要求在评估过程中采用结构化的引导机制。我们的实验结果表明，LITEN能够有效地从过去的经验中学习，生成利用高可用性指令来完成长期任务的计划。</p>
<h3 id="From-Forecasting-to-Planning-Policy-World-Model-for-Collaborative-State-Action-Prediction"><a href="#From-Forecasting-to-Planning-Policy-World-Model-for-Collaborative-State-Action-Prediction" class="headerlink" title="From Forecasting to Planning: Policy World Model for Collaborative State-Action Prediction"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19654v1">From Forecasting to Planning: Policy World Model for Collaborative State-Action Prediction</a></h3><p><strong>Categories:</strong> Robotics,World Models</p>
<p>尽管在构建世界模型方面取得了显著进展，其在自主系统中的潜力仍远未被充分挖掘：目前的世界模型主要用于世界模拟，并与轨迹规划相分离。虽然近期的研究尝试在统一框架中融合世界建模与规划，但世界建模对规划的协同促进机制仍需进一步探索。在本工作中，我们提出了一种新的驾驶范式，称为策略世界模型（Policy World Model, PWM）。该模型不仅在一个统一架构中集成了世界建模与轨迹规划，还能够通过所提出的无动作未来状态预测方案，利用学习到的世界知识来提升规划效果。通过协作的状态-动作预测，PWM可以模拟人类般的预判感知，从而实现更可靠的规划性能。为了提高视频预测的效率，我们进一步引入了一种动态增强的并行标记生成机制，该机制配备了上下文引导的分词器和自适应动态焦点损失。尽管仅使用前向摄像头输入，我们的方法在性能上与依赖多视角和多模态输入的最先进方法相匹配甚至超越了它们。代码和模型权重将在 <a target="_blank" rel="noopener" href="https://github.com/6550Zhao/Policy-World-Model">https://github.com/6550Zhao/Policy-World-Model</a> 上发布。</p>
<h3 id="Follow-the-STARs-Dynamic-ω-Regular-Shielding-of-Learned-Policies"><a href="#Follow-the-STARs-Dynamic-ω-Regular-Shielding-of-Learned-Policies" class="headerlink" title="Follow the STARs: Dynamic $ω$-Regular Shielding of Learned Policies"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.14689v3">Follow the STARs: Dynamic $ω$-Regular Shielding of Learned Policies</a></h3><p><strong>Categories:</strong> Robotics</p>
<p>本文提出了一种新颖的动态后屏蔽框架，该框架在预计算的概率策略上强制执行完整的$\omega$-正则正确性属性。这标志着从传统安全屏蔽设置（即确保任何坏事都不会发生）向一种额外强制执行活性（即确保某件好事最终会发生）的屏蔽过程的范式转变。在核心，我们的方法使用了基于策略模板的自适应运行时屏蔽（STARs），该方法利用宽容策略模板，以最小的干扰实现屏蔽。STARs的主要特性是引入了一种动态控制干扰的机制，允许一个可调的执行参数在运行时平衡形式化义务与任务特定行为。这使得在需要时可以触发更激进的执行，而在其他情况下则允许优化的策略选择。此外，STARs还支持在运行时对变化的规范或执行器故障进行适应，使其特别适用于网络物理系统应用。我们在一个移动机器人基准测试中评估STARs，以展示其在强制执行（逐步更新的）$\omega$-正则正确性属性时的可控干扰能力。</p>
<h3 id="RoboGPT-R1-Enhancing-Robot-Planning-with-Reinforcement-Learning"><a href="#RoboGPT-R1-Enhancing-Robot-Planning-with-Reinforcement-Learning" class="headerlink" title="RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.14828v2">RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning</a></h3><p><strong>Categories:</strong> Robotics,Reinforcement Learning,Vision-Language Models</p>
<p>提升具身智能体的推理能力对于机器人成功完成长期操作任务中的复杂人类指令至关重要。尽管基于监督微调（SFT）的大型语言模型和视觉语言模型在规划任务中取得了成功，但它们在复杂现实环境中的长期操作任务中仍面临挑战，这主要是由于其受限的常识和推理能力。考虑到通过监督微调将通用视觉语言模型对齐到机器人规划任务时，泛化能力较差且物理理解不足，我们提出了一种名为RoboGPT-R1的两阶段微调框架，用于具身规划。在该框架中，首先通过专家序列进行监督训练，以获取基础知识，随后通过强化学习（RL）解决模型在视觉空间理解和推理方面的不足。为了在多步推理任务中实现物理理解和动作序列的一致性，我们设计了一个基于规则的奖励函数，同时考虑了长期性能和环境中的动作约束。在Qwen2.5-VL-3B上训练的推理模型，在EmbodiedBench基准测试中显著优于更大规模的模型GPT-4o-mini，性能提升达21.33%；同时，其性能也超过了在Qwen2.5-VL-7B上训练的其他方法，提升幅度达20.33%。</p>
<h3 id="RoboMemory-A-Brain-inspired-Multi-memory-Agentic-Framework-for-Interactive-Environmental-Learning-in-Physical-Embodied-Systems"><a href="#RoboMemory-A-Brain-inspired-Multi-memory-Agentic-Framework-for-Interactive-Environmental-Learning-in-Physical-Embodied-Systems" class="headerlink" title="RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Interactive Environmental Learning in Physical Embodied Systems"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.01415v5">RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Interactive Environmental Learning in Physical Embodied Systems</a></h3><p><strong>Categories:</strong> Robotics, Reinforcement Learning, Vision-Language Models</p>
<p>具身智能体在现实环境中的应用面临着持续的挑战，包括部分可观测性、有限的空间推理能力和高延迟的多记忆集成。我们提出了RoboMemory，这是一个受大脑启发的框架，通过并行化架构统一了空间记忆、时间记忆、事件记忆和语义记忆，从而实现高效的长周期规划和交互式环境学习。一个动态空间知识图谱（KG）确保了可扩展且一致的记忆更新，而配备批评模块的闭环规划器则支持在动态环境中进行自适应决策。在EmbodiedBench上的实验表明，基于Qwen2.5-VL-72B-Ins构建的RoboMemory相比其基准模型，平均成功率提高了25%，并且在闭源的最先进模型Gemini-1.5-Pro之上提升了3%。现实世界测试进一步验证了其累积学习的能力，其性能在重复任务中持续提升。这些结果凸显了RoboMemory作为增强型具身智能可扩展基础的重要性，弥合了认知神经科学与机器人自主性之间的鸿沟。</p>
<h3 id="Using-Non-Expert-Data-to-Robustify-Imitation-Learning-via-Offline-Reinforcement-Learning"><a href="#Using-Non-Expert-Data-to-Robustify-Imitation-Learning-via-Offline-Reinforcement-Learning" class="headerlink" title="Using Non-Expert Data to Robustify Imitation Learning via Offline Reinforcement Learning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19495v1">Using Non-Expert Data to Robustify Imitation Learning via Offline Reinforcement Learning</a></h3><p><strong>Categories:</strong> Robotics,Reinforcement Learning</p>
<p>模仿学习已被证明在通过专家人类演示来训练机器人执行复杂任务方面是有效的。然而，它仍受到对高质量、任务特定数据的依赖的限制，从而限制了其对现实世界中多样化物体配置和场景的适应性。相比之下，非专家数据——如玩耍数据、次优演示、部分任务完成或次优策略的运行结果——可以提供更广泛的覆盖范围和更低的数据收集成本。然而，传统的模仿学习方法未能有效利用这些数据。为了解决这些挑战，我们认为，通过恰当的设计决策，离线强化学习可以作为一种工具，用于利用非专家数据来提升模仿学习策略的性能。我们表明，尽管标准的离线强化学习方法在现实世界中通常遇到的稀疏数据覆盖设置下可能无法有效利用非专家数据，但简单的算法修改可以使这些数据得到利用，而不需要显著的额外假设。我们的方法表明，扩展策略分布的支持范围可以使得结合离线强化学习的模仿算法在任务中实现稳健的性能，表现出显著增强的恢复和泛化能力。在操作任务中，这些创新显著提高了在引入非专家数据后学习策略成功的情况范围。此外，我们还表明，这些方法能够利用所有收集到的数据，包括部分或次优的演示，以增强任务导向策略的性能。这凸显了在机器人领域中使用非专家数据进行稳健策略学习的算法技术的重要性。</p>
<h3 id="NeSyPr-Neurosymbolic-Proceduralization-For-Efficient-Embodied-Reasoning"><a href="#NeSyPr-Neurosymbolic-Proceduralization-For-Efficient-Embodied-Reasoning" class="headerlink" title="NeSyPr: Neurosymbolic Proceduralization For Efficient Embodied Reasoning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19429v1">NeSyPr: Neurosymbolic Proceduralization For Efficient Embodied Reasoning</a></h3><p><strong>Categories:</strong> Robotics</p>
<p>我们探讨了在动态环境中采用语言模型（LM）来执行具身任务所面临的挑战，其中由于延迟、连接性和资源限制，无法在线访问大规模推理引擎或符号规划器。为此，我们提出了NeSyPr，这是一种新颖的具身推理框架，通过神经符号程序化编译知识，从而赋予基于语言模型的智能体结构化、自适应且及时的推理能力。在NeSyPr中，首先通过一个符号工具利用其声明性知识显式生成任务特定的计划。这些计划随后被转换为可组合的程序表示形式，编码了计划的隐含产生规则，使得生成的组合过程能够无缝地融入语言模型的推理过程中。这种神经符号程序化方法将多步骤的符号结构化路径寻找和推理抽象并泛化为单步语言模型推理，类似于人类的知识编译过程。它支持高效的测试时推理，无需依赖外部符号指导，因此非常适合部署在对延迟敏感且资源受限的物理系统中。我们在具身基准PDDLGym、VirtualHome和ALFWorld上评估了NeSyPr，展示了其在大规模推理模型和符号规划器上的高效推理能力，同时使用更紧凑的语言模型。</p>
<h3 id="Open-World-Drone-Active-Tracking-with-Goal-Centered-Rewards"><a href="#Open-World-Drone-Active-Tracking-with-Goal-Centered-Rewards" class="headerlink" title="Open-World Drone Active Tracking with Goal-Centered Rewards"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2412.00744v2">Open-World Drone Active Tracking with Goal-Centered Rewards</a></h3><p><strong>Categories:</strong> Robotics,Reinforcement Learning,World Models</p>
<p>无人机视觉主动跟踪旨在通过基于视觉观测控制运动系统，自主跟随目标物体，为动态环境中的有效跟踪提供更实用的解决方案。然而，由于缺乏统一的基准测试和开放世界环境中频繁干扰的复杂性，使用强化学习实现精确的无人机视觉主动跟踪仍面临挑战。为了解决这些问题，我们率先提出了一套系统性的解决方案。首先，我们提出了DAT（开放世界无人机主动空地跟踪基准），这是首个包含24个城市规模场景的基准测试数据集，这些场景中的目标具有类似人类的行为特征，并且具备高保真度的动力学模拟。DAT还提供了一个数字孪生工具，用于无限生成场景。此外，我们提出了一种名为GC-VAT的新颖强化学习方法，旨在提升无人机在复杂场景中跟踪目标的性能。具体而言，我们设计了一种以目标为中心的奖励机制，为智能体提供跨视角的精确反馈，使其能够通过无限制的视角扩展感知和运动范围。受课程学习的启发，我们引入了一种基于课程的训练策略，逐步提升复杂环境下的跟踪性能。此外，我们在模拟器和真实世界图像上的实验表明，GC-VAT表现出卓越的性能，在模拟器上实现了约72%的跟踪成功率。该基准测试和代码已开源，可在 <a target="_blank" rel="noopener" href="https://github.com/SHWplus/DAT_Benchmark">https://github.com/SHWplus/DAT_Benchmark</a> 获取。</p>
<h2 id="Vision-Language-Models"><a href="#Vision-Language-Models" class="headerlink" title="Vision-Language Models"></a>Vision-Language Models</h2><h3 id="Context-Aware-Pseudo-Label-Scoring-for-Zero-Shot-Video-Summarization"><a href="#Context-Aware-Pseudo-Label-Scoring-for-Zero-Shot-Video-Summarization" class="headerlink" title="Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.17501v3">Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>我们提出了一种基于评分标准引导、伪标签生成和提示驱动的零样本视频摘要框架，该框架将大型语言模型与结构化语义推理相结合。一小部分人类标注数据被转换为高置信度的伪标签，并根据数据集特性构建适应性的评分标准，定义明确的评估维度，例如主题相关性、动作细节和叙事连贯性。在推理过程中，边界场景（包括开头和结尾部分）根据其自身的描述独立评分，而中间场景则结合相邻片段的简洁摘要来评估叙事连贯性和冗余程度。这种设计使语言模型能够在不进行任何参数调优的情况下，平衡局部显著性和全局一致性。在三个基准数据集上，所提出的方法取得了稳定且具有竞争力的结果，在SumMe、TVSum和QFVS数据集上的F1分数分别为57.58、63.05和53.79，分别超越了零样本基线方法0.85、0.84和0.37个百分点。这些结果表明，结合评分标准引导的伪标签生成和上下文提示的方法，能够有效稳定基于大型语言模型的评分，并为通用型和查询聚焦型视频摘要建立一种通用、可解释且无需训练的范式。</p>
<h3 id="ModServe-Modality-and-Stage-Aware-Resource-Disaggregation-for-Scalable-Multimodal-Model-Serving"><a href="#ModServe-Modality-and-Stage-Aware-Resource-Disaggregation-for-Scalable-Multimodal-Model-Serving" class="headerlink" title="ModServe: Modality- and Stage-Aware Resource Disaggregation for Scalable Multimodal Model Serving"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2502.00937v3">ModServe: Modality- and Stage-Aware Resource Disaggregation for Scalable Multimodal Model Serving</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>大型多模态模型（LMMs）在理解图像、视频和音频等方面展现出超越文本的出色能力。然而，由于其复杂的架构和多阶段推理流程中的异构特性，将LMMs高效地部署到生产环境中面临显著挑战。我们首次对两种主流的LMM架构——仅解码器和交叉注意力架构，在六个代表性开源模型中进行了全面的系统分析，揭示了关键的系统设计启示。我们还深入分析了生产环境中的LMM推理轨迹，发现了独特的负载特征，包括可变的、重尾请求分布以及突发的流量模式。基于这些洞察，我们提出了ModServe，一个模块化的LMM服务系统，它将各个阶段解耦，以便进行独立优化和自适应扩展。ModServe通过模态感知的调度和自动扩展动态重新配置各个阶段，以应对突发流量，在满足尾部延迟服务等级协议（SLO）的同时最小化成本。在128-GPU集群上使用生产轨迹进行测试，ModServe实现了3.3到5.5倍更高的吞吐量（带来25%到41.3%的成本节省），同时满足SLO要求。</p>
<h3 id="I-Spy-With-My-Model’s-Eye-Visual-Search-as-a-Behavioural-Test-for-MLLMs"><a href="#I-Spy-With-My-Model’s-Eye-Visual-Search-as-a-Behavioural-Test-for-MLLMs" class="headerlink" title="I Spy With My Model’s Eye: Visual Search as a Behavioural Test for MLLMs"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19678v1">I Spy With My Model’s Eye: Visual Search as a Behavioural Test for MLLMs</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>多模态大语言模型（MLLMs）在视觉-语言任务中表现出色，但其视觉处理过程却具有一定的不透明性。大多数黑箱评估方法主要衡量任务准确性，却对模型内部机制了解甚少。借鉴认知心理学理论，我们采用经典的视觉搜索范式——最初用于研究人类感知——来检验MLLMs是否表现出“突出效应”（pop-out effect）。该效应指的是在干扰项数量不同的情况下，显著的视觉特征能够被独立检测出来。通过针对颜色、大小和光照特征进行的控制实验，我们发现先进的MLLMs在基于颜色或大小的析取性（单特征）视觉搜索中表现出类似人类的突出效应，而在基于多个特征的联合性（多特征）搜索中则表现出一定的容量限制。此外，我们还发现证据表明，与人类类似，MLLMs在对象表征中也整合了自然场景先验知识，例如光照方向。我们通过有针对性的微调和机制可解释性分析进一步验证了这些发现。我们的研究展示了如何利用视觉搜索作为认知基础的诊断工具，以评估MLLMs的感知能力。</p>
<h3 id="AgentSense-LLMs-Empower-Generalizable-and-Explainable-Web-Based-Participatory-Urban-Sensing"><a href="#AgentSense-LLMs-Empower-Generalizable-and-Explainable-Web-Based-Participatory-Urban-Sensing" class="headerlink" title="AgentSense: LLMs Empower Generalizable and Explainable Web-Based Participatory Urban Sensing"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19661v1">AgentSense: LLMs Empower Generalizable and Explainable Web-Based Participatory Urban Sensing</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>基于网络的参与式城市感知已成为现代城市治理的重要方法，通过将移动个体作为分布式传感器来实现。然而，现有的城市感知系统在应对多样化城市场景时泛化能力有限，并且在决策过程中可解释性较差。在本文中，我们提出了AgentSense，这是一个混合型、无需训练的框架，通过多智能体进化系统将大语言模型（LLMs）集成到参与式城市感知中。AgentSense首先使用经典规划器生成基准解决方案，然后迭代优化这些方案，以适应动态的城市环境和异构工作者的偏好，同时生成自然语言解释，从而提高透明度和信任度。我们在两个大规模移动性数据集和七种类型的动态干扰上进行了广泛的实验，结果表明AgentSense在适应性和可解释性方面相较于传统方法具有显著优势。此外，与单一智能体LLM基线方法相比，我们的方法在性能和鲁棒性方面均表现更优，并且能够提供更合理、更透明的解释。这些结果表明，AgentSense是向部署适应性强且可解释的城市感知系统迈出的重要一步。</p>
<h3 id="Learning-from-Videos-for-3D-World-Enhancing-MLLMs-with-3D-Vision-Geometry-Priors"><a href="#Learning-from-Videos-for-3D-World-Enhancing-MLLMs-with-3D-Vision-Geometry-Priors" class="headerlink" title="Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision Geometry Priors"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.24625v3">Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision Geometry Priors</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>此前的研究已经探讨了将多模态大语言模型（MLLMs）应用于理解3D场景的方法，即将其视为视频进行处理。这些方法通常依赖于全面的3D数据输入，例如点云或重建的鸟瞰图（BEV）地图。在我们的研究中，我们通过增强MLLMs直接从视频数据中理解并推理3D空间的能力，推动了这一领域的发展，而无需额外的3D输入。我们提出了一种新颖且高效的方法，称为视频-3D几何大语言模型（Video-3D Geometry Large Language Model，简称VG LLM）。我们的方法利用3D视觉几何编码器，从视频序列中提取3D先验信息，然后将这些信息与视觉标记结合，并输入到MLLM中。大量实验表明，我们的方法在与3D场景理解及空间推理相关的各种任务中取得了显著的提升，所有这些能力都是直接从视频来源中学习得到的。令人印象深刻的是，我们的4B模型在不依赖显式3D数据输入的情况下，其表现与现有的最先进方法相当，甚至在VSI-Bench评估中超过了Gemini-1.5-Pro。</p>
<h3 id="XBench-A-Comprehensive-Benchmark-for-Visual-Language-Explanations-in-Chest-Radiography"><a href="#XBench-A-Comprehensive-Benchmark-for-Visual-Language-Explanations-in-Chest-Radiography" class="headerlink" title="XBench: A Comprehensive Benchmark for Visual-Language Explanations in Chest Radiography"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19599v1">XBench: A Comprehensive Benchmark for Visual-Language Explanations in Chest Radiography</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>视觉-语言模型（VLMs）最近在医学图像理解方面展示了显著的零样本性能，但它们的“锚定”能力，即文本概念与视觉证据的一致程度，仍被严重忽视。然而，在医学领域，可靠的锚定对于可解释性和临床应用至关重要。在本研究中，我们提出了首个系统性基准测试，用于评估七种CLIP风格VLM变体在胸部X光图像中的跨模态可解释性。我们利用交叉注意力和基于相似性的定位图生成视觉解释，并定量评估其与放射科医生标注区域在多种病理情况下的匹配程度。我们的分析表明：（1）虽然所有VLM变体在大而明确的病变上都表现出合理的定位能力，但在小或弥漫性病变上其性能显著下降；（2）在胸部X光专用数据集上预训练的模型，其与通用领域数据训练模型相比表现出更好的对齐效果；（3）模型的整体识别能力和锚定能力之间存在强相关性。这些发现表明，尽管当前VLMs具有强大的识别能力，但其在临床可靠锚定方面仍存在不足，强调在部署到医学实践中之前，需要建立有针对性的可解释性基准。XBench代码可在<a target="_blank" rel="noopener" href="https://github.com/Roypic/Benchmarkingattention%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Roypic/Benchmarkingattention获取。</a></p>
<h3 id="Detecting-Latin-in-Historical-Books-with-Large-Language-Models-A-Multimodal-Benchmark"><a href="#Detecting-Latin-in-Historical-Books-with-Large-Language-Models-A-Multimodal-Benchmark" class="headerlink" title="Detecting Latin in Historical Books with Large Language Models: A Multimodal Benchmark"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19585v1">Detecting Latin in Historical Books with Large Language Models: A Multimodal Benchmark</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>本文提出了一种新颖的任务，即从具有多种排版形式的混合语言历史文献中提取拉丁文片段。我们基于包含724页标注数据的多模态数据集，对大型基础模型的性能进行了基准测试和评估。实验结果表明，使用现代模型可以实现可靠的拉丁文检测。我们的研究首次对该任务中这些模型的能力和局限性进行了全面分析。</p>
<h3 id="DAIL-Beyond-Task-Ambiguity-for-Language-Conditioned-Reinforcement-Learning"><a href="#DAIL-Beyond-Task-Ambiguity-for-Language-Conditioned-Reinforcement-Learning" class="headerlink" title="DAIL: Beyond Task Ambiguity for Language-Conditioned Reinforcement Learning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19562v1">DAIL: Beyond Task Ambiguity for Language-Conditioned Reinforcement Learning</a></h3><p><strong>Categories:</strong> Vision-Language Models, Reinforcement Learning</p>
<p>理解自然语言并遵循人类指令是智能代理的关键能力。然而，语言指令的灵活性在语言条件任务中引入了大量歧义，严重降低了算法性能。为了解决这些限制，我们提出了一种名为DAIL（分布对齐学习）的新方法，其包含两个关键组成部分：分布策略和语义对齐。具体而言，我们提供了理论结果，证明了价值分布估计机制能够增强任务的可区分性。同时，语义对齐模块捕捉了轨迹与语言指令之间的对应关系。在结构化和视觉观察基准上的大量实验结果表明，DAIL有效解决了指令歧义问题，其性能优于基线方法。我们的实现可在<a target="_blank" rel="noopener" href="https://github.com/RunpengXie/Distributional-Aligned-Learning%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/RunpengXie/Distributional-Aligned-Learning上获取。</a></p>
<h3 id="A-Matter-of-Time-Revealing-the-Structure-of-Time-in-Vision-Language-Models"><a href="#A-Matter-of-Time-Revealing-the-Structure-of-Time-in-Vision-Language-Models" class="headerlink" title="A Matter of Time: Revealing the Structure of Time in Vision-Language Models"></a><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19559v1">A Matter of Time: Revealing the Structure of Time in Vision-Language Models</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>像CLIP这样的大规模视觉-语言模型（VLMs）因其通用且表达力强的多模态表示而受到欢迎。通过利用包含多样文本元数据的大规模训练数据，VLMs获得了开放词汇能力，从而能够解决超出其训练范围的任务。本文研究了VLMs的时间感知能力，评估它们将视觉内容定位在时间上的能力。我们引入了TIME10k，这是一个包含超过10,000张图像且具有时间真实值的基准数据集，并通过一种新颖的方法评估了37个VLMs的时间感知能力。我们的研究发现，时间信息在VLM嵌入空间中沿着一个低维、非线性的流形结构分布。基于这一发现，我们提出了一种从嵌入空间中推导出显式“时间线”表示的方法。这些表示建模了时间及其时间顺序，从而有助于进行时间推理任务。我们的时间线方法在计算效率方面优于基于提示的基线方法，且在准确性上具有竞争力甚至更优。所有代码和数据均可在<a target="_blank" rel="noopener" href="https://tekayanidham.github.io/timeline-page/">https://tekayanidham.github.io/timeline-page/</a> 获取。</p>
<h3 id="CARES-Context-Aware-Resolution-Selector-for-VLMs"><a href="#CARES-Context-Aware-Resolution-Selector-for-VLMs" class="headerlink" title="CARES: Context-Aware Resolution Selector for VLMs"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19496v1">CARES: Context-Aware Resolution Selector for VLMs</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>大型视觉-语言模型（VLMs）通常以原生或高分辨率处理图像，以保持在各种任务中的有效性。这使得视觉标记通常占总标记的97%-99%，即使低分辨率图像就足够时，也会导致计算量和延迟很高。我们引入了CARES——一个上下文感知的分辨率选择器（Context-Aware Resolution Selector），这是一个轻量级的预处理模块。给定图像和查询对，CARES可以预测最小的足够输入分辨率。CARES使用一个紧凑型的VLM（3.5亿参数）来提取特征，并预测目标预训练VLM的响应何时会收敛到其正确回答的最大能力。尽管CARES是在一组可选分辨率上训练的离散分类器，但在推理过程中，它会对连续分辨率进行插值，以实现更精细的控制。在涵盖文档和自然图像的五个多模态基准测试中，以及多种不同的目标VLMs上，CARES在保持任务性能的同时，计算量减少了高达80%。</p>
<h3 id="EgoBlind-Towards-Egocentric-Visual-Assistance-for-the-Blind"><a href="#EgoBlind-Towards-Egocentric-Visual-Assistance-for-the-Blind" class="headerlink" title="EgoBlind: Towards Egocentric Visual Assistance for the Blind"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2503.08221v3">EgoBlind: Towards Egocentric Visual Assistance for the Blind</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>我们提出了EgoBlind，这是首个从盲人个体日常生活中收集的以第一视角拍摄的视频问答（VideoQA）数据集，用于评估当前多模态大语言模型（MLLMs）在辅助功能方面的表现。EgoBlind包含1,392段来自盲人和视觉障碍人士日常生活的第一视角视频。该数据集还包含5,311个问题，这些问题直接由盲人提出或验证，以反映他们在现实场景中对视觉辅助的需求。每个问题平均有3个经过人工标注的参考答案，以减少主观性。通过EgoBlind，我们对16个先进的MLLMs进行了全面评估，发现所有模型的表现都不理想。表现最好的模型准确率接近60%，远低于人类87.4%的水平。为了指导未来的发展，我们识别并总结了现有MLLMs在盲人以第一视角视觉辅助方面的主要局限性，并探索了改进的启发式解决方案。通过这些努力，我们希望EgoBlind能够成为开发有效AI助手的基础，从而提升盲人和视觉障碍人士的独立性。数据和代码可在<a target="_blank" rel="noopener" href="https://github.com/doc-doc/EgoBlind%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/doc-doc/EgoBlind获取。</a></p>
<h3 id="MUG-V-10B-High-efficiency-Training-Pipeline-for-Large-Video-Generation-Models"><a href="#MUG-V-10B-High-efficiency-Training-Pipeline-for-Large-Video-Generation-Models" class="headerlink" title="MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.17519v2">MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>近年来，用于视觉内容（例如图像、视频和3D对象&#x2F;场景）的大规模生成模型取得了显著进展。然而，由于跨模态的文本-视频对齐、涉及的长序列以及复杂的时空依赖关系，训练大规模视频生成模型仍然特别具有挑战性且资源消耗巨大。为了解决这些挑战，我们提出了一种训练框架，优化了四个核心方面：(i) 数据处理，(ii) 模型架构，(iii) 训练策略，以及 (iv) 大规模视频生成模型的基础设施。这些优化在数据预处理、视频压缩、参数扩展、基于课程的预训练以及以对齐为中心的后训练等所有阶段都带来了显著的效率提升和性能改进。我们的最终模型MUG-V 10B在整体上与最新的视频生成模型表现相当，并在面向电商的视频生成任务中，通过人类评估超越了领先的开源基线模型。更重要的是，我们开源了完整的工具链，包括模型权重、基于Megatron-Core的大规模训练代码以及视频生成和增强的推理流程。据我们所知，这是首个利用Megatron-Core实现高训练效率和近线性多节点扩展的大规模视频生成训练代码的公开发布，详细信息请参见<a target="_blank" rel="noopener" href="https://github.com/Shopee-MUG/MUG-V%E3%80%82">https://github.com/Shopee-MUG/MUG-V。</a></p>
<h3 id="Efficient-Vision-Language-Action-Models-for-Embodied-Manipulation-A-Systematic-Survey"><a href="#Efficient-Vision-Language-Action-Models-for-Embodied-Manipulation-A-Systematic-Survey" class="headerlink" title="Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.17111v2">Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey</a></h3><p><strong>Categories:</strong> Vision-Language Models, Robotics</p>
<p>视觉-语言-动作（Vision-Language-Action，VLA）模型通过将自然语言指令和视觉观察映射为机器人动作，将视觉-语言模型扩展至具身控制。尽管VLA模型具有强大的能力，但由于其巨大的计算和内存需求，VLA系统面临着显著的挑战，这与需要实时性能的边缘平台（如车载移动机械臂）的限制相冲突。解决这一矛盾已成为近期研究的核心重点。鉴于当前针对更高效、可扩展的VLA系统的努力不断增加，本文对提升VLA效率的方法进行了系统性综述，重点强调降低延迟、内存占用以及训练和推理成本。我们将现有解决方案分为四个维度：模型架构、感知特征、动作生成以及训练&#x2F;推理策略，并对每个类别中的代表性技术进行了总结。最后，我们讨论了未来趋势和开放性挑战，指明了推进高效具身智能的发展方向。</p>
<h3 id="Training-Free-Label-Space-Alignment-for-Universal-Domain-Adaptation"><a href="#Training-Free-Label-Space-Alignment-for-Universal-Domain-Adaptation" class="headerlink" title="Training-Free Label Space Alignment for Universal Domain Adaptation"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2509.17452v2">Training-Free Label Space Alignment for Universal Domain Adaptation</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>通用领域自适应（Universal Domain Adaptation，简称 UniDA）是从有标签的源域向无标签的目标域迁移知识，其中标签空间可能不同，目标域可能包含私有类别。以往的 UniDA 方法主要关注视觉空间对齐，但由于内容差异，常常难以处理视觉歧义，这限制了其鲁棒性和泛化能力。为了解决这个问题，我们提出了一种新的方法，利用最近视觉-语言基础模型（如 CLIP）强大的<strong>零样本能力</strong>，专注于标签空间对齐，以提高自适应的稳定性。CLIP 可以仅根据标签名称生成任务特定的分类器。然而，将 CLIP 适配到 UniDA 中具有挑战性，因为标签空间通常不是提前完全已知的。</p>
<p>在本研究中，我们首先利用生成式视觉-语言模型来识别目标域中的未知类别。在发现的标签中，噪声和语义歧义（例如与源域标签相似的类别，如同义词、上义词、下义词等）会使得标签对齐变得复杂。为了解决这一问题，我们提出了一种无需训练的 UniDA 标签空间对齐方法（\ours）。我们的方法通过在域之间过滤和精炼噪声标签，直接对齐标签空间，而非视觉空间。随后，我们构建了一个<strong>通用分类器</strong>，它整合了共享知识和目标私有类别信息，从而在领域变化下提高泛化能力。实验结果表明，所提出的方法在关键的 DomainBed 基准测试中显著优于现有 UniDA 技术，H-score 平均提升了 \textcolor{blue}{+7.9%}，H$^3$-score 平均提升了 \textcolor{blue}{+6.1%}。此外，结合自训练进一步提升了性能，在 H-score 和 H$^3$-score 上分别再提升 \textcolor{blue}{+1.6%}。</p>
<h3 id="With-Limited-Data-for-Multimodal-Alignment-Let-the-STRUCTURE-Guide-You"><a href="#With-Limited-Data-for-Multimodal-Alignment-Let-the-STRUCTURE-Guide-You" class="headerlink" title="With Limited Data for Multimodal Alignment, Let the STRUCTURE Guide You"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2506.16895v2">With Limited Data for Multimodal Alignment, Let the STRUCTURE Guide You</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>多模态模型在需要多模态对齐的复杂任务中表现出强大的能力，包括零样本分类和跨模态检索。然而，现有的模型通常依赖数百万对多模态样本，而在许多领域中，获取这些样本的成本过高或不可行。在本工作中，我们探索了在仅有少量成对数据的情况下构建多模态模型的可行性，方法是通过对齐预训练的单模态基础模型。我们证明，仅需数万个成对样本（通常用于该领域的数据的不到1%）即可实现高质量的对齐。为了达到这一目标，我们引入了STRUCTURE，这是一种有效的正则化技术，能够保留单模态编码器潜在空间的邻域几何结构。此外，我们还表明，对齐最后一层通常是次优的，并展示了在跨模态中具有最高表示相似性的层进行对齐的优势。这两个组件可以轻松地集成到现有的对齐方法中，在24个零样本图像分类和检索基准测试中取得了显著的提升，分类任务的平均相对提升为51.6%，检索任务的平均相对提升为91.8%。我们的结果突显了我们框架在小样本多模态学习中的有效性和广泛适用性，并为资源受限领域提供了一条有前景的发展路径。</p>
<h3 id="Towards-Enhanced-Image-Generation-Via-Multi-modal-Chain-of-Thought-in-Unified-Generative-Models"><a href="#Towards-Enhanced-Image-Generation-Via-Multi-modal-Chain-of-Thought-in-Unified-Generative-Models" class="headerlink" title="Towards Enhanced Image Generation Via Multi-modal Chain of Thought in Unified Generative Models"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2503.01298v2">Towards Enhanced Image Generation Via Multi-modal Chain of Thought in Unified Generative Models</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>统一生成模型在文本和图像生成任务中表现出色。对于图像合成任务，它们通常采用直接的文本到图像（T2I）生成方法。然而，直接的T2I生成方法限制了模型处理复杂组合指令的能力，而这类指令在现实场景中非常常见。尽管这一问题非常重要，但现有研究主要集中在提升模型的基本图像生成能力上。虽然这些改进在一定程度上有帮助，但仍无法充分解决该问题。受“思维链”（Chain of Thought, CoT）逐步解决复杂问题的启发，本研究旨在将CoT引入统一生成模型，以应对直接T2I生成方法无法有效解决的复杂图像生成挑战，从而增强模型的图像生成能力。为此，我们首先提出了功能导向专家（Functionality-oriented eXperts，FoXperts），这是我们在模型FoX中的专家并行架构，根据功能分配专家。FoXperts分离了主流模态导向设计中潜在的冲突，并为CoT提供了坚实的基础。在引入CoT时，首要问题是：如何为复杂的图像生成设计CoT。为此，我们模拟了类似人类的艺术创作流程——规划、执行、反思和修正，并提出了多模态思维链（Multimodal Chain of Thought, MCoT）方法，因为数据同时涉及文本和图像。为了解决后续挑战——设计有效的MCoT训练范式，我们开发了一种多任务联合训练方案，以解耦的方式为模型赋予完成每个MCoT步骤所需的所有能力。这种范式避免了收集一致的多步骤数据元组的困难。大量实验表明，FoX在各种T2I基准测试中持续优于现有的统一模型，在复杂图像生成方面取得了显著的改进。</p>
<h3 id="Merge-then-Realign-Simple-and-Effective-Modality-Incremental-Continual-Learning-for-Multimodal-LLMs"><a href="#Merge-then-Realign-Simple-and-Effective-Modality-Incremental-Continual-Learning-for-Multimodal-LLMs" class="headerlink" title="Merge then Realign: Simple and Effective Modality-Incremental Continual Learning for Multimodal LLMs"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2503.07663v2">Merge then Realign: Simple and Effective Modality-Incremental Continual Learning for Multimodal LLMs</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>近年来，多模态大语言模型（Multimodal Large Language Models，MLLMs）在整合越来越多的模态后，其多功能性得到了显著增强。考虑到训练MLLMs的成本较高，通过模态增量持续学习（Modality-incremental Continual Learning，MCL）重用现有模型并扩展至更多模态是一种高效的方法。目前，MCL的研究仍处于早期阶段。在本工作中，我们深入探讨了MCL中性能下降的原因。我们发现，MCL不仅会像传统持续学习一样出现遗忘问题，还存在模态无关组件与模态特定组件之间不匹配的问题。为此，我们提出了一种名为“先合并后重新对齐”（MErge then ReAlign，MERA）的优雅且简洁的MCL范式，以同时解决遗忘和不匹配问题。MERA无需引入额外的模型预算或修改模型结构，因此易于部署且在MLLM社区中具有高度的可重用性。大量实验表明，MERA表现出色，在扩展至四种模态时，其平均向后相对增益达到99.84%，实现了几乎无损的MCL性能。我们的研究突显了MCL中的不匹配问题。更广泛地看，我们的工作展示了如何在持续学习过程中调整MLLMs的不同组件。</p>
<h3 id="PULSE-Practical-Evaluation-Scenarios-for-Large-Multimodal-Model-Unlearning"><a href="#PULSE-Practical-Evaluation-Scenarios-for-Large-Multimodal-Model-Unlearning" class="headerlink" title="PULSE: Practical Evaluation Scenarios for Large Multimodal Model Unlearning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2507.01271v3">PULSE: Practical Evaluation Scenarios for Large Multimodal Model Unlearning</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>近年来，去学习技术（unlearning techniques）作为一种方法，用于促使模型“忘记”之前学习到的信息，因其在解决大型语言模型（LLMs）和大型多模态模型（LMMs）中的隐私和版权问题方面受到关注。尽管已经为LLMs建立了多个去学习基准，但对于LMMs的去学习的实用评估框架却研究较少。具体而言，现有的LMMs去学习基准仅考虑了模型通过一次去学习操作来去除微调知识的场景。在本研究中，我们引入了PULSE协议，从两个关键视角出发，为LMMs的现实去学习场景提供支持：（i）预训练知识的去学习，用于分析不同知识获取阶段的效果；（ii）长期可持续性评估，以应对连续请求。随后，我们沿这些维度评估现有的去学习方法。我们的结果表明，尽管一些技术能够成功地去除通过微调获得的知识，但它们难以消除在预训练过程中学到的信息。此外，那些能够在单次操作中有效去除一批目标数据的方法，在将相同数据分割并依次去除时，会表现出显著的性能下降。</p>
<h3 id="See-Think-Act-Online-Shopper-Behavior-Simulation-with-VLM-Agents"><a href="#See-Think-Act-Online-Shopper-Behavior-Simulation-with-VLM-Agents" class="headerlink" title="See, Think, Act: Online Shopper Behavior Simulation with VLM Agents"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19245v1">See, Think, Act: Online Shopper Behavior Simulation with VLM Agents</a></h3><p><strong>Categories:</strong> Vision-Language Models,Reinforcement Learning</p>
<p>最近，大型语言模型（LLMs）在模拟在线购物者行为方面展现出巨大的潜力。先前的工作通过在动作轨迹上应用监督微调（SFT）并结合LLM生成的推理理由，以及利用强化学习（RL）进一步增强推理能力，提升了动作预测的效果。尽管取得了这些进展，当前的方法仍然依赖于基于文本的输入，忽视了视觉感知在网页GUI交互过程中对人类决策的关键作用。在本文中，我们研究了通过视觉语言模型（VLMs）将视觉信息，特别是网页截图，整合到行为模拟中，利用OPERA数据集进行探索。通过将智能体的决策过程同时基于文本和视觉模态进行锚定，我们旨在缩小合成智能体与真实用户之间的差距，从而实现更符合人类认知的在线购物行为模拟。具体而言，我们采用SFT进行联合动作预测和推理理由生成，并基于完整的交互上下文进行条件建模，该上下文包括动作历史、过去的HTML观察以及当前网页截图。为进一步增强推理能力，我们将强化学习与分层奖励结构相结合，并通过一个难度感知因子进行缩放，优先处理具有挑战性的决策点。实证研究表明，引入视觉锚定带来了显著的提升：结合文本和图像输入的方法，相比仅使用文本输入，精确匹配准确率提高了超过6%。这些结果表明，多模态锚定不仅提升了预测的准确性，还在视觉复杂环境中增强了模拟的保真度，从而捕捉到了仅依赖文本的智能体常常忽略的人类注意力和决策的细微差别。最后，我们重新审视行为模拟框架的设计空间，识别出关键的方法论局限性，并提出了未来研究方向，以构建高效且有效的人类行为模拟器。</p>
<h3 id="Grasp-Any-Region-Towards-Precise-Contextual-Pixel-Understanding-for-Multimodal-LLMs"><a href="#Grasp-Any-Region-Towards-Precise-Contextual-Pixel-Understanding-for-Multimodal-LLMs" class="headerlink" title="Grasp Any Region: Towards Precise, Contextual Pixel Understanding for Multimodal LLMs"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18876v2">Grasp Any Region: Towards Precise, Contextual Pixel Understanding for Multimodal LLMs</a></h3><p><strong>Categories:</strong> Vision-Language Models, World Models</p>
<p>尽管多模态大语言模型（MLLMs）在整体理解方面表现出色，但在捕捉复杂场景中的密集世界时却存在困难，需要对精细的细节和物体间的相互关系进行细致分析。区域级的MLLMs已成为一个有前景的突破。然而，以往的研究大多专注于孤立理解给定区域，忽略了关键的全局上下文。为了解决这一问题，我们提出了Grasp Any Region（GAR），以实现全面的区域级视觉理解。通过一种有效的RoI对齐特征重放技术，GAR支持以下三个关键能力：（1）利用必要的全局上下文实现精确感知；（2）建模多个提示之间的交互；（3）实现先进的组合推理能力，以回答任何区域的特定自由形式问题，从而将范式从被动描述转变为积极对话。此外，我们构建了GAR-Bench，它不仅提供了对单区域理解更准确的评估，更重要的是，还能衡量多个区域之间的交互和复杂推理。大量实验表明，GAR-1B不仅保持了最先进的描述能力，例如在DLC-Bench上优于DAM-3B +4.5，而且在建模多个提示之间的关系方面表现出色，甚至在GAR-Bench-VQA上超越了InternVL3-78B。更重要的是，我们的零样本GAR-8B在VideoRefer-BenchQ上甚至优于同领域的VideoRefer-7B，表明其强大的能力可以轻松迁移到视频领域。</p>
<h3 id="Chiron-o1-Igniting-Multimodal-Large-Language-Models-towards-Generalizable-Medical-Reasoning-via-Mentor-Intern-Collaborative-Search"><a href="#Chiron-o1-Igniting-Multimodal-Large-Language-Models-towards-Generalizable-Medical-Reasoning-via-Mentor-Intern-Collaborative-Search" class="headerlink" title="Chiron-o1: Igniting Multimodal Large Language Models towards Generalizable Medical Reasoning via Mentor-Intern Collaborative Search"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2506.16962v2">Chiron-o1: Igniting Multimodal Large Language Models towards Generalizable Medical Reasoning via Mentor-Intern Collaborative Search</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>多模态大语言模型（MLLMs）已经开始在通用任务中展现出强大的推理能力，但其在医疗领域的应用仍处于初级阶段。构建链式推理（CoT）训练数据对于增强医疗MLLMs的推理能力至关重要。然而，现有方法在提供全面的搜索和评估有效推理路径的框架方面存在不足，特别是在关键诊断中的推理路径搜索方面。为了解决这一挑战，我们提出了Mentor-Intern协作搜索（MICS），这是一种新颖的推理路径搜索方案，用于生成严谨且有效的医疗CoT数据。MICS首先利用导师模型逐步初始化推理路径，然后提示每个实习生模型沿着这些初始化路径继续推理，最后根据多个实习生模型的整体推理性能选择最优的推理路径。推理性能由MICS-Score评估，该指标用于评估生成推理路径的质量。最终，我们构建了MMRP，一个具有难度分级的多任务医疗推理数据集，并提出了Chiron-o1，这是通过课程学习策略设计的新一代医疗MLLM，具备强大的视觉问答能力和可泛化的推理能力。大量实验表明，使用MICS构建的CoT数据集训练的Chiron-o1，在一系列医疗视觉问答和推理基准测试中均达到了最先进的性能。代码可在<a target="_blank" rel="noopener" href="https://github.com/manglu097/Chiron-o1%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/manglu097/Chiron-o1获取。</a></p>
<h3 id="Probing-Perceptual-Constancy-in-Large-Vision-Language-Models"><a href="#Probing-Perceptual-Constancy-in-Large-Vision-Language-Models" class="headerlink" title="Probing Perceptual Constancy in Large Vision-Language Models"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2502.10273v2">Probing Perceptual Constancy in Large Vision-Language Models</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>知觉恒常性是指在感觉输入发生变化（如距离、角度或光照的变化）的情况下，保持对物体稳定感知的能力。这种能力对于在动态世界中进行视觉理解至关重要。在此，我们探讨了当前视觉语言模型（Vision Language Models，VLMs）的这种能力。在本研究中，我们通过在三个领域（颜色、大小和形状恒常性）进行236项实验，评估了155个VLMs的性能。这些实验包括经典认知任务的单图像和视频版本，以及在真实环境中的新颖任务。我们发现，在这些领域中，VLMs的性能存在显著差异，其中形状恒常性的表现明显与颜色和大小恒常性表现相分离。</p>
<h3 id="PruneHal-Reducing-Hallucinations-in-Multi-modal-Large-Language-Models-through-Adaptive-KV-Cache-Pruning"><a href="#PruneHal-Reducing-Hallucinations-in-Multi-modal-Large-Language-Models-through-Adaptive-KV-Cache-Pruning" class="headerlink" title="PruneHal: Reducing Hallucinations in Multi-modal Large Language Models through Adaptive KV Cache Pruning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19183v1">PruneHal: Reducing Hallucinations in Multi-modal Large Language Models through Adaptive KV Cache Pruning</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>尽管多模态大语言模型（MLLMs）近年来取得了显著进展，但幻觉问题仍然是一个主要挑战。为了解决这一现象，现有方法要么引入额外数据进行进一步训练，要么在推理过程中结合外部或内部信息。然而，这些方法不可避免地增加了额外的计算成本。在本文中，我们观察到MLLMs中的幻觉现象与对视觉标记分配的注意力不足密切相关。特别是，冗余视觉标记会分散模型的注意力，使其无法聚焦于最具信息量的标记。因此，关键的视觉线索常常被忽视，从而加剧了幻觉的发生。基于这一观察，我们提出了一种名为<strong>PruneHal</strong>的无训练、简单而有效的技术，该技术利用自适应的KV缓存剪枝，增强模型对关键视觉信息的聚焦，从而减轻幻觉问题。据我们所知，我们是第一个在MLLMs中应用标记剪枝来缓解幻觉的团队。值得注意的是，我们的方法不需要额外训练，几乎不增加推理成本。此外，PruneHal是模型无关的，可以无缝集成到不同的解码策略中，包括专门设计用于缓解幻觉的策略。我们在四个主流MLLMs上，对多个广泛使用的幻觉评估基准进行了评估，取得了稳健且出色的结果，突显了我们方法的有效性和优越性。我们的代码将公开发布。</p>
<h3 id="Text-or-Pixels-It-Takes-Half-On-the-Token-Efficiency-of-Visual-Text-Inputs-in-Multimodal-LLMs"><a href="#Text-or-Pixels-It-Takes-Half-On-the-Token-Efficiency-of-Visual-Text-Inputs-in-Multimodal-LLMs" class="headerlink" title="Text or Pixels? It Takes Half: On the Token Efficiency of Visual Text Inputs in Multimodal LLMs"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18279v2">Text or Pixels? It Takes Half: On the Token Efficiency of Visual Text Inputs in Multimodal LLMs</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>大型语言模型（LLMs）及其多模态变体现在可以处理视觉输入，包括文本图像。这引发了一个引人入胜的问题：我们能否通过将文本输入作为图像输入，从而减少标记（token）的使用量，同时保持性能不变？在本文中，我们展示了视觉文本表示是一种实用且出人意料的有效输入压缩形式，适用于解码器语言模型。我们利用了将长文本输入渲染为单个图像并直接提供给模型的思路。这种方法大大减少了所需解码器标记的数量，提供了一种新的输入压缩形式。通过在两个不同的基准测试——RULER（长上下文检索）和CNN&#x2F;DailyMail（文档摘要）上的实验，我们证明了这种将文本作为图像的方法能够带来显著的标记节省（通常接近一半），而不会影响任务性能。</p>
<h3 id="FairGen-Controlling-Sensitive-Attributes-for-Fair-Generations-in-Diffusion-Models-via-Adaptive-Latent-Guidance"><a href="#FairGen-Controlling-Sensitive-Attributes-for-Fair-Generations-in-Diffusion-Models-via-Adaptive-Latent-Guidance" class="headerlink" title="FairGen: Controlling Sensitive Attributes for Fair Generations in Diffusion Models via Adaptive Latent Guidance"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2503.01872v2">FairGen: Controlling Sensitive Attributes for Fair Generations in Diffusion Models via Adaptive Latent Guidance</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>文本到图像的扩散模型通常会对特定的人口群体表现出偏见，例如在被提示生成工程师图像时，会生成更多男性而非女性的图像，这引发了伦理问题并限制了其应用。在本文中，我们致力于解决在扩散模型中减轻对任何目标属性值（例如“性别”中的“男性”）的生成偏见的挑战，同时保持生成质量。我们提出了一种名为FairGen的自适应潜在引导机制，用于在推理过程中控制生成分布。在FairGen中，一个潜在引导模块动态调整扩散过程以强制执行特定属性，而一个记忆模块则跟踪生成统计信息，并引导潜在引导以与目标属性值的公平分布对齐。此外，我们通过引入全面偏见评估（Holistic Bias Evaluation，HBE）基准来解决现有数据集的局限性，该基准涵盖多样化的领域，并结合复杂的提示以更全面地评估偏见。在HBE和Stable Bias数据集上的大量评估表明，FairGen优于现有的偏见缓解方法，实现了显著的偏见减少（例如在Stable Diffusion 2上实现了68.5%的性别偏见减少）。消融研究突显了FairGen能够灵活控制用户指定任何粒度的输出分布，确保适应性和针对性的偏见缓解。</p>
<h2 id="World-Models"><a href="#World-Models" class="headerlink" title="World Models"></a>World Models</h2><h3 id="Benchmarking-World-Model-Learning"><a href="#Benchmarking-World-Model-Learning" class="headerlink" title="Benchmarking World-Model Learning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19788v1">Benchmarking World-Model Learning</a></h3><p><strong>Categories:</strong> World Models, Reinforcement Learning</p>
<p>模型学习智能体应收集信息，以学习能够支持许多下游任务和推断的世界模型，例如预测未观察到的状态、估计行为的短期和长期后果、规划动作序列以及检测动态变化。当前用于学习和评估世界模型的方法偏离了这一目标：训练和评估都锚定在下一帧预测上，并且通过在相同环境中的奖励最大化来衡量成功。我们提出了WorldTest，这是一种评估模型学习智能体的协议，它将无奖励交互与在不同但相关的环境中进行评分的测试阶段分开。WorldTest是开放式的——模型应支持许多事先未知的不同任务——并且与模型表示无关，从而允许不同方法之间的比较。我们使用AutumnBench对WorldTest进行了实例化，AutumnBench包含43个交互式网格世界环境和129个任务，涵盖三个家族：遮挡帧预测、规划以及预测因果动态的变化。我们在AutumnBench上比较了517名人类参与者和三个前沿模型。我们发现人类的表现优于模型，而计算能力的扩展仅在某些环境中提高了表现，而在其他环境中则没有。WorldTest提供了一种新颖的模板——无奖励探索、衍生测试和基于行为的评分——用于评估智能体对环境动态的学习情况，而AutumnBench揭示了世界模型学习方面存在显著的提升空间。</p>
<h3 id="SparseWorld-A-Flexible-Adaptive-and-Efficient-4D-Occupancy-World-Model-Powered-by-Sparse-and-Dynamic-Queries"><a href="#SparseWorld-A-Flexible-Adaptive-and-Efficient-4D-Occupancy-World-Model-Powered-by-Sparse-and-Dynamic-Queries" class="headerlink" title="SparseWorld: A Flexible, Adaptive, and Efficient 4D Occupancy World Model Powered by Sparse and Dynamic Queries"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.17482v2">SparseWorld: A Flexible, Adaptive, and Efficient 4D Occupancy World Model Powered by Sparse and Dynamic Queries</a></h3><p><strong>Categories:</strong> World Models, Robotics</p>
<p>语义占据（Semantic occupancy）作为一种强大的表示方法，在世界模型中崭露头角，因其能够捕捉丰富的空间语义。然而，大多数现有的占据世界模型依赖于静态且固定的嵌入或网格，这在本质上限制了感知的灵活性。此外，它们对网格进行“原地分类”的方式，可能与真实场景的动态性和连续性存在潜在的不匹配。在本文中，我们提出了SparseWorld，这是一种新颖的4D占据世界模型，具有灵活性、自适应性和高效性，其核心是稀疏且动态的查询机制。我们提出了一种范围自适应感知模块（Range-Adaptive Perception module），其中可学习的查询受到自身车辆状态的调控，并通过时空关联进行增强，从而实现远距离感知。为了有效捕捉场景的动态特性，我们设计了一个状态条件预测模块（State-Conditioned Forecasting module），该模块用回归引导的公式替代基于分类的预测，从而精确地将动态查询与4D环境的连续性对齐。此外，我们专门设计了一种时序感知的自调度训练策略（Temporal-Aware Self-Scheduling training strategy），以实现平滑且高效的训练。大量实验表明，SparseWorld在感知、预测和规划任务中均取得了最先进的性能。全面的可视化分析和消融研究进一步验证了SparseWorld在灵活性、自适应性和效率方面的优势。代码可在<a target="_blank" rel="noopener" href="https://github.com/MSunDYY/SparseWorld%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/MSunDYY/SparseWorld获取。</a></p>
<h3 id="Modeling-realistic-human-behavior-using-generative-agents-in-a-multimodal-transport-system-Software-architecture-and-Application-to-Toulouse"><a href="#Modeling-realistic-human-behavior-using-generative-agents-in-a-multimodal-transport-system-Software-architecture-and-Application-to-Toulouse" class="headerlink" title="Modeling realistic human behavior using generative agents in a multimodal transport system: Software architecture and Application to Toulouse"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19497v1">Modeling realistic human behavior using generative agents in a multimodal transport system: Software architecture and Application to Toulouse</a></h3><p><strong>Categories:</strong> World Models</p>
<p>为了理解人们在出行方式选择上的行为，从而提出个性化的移动解决方案，模拟现实的人类行为仍然具有挑战性。本文提出了一种用于建模复杂多模式交通系统中现实人类移动行为的架构，并通过法国图卢兹的案例研究进行了演示。我们在一个基于代理的模拟环境中应用大型语言模型（LLMs），以捕捉真实城市环境中的决策过程。该框架集成了GAMA模拟平台与基于LLM的生成式代理，结合了通用交通数据馈送规范（GTFS）数据用于公共交通，以及OpenTripPlanner用于多模式路径规划。GAMA平台模拟了交互式的交通环境，提供可视化和动态代理互动，同时避免了从头构建模拟环境的需要。这种设计使我们能够更专注于生成式代理的开发，并评估其在交通决策过程中的表现。在模拟的一个月内，结果表明，代理不仅能够做出情境感知的交通决策，而且随着时间的推移还会形成习惯。我们得出结论，将LLMs与基于代理的模拟相结合，为推进智能交通系统和个性化的多模式移动解决方案提供了一个有前景的方向。此外，我们还讨论了该方法的一些局限性，并概述了未来的工作方向，包括扩展到更大的区域、整合实时数据以及改进记忆模型。</p>
<h3 id="Social-World-Model-Augmented-Mechanism-Design-Policy-Learning"><a href="#Social-World-Model-Augmented-Mechanism-Design-Policy-Learning" class="headerlink" title="Social World Model-Augmented Mechanism Design Policy Learning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19270v1">Social World Model-Augmented Mechanism Design Policy Learning</a></h3><p><strong>Categories:</strong> World Models, Reinforcement Learning</p>
<p>设计适应性机制以协调个体与集体利益，仍然是人工社会智能领域的一个核心挑战。现有方法在建模具有持续潜在特征（如技能、偏好）的异构智能体以及处理复杂多智能体系统动态方面往往面临困难。这些挑战进一步加剧了由于现实世界交互成本高昂而对高样本效率的迫切需求。通过学习预测环境动态，世界模型为提升异构和复杂系统中的机制设计提供了有前景的途径。在本文中，我们引入了一种名为SWM-AP（社会世界模型增强的机制设计策略学习）的新方法。该方法通过分层建模智能体的行为来学习社会世界模型，从而增强机制设计。具体而言，社会世界模型从智能体的交互轨迹中推断其特征，并学习基于特征的模型以预测智能体对部署机制的响应。机制设计策略通过与社会世界模型进行交互来收集大量训练轨迹，同时在现实世界交互过程中在线推断智能体的特征，从而进一步提升策略学习的效率。在多种应用场景（如税收政策设计、团队协作和设施选址）中的实验表明，SWM-AP在累积奖励和样本效率方面均优于现有的基于模型和无模型强化学习基线方法。</p>
<h3 id="The-Emergence-of-Complex-Behavior-in-Large-Scale-Ecological-Environments"><a href="#The-Emergence-of-Complex-Behavior-in-Large-Scale-Ecological-Environments" class="headerlink" title="The Emergence of Complex Behavior in Large-Scale Ecological Environments"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18221v2">The Emergence of Complex Behavior in Large-Scale Ecological Environments</a></h3><p><strong>Categories:</strong> World Models</p>
<p>我们探讨了物理尺度和人口规模如何影响在开放性生态环境中复杂行为的产生。在我们的设定中，智能体是无监督的，没有明确的奖励或学习目标，而是随着时间的推移，通过繁殖、突变和自然选择来进化。在行动过程中，智能体也会不断塑造其环境以及周围的种群，形成一个持续动态的生态体系。我们的目标并不是优化单一的高性能策略，而是研究在大规模种群中，由于自然竞争和环境压力，行为是如何产生并演化的。为了探索复杂行为是如何自然产生的，我们在大规模世界中进行实验，这些世界中个体智能体的数量超过60,000个，每个智能体都有其独立进化的神经网络策略。我们识别出多种涌现行为，如远距离资源开采、基于视觉的觅食以及捕食行为，这些行为是在竞争和生存压力下产生的。我们研究了感知方式和环境规模如何影响这些行为的产生，发现某些行为仅在足够大的环境和种群中出现，更大的尺度提高了行为的稳定性和一致性。虽然在进化设定下的研究已有丰富历史，但我们的规模实验结果为在计算资源充足的时代，将生态学作为机器学习工具进行探索提供了有希望的新方向。实验代码可在以下链接获取：<a target="_blank" rel="noopener" href="https://github.com/jbejjani2022/ecological-emergent-behavior%E3%80%82">https://github.com/jbejjani2022/ecological-emergent-behavior。</a></p>
<h3 id="Rethinking-Driving-World-Model-as-Synthetic-Data-Generator-for-Perception-Tasks"><a href="#Rethinking-Driving-World-Model-as-Synthetic-Data-Generator-for-Perception-Tasks" class="headerlink" title="Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19195v1">Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks</a></h3><p><strong>Categories:</strong> World Models</p>
<p>最近，驾驶世界模型领域的进展使得生成高质量的RGB视频或多模态视频成为可能。现有的方法主要关注生成质量与可控性相关的指标。然而，它们往往忽略了对下游感知任务的评估，而这些任务对于自动驾驶性能至关重要。现有方法通常采用一种训练策略，即首先在合成数据上进行预训练，然后在真实数据上进行微调，这导致训练的总轮次是仅使用真实数据的两倍。当我们把基准模型的训练轮次加倍时，合成数据带来的优势变得微乎其微。为了彻底展示合成数据的优势，我们提出了Dream4Drive，这是一种专门设计用于增强下游感知任务的新型合成数据生成框架。Dream4Drive首先将输入视频分解为多个具有3D感知能力的引导图，随后将3D资产渲染到这些引导图上。最后，通过微调驾驶世界模型，生成编辑后的、多视角的逼真视频，这些视频可用于训练下游感知模型。Dream4Drive实现了在大规模生成多视角极端场景方面的前所未有的灵活性，显著提升了自动驾驶中的极端场景感知能力。为了促进未来的研究，我们还贡献了一个大规模的3D资产数据集，名为DriveObj3D，涵盖了驾驶场景中的典型类别，支持多样化的3D感知视频编辑。我们进行了全面的实验，证明Dream4Drive在不同训练轮次下都能有效提升下游感知模型的性能。项目链接：$\href{<a target="_blank" rel="noopener" href="https://wm-research.github.io/Dream4Drive/%7D%7Bthis/">https://wm-research.github.io/Dream4Drive/}{this\</a> https\ URL}$</p>
<h2 id="Rejected-Papers"><a href="#Rejected-Papers" class="headerlink" title="Rejected Papers"></a>Rejected Papers</h2><ul>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2501.13133v2">Graph Representation Learning with Diffusion Generative Models</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.24379v3">Unlearned but Not Forgotten: Data Extraction after Exact Unlearning in LLM</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19799v1">Integrating Transparent Models, LLMs, and Practitioner-in-the-Loop: A Case of Nonprofit Program Evaluation</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19792v1">On Controlled Change: Generative AI’s Impact on Professional Authority in Journalism</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2506.08800v2">Measuring Data Science Automation: A Survey of Evaluation Tools for AI Assistants and Agents</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19779v1">AdaSPEC: Selective Knowledge Distillation for Efficient Speculative Decoders</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19771v1">Beyond Reactivity: Measuring Proactive Problem Solving in LLM Agents</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19767v1">SmartSwitch: Advancing LLM Reasoning by Overcoming Underthinking via Promoting Deeper Thought Exploration</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.08450v2">gLSTM: Mitigating Over-Squashing by Increasing Storage Capacity</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19755v1">A Survey on Cache Methods in Diffusion Models: Toward Efficient Multi-Modal Generation</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19728v1">Enabling Granular Subgroup Level Model Evaluations by Generating Synthetic Medical Time Series</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.15020v2">The Coverage Principle: How Pre-Training Enables Post-Training</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2503.18129v2">GeoBenchX: Benchmarking LLMs in Agent Solving Multistep Geospatial Tasks</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.07364v3">Base Models Know How to Reason, Thinking Models Learn When</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19698v1">RLIE: Rule Generation with Logistic Regression, Iterative Refinement, and Evaluation for Large Language Models</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19694v1">Do Prompts Reshape Representations? An Empirical Study of Prompting Effects on Embeddings</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19692v1">Toward Agentic Software Engineering Beyond Code: Framing Vision, Values, and Vocabulary</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2507.05101v2">PRING: Rethinking Protein-Protein Interaction Prediction from Pairs to Graphs</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19689v1">Serverless GPU Architecture for Enterprise HR Analytics: A Production-Scale BDaaS Implementation</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19687v1">Are Large Language Models Sensitive to the Motives Behind Communication?</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19685v1">Directive, Metacognitive or a Blend of Both? A Comparison of AI-Generated Feedback Types on Student Engagement, Confidence, and Outcomes</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.16853v2">Agentic Inequality</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.02511v2">Test-time Prompt Intervention</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2506.21374v2">Pay Attention to Small Weights</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19675v1">Study of Training Dynamics for Memory-Constrained Fine-Tuning</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19671v1">Explainable e-sports win prediction through Machine Learning classification in streaming</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.06293v2">BlockGPT: Spatio-Temporal Modelling of Rainfall via Frame-Level Autoregression</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19668v1">Unraveling Emotions with Pre-Trained Models</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19666v1">A Graph Engine for Guitar Chord-Tone Soloing Education</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2503.12730v5">TinySQL: A Progressive Text-to-SQL Dataset for Mechanistic Interpretability Research</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19641v1">Style Attack Disguise: When Fonts Become a Camouflage for Adversarial Intent</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2410.10101v3">Learning Linear Attention in Polynomial Time</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.08666v3">dInfer: An Efficient Inference Framework for Diffusion Language Models</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2410.06019v2">Unveiling Transformer Perception by Exploring Input Manifolds</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19631v1">HSCodeComp: A Realistic and Expert-level Benchmark for Deep Search Agents in Hierarchical Rule Application</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2507.05916v3">On the Effectiveness of Methods and Metrics for Explainable AI in Remote Sensing Image Scene Classification</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2502.02197v3">An Efficient Local Search Approach for Polarized Community Discovery in Signed Networks</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2509.25434v2">The Open Syndrome Definition</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2507.15268v2">IM-Chat: A Multi-agent LLM Framework Integrating Tool-Calling and Diffusion Modeling for Knowledge Transfer in Injection Molding Industry</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.16206v2">The Right to Be Remembered: Preserving Maximally Truthful Digital Memory in the Age of AI</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19600v1">Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2312.10107v3">Towards Context-Aware Domain Generalization: Understanding the Benefits and Limits of Marginal Transfer Learning</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2406.19195v3">Estimating Long-term Heterogeneous Dose-response Curve: Generalization Bound Leveraging Optimal Transport Weights</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19593v1">A Goal-Driven Survey on Root Cause Analysis</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.19955v3">MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.12829v2">Mathematics with large language models as provers and verifiers</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19579v1">Multi-modal Co-learning for Earth Observation: Enhancing single-modality models via modality collaboration</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2509.05346v2">Benchmarking Large Language Models for Personalized Guidance in AI-Enhanced Learning</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2310.18608v3">Embedding in Recommender Systems: A Survey</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19544v1">Demonstrating Real Advantage of Machine-Learning-Enhanced Monte Carlo for Combinatorial Optimization</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19535v1">Insights into the Unknown: Federated Data Diversity Analysis on Molecular Data</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2509.21567v2">EEG-Based Consumer Behaviour Prediction: An Exploration from Classical Machine Learning to Graph Neural Networks</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19514v1">From Prototypes to Sparse ECG Explanations: SHAP-Driven Counterfactuals for Multivariate Time-Series Multi-class Classification</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.18817v2">High-order Equivariant Flow Matching for Density Functional Theory Hamiltonian Prediction</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2506.06407v3">TimeWak: Temporal Chained-Hashing Watermark for Time Series Data</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2506.15591v3">One-Step Diffusion for Detail-Rich and Temporally Consistent Video Super-Resolution</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.01687v3">Explaining Time Series Classifiers with PHAR: Rule Extraction and Fusion from Post-hoc Attributions</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19484v1">KnowMol: Advancing Molecular Large Language Models with Multi-Level Chemical Knowledge</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19479v1">Graph Unlearning Meets Influence-aware Negative Preference Optimization</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19476v1">A Concrete Roadmap towards Safety Cases based on Chain-of-Thought Monitoring</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.21589v5">Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19470v1">HybridEP: Scaling Expert Parallelism to Cross-Datacenter Scenario via Hybrid Expert&#x2F;Data Transmission</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2504.09909v2">Quantum Natural Language Processing: A Comprehensive Review of Models, Methods, and Applications</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07711v2">Merging Embedded Topics with Optimal Transport for Online Topic Modeling on Data Streams</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2507.14909v2">The Endless Tuning. An Artificial Intelligence Design To Avoid Human Replacement and Trace Back Responsibilities</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2506.18167v4">Understanding Reasoning in Thinking Language Models via Steering Vectors</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.15727v2">Invoice Information Extraction: Methods and Performance Evaluation</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19425v1">Neural Variational Dropout Processes</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19423v1">MSC-Bench: A Rigorous Benchmark for Multi-Server Tool Orchestration</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19421v1">FairNet: Dynamic Fairness Correction without Performance Loss via Contrastive Conditional LoRA</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19420v1">Monitoring LLM-based Multi-Agent Systems Against Corruptions via Node Evaluation</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19414v1">EchoFake: A Replay-Aware Dataset for Practical Speech Deepfake Detection</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19410v1">ToMMeR – Efficient Entity Mention Detection from Large Language Models</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.06098v2">MeanAudio: Fast and Faithful Text-to-Audio Generation with Mean Flows</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2509.26433v2">ACT: Agentic Classification Tree</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.16062v2">Can LLMs Correct Themselves? A Benchmark of Self-Correction in LLMs</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01322v3">Explainable fault and severity classification for rolling element bearings using Kolmogorov-Arnold networks</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.16708v2">Natural Language Processing for Cardiology: A Narrative Review</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19365v1">The Massive Legal Embedding Benchmark (MLEB)</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.17950v2">Evaluating NLP Embedding Models for Handling Science-Specific Symbolic Expressions in Student Texts</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19361v1">AgenticMath: Enhancing LLM Reasoning via Agentic-based Math Data Generation</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19358v1">M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19347v1">A New Type of Adversarial Examples</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.08727v2">Memorization-Compression Cycles Improve Generalization</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19345v1">Foundation Model Forecasts: Form and Function</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19342v1">To Use or to Refuse? Re-Centering Student Agency with Generative AI in Engineering Design Education</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2507.07780v2">Where are we with calibration under dataset shift in image classification?</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19334v1">Metadata Extraction Leveraging Large Language Models</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19329v1">Seabed-Net: A multi-task network for joint bathymetry estimation and seabed classification from remote sensing imagery in shallow waters</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19327v1">SORA-ATMAS: Adaptive Trust Management and Multi-LLM Aligned Governance for Future Smart Cities</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19322v1">Enabling Reconfiguration-Communication Overlap for Collective Communication in Optical Networks</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19321v1">Online Handwritten Signature Verification Based on Temporal-Spatial Graph Attention Transformer</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19303v1">Collaborative penetration testing suite for emerging generative AI algorithms</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19298v1">Knowledge and Common Knowledge of Strategies</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.14489v2">Reasoning Models Better Express Their Confidence</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19282v1">Enhancing Early Alzheimer Disease Detection through Big Data and Ensemble Few-Shot Learning</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2509.20214v2">Q-Palette: Fractional-Bit Quantizers Toward Optimal Bit Allocation for Efficient LLM Deployment</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.14623v3">LeapFactual: Reliable Visual Counterfactual Explanation Using Conditional Flow Matching</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2509.25271v3">RADAR: A Risk-Aware Dynamic Multi-Agent Framework for LLM Safety Evaluation via Role-Specialized Collaboration</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2410.10481v4">Model-based Large Language Model Customization as Service</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19264v1">LAPRAD: LLM-Assisted PRotocol Attack Discovery</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19263v1">An Argumentative Explanation Framework for Generalized Reason Model with Inconsistent Precedents</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19261v1">ChatGPT Unveils Its Limits: Principles of Law Deliver Checkmate</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19257v1">FnRGNN: Distribution-aware Fairness in Graph Neural Network</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18193v2">FST.ai 2.0: An Explainable AI Ecosystem for Fair, Fast, and Inclusive Decision-Making in Olympic and Paralympic Taekwondo</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.20749v3">Can Agents Fix Agent Issues?</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18431v2">ScaleNet: Scaling up Pretrained Neural Networks with Incremental Parameters</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19212v1">No Intelligence Without Statistics: The Invisible Backbone of Artificial Intelligence</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2406.18851v2">LICO: Large Language Models for In-Context Molecular Optimization</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.17064v2">A Brain Cell Type Resource Created by Large Language Models and a Multi-Agent AI System for Collaborative Community Annotation</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2506.13579v2">Flexible-length Text Infilling for Discrete Diffusion Models</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19202v1">An Active Diffusion Neural Network for Graphs</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2504.03931v3">NAACL2025 Tutorial: Adaptation of Large Language Models</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2501.04961v4">Demystifying Domain-adaptive Post-training for Financial LLMs</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.15831v2">Who’s Asking? Investigating Bias Through the Lens of Disability Framed Queries in LLMs</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2509.18942v2">Data Efficient Adaptation in Large Language Models via Continuous Low-Rank Fine-Tuning</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19181v1">Interpretable Question Answering with Knowledge Graphs</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2503.12499v2">PTFA: An LLM-based Agent that Facilitates Online Consensus Building through Parallel Thinking</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19176v1">The Zero-Step Thinking: An Empirical Study of Mode Selection as Harder Early Exit in Reasoning Models</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2506.10946v2">GUARD: Guided Unlearning and Retention via Data Attribution for Large Language Models</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.15297v2">VERA-MH Concept Paper</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19172v1">When Facts Change: Probing LLMs on Evolving Knowledge with evolveQA</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.17830v2">Multi-Agent Design Assistant for the Simulation of Inertial Fusion Energy</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.16309v2">MedRule-KG: A Knowledge-Graph–Steered Scaffold for Mathematical Reasoning with a Lightweight Verifier</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.17947v2">PLAGUE: Plug-and-play framework for Lifelong Adaptive Generation of Multi-turn Exploits</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.16705v2">An Analysis of Concept Bottleneck Models: Measuring, Understanding, and Mitigating the Impact of Noisy Annotations</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.07897v3">LongCodeBench: Evaluating Coding LLMs at 1M Context Windows</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2509.23426v2">Democratizing AI scientists using ToolUniverse</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.09160v2">A Multi-Task Foundation Model for Wireless Channel Representation Using Contrastive and Masked Autoencoder Learning</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.09660v3">Learning What Matters: Steering Diffusion via Spectrally Anisotropic Forward Noise</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19139v1">A Multi-faceted Analysis of Cognitive Abilities: Evaluating Prompt Methods with Large Language Models on the CONSORT Checklist</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19138v1">InvarGC: Invariant Granger Causality for Heterogeneous Interventional Time Series under Latent Confounding</a></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://fan-jj24.github.io/2025/10/23/2025-10-22/" data-id="cuidbrg0Yegl94ihEiAgEYyan" data-title="2025-10-22" class="article-share-link"><span class="fa fa-share">Share</span></a>
      
      
      
    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2025/10/">October 2025</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2025/10/23/2025-10-22/">2025-10-22</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2025 Pop Fan<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.6.4.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>