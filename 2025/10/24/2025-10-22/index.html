<!DOCTYPE html>


<html lang="en">
  

    <head>
      <meta charset="utf-8" />
       
      <meta name="keywords" content="Papers, Instant, Customize, Reading" />
       
      <meta name="description" content="Customize your instant paper reading experience" />
      
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title>2025-10-22 |  Instant Papers</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="/favicon.ico" />
       
<link rel="stylesheet" href="/dist/main.css">

      
<link rel="stylesheet" href="/css/fonts/remixicon.css">

      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.staticfile.org/pace/1.2.4/pace.min.js"></script>
       
 

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"
      />
      <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script>

      <!-- mermaid -->
      
      <style>
        .swal2-styled.swal2-confirm {
          font-size: 1.6rem;
        }
      </style>
    <link rel="alternate" href="/atom.xml" title="Instant Papers" type="application/atom+xml">
</head>
  </html>
</html>


<body>
  <div id="app">
    
      
    <main class="content on">
      <section class="outer">
  <article
  id="post-2025-10-22"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  2025-10-22
</h1>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2025/10/24/2025-10-22/" class="article-date">
  <time datetime="2025-10-24T07:11:21.345Z" itemprop="datePublished">2025-10-24</time>
</a>   
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> Word count:</span>
            <span class="post-count">29.2k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> Reading time≈</span>
            <span class="post-count">108 min</span>
        </span>
    </span>
</div>
 
    </div>
      
    <div class="tocbot"></div>




  
    <div class="article-entry" itemprop="articleBody">
       
  <p>今日共搜索AI论文: 240<br>请注意，我只想阅读指定类别的论文。如果您需要其他类别，请与我联系<br>那些不符合要求的论文将被拒绝，并在文本末尾显示<br>目前的筛选类别<br>Reinforcement Learning, Robotics, Vision-Language Models, World Models</p>
<p>今日共筛选论文: 74</p>
<h2 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h2><h3 id="Aligning-Transformers-with-Continuous-Feedback-via-Energy-Rank-Alignment"><a href="#Aligning-Transformers-with-Continuous-Feedback-via-Energy-Rank-Alignment" class="headerlink" title="Aligning Transformers with Continuous Feedback via Energy Rank Alignment"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2405.12961v3">Aligning Transformers with Continuous Feedback via Energy Rank Alignment</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>在化学空间中进行搜索是一个极具挑战性的问题，因为可能的分子数量会随着原子数量呈组合性增长。基于化合物数据库训练的大型自回归模型已经产生了强大的生成器，但我们仍然缺乏生成具有所需性质的分子的稳健策略。这种分子搜索问题在很大程度上类似于大型语言模型中的“对齐”问题，不过对于许多化学任务，我们拥有一个具体且易于评估的奖励函数。在这里，我们引入了一种名为“能量排名对齐”（Energy Rank Alignment, ERA）的算法，该算法利用一个显式的奖励函数来生成一个基于梯度的优化目标，用于优化自回归策略。我们在理论上证明，这种算法与近端策略优化（PPO）和直接偏好优化（DPO）密切相关，但其最小化器能够收敛到一个理想的吉布斯-玻尔兹曼分布，其中奖励函数在其中扮演能量函数的角色。此外，该算法具有高度的可扩展性，不需要强化学习，并且在每对数据中偏好观察数量较少时，其性能与DPO相当。我们将这种方法应用于对齐分子变换器和蛋白质语言模型，以分别生成具有外部指定性质的分子和蛋白质序列，并发现该方法能够稳健地进行搜索，遍历化学空间中多样的区域。</p>
<h3 id="Approximate-Model-Predictive-Control-for-Microgrid-Energy-Management-via-Imitation-Learning"><a href="#Approximate-Model-Predictive-Control-for-Microgrid-Energy-Management-via-Imitation-Learning" class="headerlink" title="Approximate Model Predictive Control for Microgrid Energy Management via Imitation Learning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.20040v1">Approximate Model Predictive Control for Microgrid Energy Management via Imitation Learning</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>在可再生能源集成不断加深的背景下，高效的能源管理对于微电网的可靠和可持续运行至关重要。本文提出了一种基于模仿学习的框架，用于近似混合整数经济模型预测控制（EMPC）以实现微电网的能源管理。所提出的方法通过离线轨迹训练神经网络，以模仿专家EMPC控制动作，从而实现快速、实时的决策，而无需在线求解优化问题。为了增强鲁棒性和泛化能力，学习过程中在训练阶段引入噪声以缓解分布偏移，并显式地将可再生能源发电和负荷需求的预测不确定性纳入考虑。仿真结果表明，所学到的策略在经济性能方面与EMPC相当，而在实践中仅需基于优化的EMPC约10%的计算时间。</p>
<h3 id="A-Principle-of-Targeted-Intervention-for-Multi-Agent-Reinforcement-Learning"><a href="#A-Principle-of-Targeted-Intervention-for-Multi-Agent-Reinforcement-Learning" class="headerlink" title="A Principle of Targeted Intervention for Multi-Agent Reinforcement Learning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.17697v2">A Principle of Targeted Intervention for Multi-Agent Reinforcement Learning</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>将协作多智能体强化学习（MARL）引导至期望结果具有挑战性，特别是在大规模MARL中，人类对整个多智能体系统进行全局指导往往不切实际。另一方面，设计外部机制（如内在奖励和人类反馈）来协调智能体大多依赖于经验研究，缺乏易于使用的研究工具。在本工作中，我们采用多智能体影响图（MAIDs）作为图示框架来解决上述问题。首先，我们引入了MARL交互范式的概念，并利用MAIDs来分析和可视化MARL中的无引导自组织机制与全局指导机制。随后，我们设计了一种新的MARL交互范式，称为“目标干预范式”，该范式仅应用于单个目标智能体，从而缓解全局指导问题。在我们的实现中，我们引入了一种因果推理技术，称为预策略干预（Pre-Strategy Intervention，PSI），以实现目标干预范式。由于MAIDs可以被视为因果图的一种特殊类型，通过PSI最大化相应的因果效应，可以实现整合主要任务目标和额外期望结果的复合期望结果。此外，MAIDs的捆绑相关性图分析提供了一种工具，用于判断在特定MARL交互范式设计下，MARL学习范式是否可行。在实验中，我们验证了所提出的目标干预的有效性，并验证了相关性图分析的结果。</p>
<h3 id="GTAlign-Game-Theoretic-Alignment-of-LLM-Assistants-for-Mutual-Welfare"><a href="#GTAlign-Game-Theoretic-Alignment-of-LLM-Assistants-for-Mutual-Welfare" class="headerlink" title="GTAlign: Game-Theoretic Alignment of LLM Assistants for Mutual Welfare"></a><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.08872v2">GTAlign: Game-Theoretic Alignment of LLM Assistants for Mutual Welfare</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>大型语言模型（LLMs）在推理方面取得了显著进展，但在诸如写作、信息检索或提供实用指导等任务中，有时会产生对用户不利的响应。传统的对齐方法通常假设最大化模型奖励也等同于最大化用户福利，但这一假设在实践中经常失效：当用户更倾向于简洁答案时，模型可能会过度澄清或生成冗长的推理过程。这种行为类似于囚徒困境，其中个体理性的选择会导致社会整体的次优结果。根本性的挑战在于缺乏一种能够同时使LLM和用户受益的决策机制。为此，我们提出了博弈论对齐（Game-Theoretic Alignment，GTAlign），这是一种将博弈论决策机制整合到推理和训练中的对齐框架。在推理过程中，模型将用户与LLM的交互明确视为一种策略性博弈：它在推理链中构建收益矩阵，以估算自身和用户的福利，然后选择对双方都有利的行为。在训练过程中，我们引入了一种相互福利奖励机制，以强化合作性响应，使模型行为与社会高效结果相一致。此外，我们还引入了一种推理技术，利用博弈论推理动态适应LLM在服务定价政策变化时的响应。大量实验表明，GTAlign在多种任务中相较于基线方法显著提升了推理效率、回答质量和双方福利。代码可在<a target="_blank" rel="noopener" href="https://github.com/ulab-uiuc/GTAlign%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ulab-uiuc/GTAlign获取。</a></p>
<h3 id="Robust-Reinforcement-Learning-in-Finance-Modeling-Market-Impact-with-Elliptic-Uncertainty-Sets"><a href="#Robust-Reinforcement-Learning-in-Finance-Modeling-Market-Impact-with-Elliptic-Uncertainty-Sets" class="headerlink" title="Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19950v1">Robust Reinforcement Learning in Finance: Modeling Market Impact with Elliptic Uncertainty Sets</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>在金融应用中，强化学习（RL）代理通常是在历史数据上进行训练的，此时其操作不会影响价格。然而，在部署时，这些代理会在实时市场中进行交易，其自身的交易行为可能会改变资产价格，这种现象称为市场影响。训练环境和部署环境之间的这种不匹配可能会显著降低性能。传统的鲁棒强化学习方法通过在一组不确定性中优化最坏情况下的性能来解决这种模型误设问题，但通常依赖对称结构，无法捕捉市场影响的方向性。为了解决这个问题，我们开发了一类新的椭圆不确定性集。我们为这些集合下的最坏情况不确定性建立了显式和隐式的闭式解，从而实现了高效且可处理的鲁棒策略评估。在单资产和多资产交易任务上的实验表明，我们的方法实现了更高的夏普比率，并且在交易量增加的情况下仍保持稳健，为金融市场的强化学习提供了一种更加真实且可扩展的方法。</p>
<h3 id="Scaf-GRPO-Scaffolded-Group-Relative-Policy-Optimization-for-Enhancing-LLM-Reasoning"><a href="#Scaf-GRPO-Scaffolded-Group-Relative-Policy-Optimization-for-Enhancing-LLM-Reasoning" class="headerlink" title="Scaf-GRPO: Scaffolded Group Relative Policy Optimization for Enhancing LLM Reasoning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19807v1">Scaf-GRPO: Scaffolded Group Relative Policy Optimization for Enhancing LLM Reasoning</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>从可验证奖励中进行强化学习已成为增强大型语言模型（LLMs）复杂推理能力的一种强大技术。然而，这些方法本质上受到“学习悬崖”现象的限制：当面对远超当前能力范围的问题时，模型会持续失败，产生持续的零奖励信号。在如GRPO这样的策略优化算法中，这种现象会导致优势计算坍缩为零，使学习梯度无法感知这些困难问题，从而阻碍了进步。为了解决这一问题，我们引入了Scaf-GRPO（分层引导式组相对策略优化），这是一种渐进式训练框架，只有在模型的独立学习达到瓶颈时，才会战略性地提供最小的引导。该框架首先诊断学习停滞，然后通过注入从抽象概念到具体步骤的分层提示，干预模型的学习过程，使模型能够自行构建有效的解决方案。在具有挑战性的数学基准测试中进行的大量实验表明，Scaf-GRPO是有效的，其在AIME24基准测试中将Qwen2.5-Math-7B模型的pass@1得分比普通GRPO基线提升了44.3%。这一结果表明，我们的框架提供了一种稳健且有效的方法，使模型能够解决此前无法处理的问题，这是向扩展LLM自主推理边界迈出的关键一步。</p>
<h3 id="Provably-Efficient-Reward-Transfer-in-Reinforcement-Learning-with-Discrete-Markov-Decision-Processes"><a href="#Provably-Efficient-Reward-Transfer-in-Reinforcement-Learning-with-Discrete-Markov-Decision-Processes" class="headerlink" title="Provably Efficient Reward Transfer in Reinforcement Learning with Discrete Markov Decision Processes"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2503.13414v3">Provably Efficient Reward Transfer in Reinforcement Learning with Discrete Markov Decision Processes</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>在本文中，我们提出了一种新的奖励适应（RA）方法，用于强化学习。该方法中，智能体根据在相同领域动态下但不同奖励函数下事先学习得到的一个或多个源行为，适应于目标奖励函数。虽然可以从头开始学习目标行为，但鉴于已有源行为的存在，这种方法通常效率低下。我们的工作通过操控Q函数提出了一种新的RA方法。假设目标奖励函数是源奖励函数的已知函数，我们计算Q函数的界限，并提出一种迭代过程（类似于值迭代），以收紧这些界限。这些界限能够在学习开始之前，就对目标领域中的动作进行剪枝。我们将这种方法称为“Q操控”（Q-M）。该迭代过程假设可以访问一个轻量级模型，这种模型易于提供或学习。我们正式证明，在离散领域中，Q-M不会影响返回策略的最优性，并且在概率意义上，它在样本复杂度方面具有可证明的高效性。我们通过多种合成和仿真环境评估Q-M，以展示其有效性、泛化能力和实用性。</p>
<h3 id="QoQ-Med-Building-Multimodal-Clinical-Foundation-Models-with-Domain-Aware-GRPO-Training"><a href="#QoQ-Med-Building-Multimodal-Clinical-Foundation-Models-with-Domain-Aware-GRPO-Training" class="headerlink" title="QoQ-Med: Building Multimodal Clinical Foundation Models with Domain-Aware GRPO Training"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2506.00711v2">QoQ-Med: Building Multimodal Clinical Foundation Models with Domain-Aware GRPO Training</a></h3><p><strong>Categories:</strong> Reinforcement Learning,Vision-Language Models</p>
<p>临床决策通常需要在异构数据上进行推理，但现有的多模态语言模型（MLLMs）仍主要以视觉为中心，难以在不同临床专科间进行泛化。为弥合这一差距，我们引入了QoQ-Med-7B&#x2F;32B，这是首个开放的通用型临床基础模型，能够联合推理医学图像、时间序列信号和文本报告。QoQ-Med采用了一种新的强化学习目标——领域感知相对策略优化（DRPO），该方法根据领域稀有度和模态难度层级缩放归一化奖励，从而缓解因临床数据分布不均而导致的性能不平衡问题。在包含9个临床领域的261万条指令微调数据对上训练后，我们证明与其它无批评者训练方法（如GRPO）相比，DRPO训练在所有视觉领域中平均将宏观F1诊断性能提升了43%。此外，QoQ-Med在密集分割数据上进行训练，能够突出与诊断相关的显著区域，其IoU（交并比）比开源模型高10倍，同时达到OpenAI o4-mini的性能水平。为促进可重复性及下游研究，我们开放了以下资源：（i）完整模型权重，（ii）模块化训练流程，以及（iii）所有中间推理轨迹，详见<a target="_blank" rel="noopener" href="https://github.com/DDVD233/QoQ_Med%E3%80%82">https://github.com/DDVD233/QoQ_Med。</a></p>
<h3 id="Breaking-the-Exploration-Bottleneck-Rubric-Scaffolded-Reinforcement-Learning-for-General-LLM-Reasoning"><a href="#Breaking-the-Exploration-Bottleneck-Rubric-Scaffolded-Reinforcement-Learning-for-General-LLM-Reasoning" class="headerlink" title="Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.16949v5">Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement Learning for General LLM Reasoning</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>近年来，大型语言模型（LLMs）的进展凸显了强化学习（RL）在促进推理能力发展方面的潜力。尽管已有令人鼓舞的成果，但一个根本性矛盾依然存在：强化学习的改进依赖于高质量样本的学习，然而，这些样本的探索却受到LLMs固有局限性的制约。这实际上形成了一种不理想的循环：无法探索的领域也就无法学习。在本文中，我们提出了一种新颖的指导性脚手架框架——Rubric-Scaffolded Reinforcement Learning（RuscaRL），旨在突破通用LLM推理的探索瓶颈。具体而言，RuscaRL引入了清单式评分标准（rubrics）作为（1）在生成轨迹时的显式探索脚手架，其中不同的评分标准作为任务指令中的外部指导，引导生成多样化且高质量的响应；这种指导会随时间逐渐减弱，鼓励模型内部化潜在的推理模式；（2）在模型训练过程中用于探索的可验证奖励，我们可以通过评分标准作为参考获得稳健的“LLM作为裁判”的评分，从而在通用推理任务上实现有效的强化学习。大量实验表明，所提出的RuscaRL在多个基准测试中表现出色，有效拓展了在Best-of-N评估下的推理边界。值得注意的是，RuscaRL将Qwen2.5-7B-Instruct在HealthBench-500上的表现从23.6提升至50.3，超过了GPT-4.1。此外，我们对Qwen3-30B-A3B-Instruct进行微调后，在HealthBench-500上达到了61.1，优于包括OpenAI-o3在内的领先LLM。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/IANNXANG/RuscaRL%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/IANNXANG/RuscaRL获取。</a></p>
<h3 id="Memo-Training-Memory-Efficient-Embodied-Agents-with-Reinforcement-Learning"><a href="#Memo-Training-Memory-Efficient-Embodied-Agents-with-Reinforcement-Learning" class="headerlink" title="Memo: Training Memory-Efficient Embodied Agents with Reinforcement Learning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19732v1">Memo: Training Memory-Efficient Embodied Agents with Reinforcement Learning</a></h3><p><strong>Categories:</strong> Reinforcement Learning, Robotics</p>
<p>为了使具身智能体在长时间范围内有效运行，开发能够形成并访问记忆的模型至关重要，以便其在环境中保持上下文感知。在当前基于Transformer的具身序列决策任务的训练范式中，视觉输入常常超出Transformer的上下文限制，而人类则能够利用一生的经验（以记忆形式压缩存储）进行维护和使用。从理论上讲，存在显著的压缩空间，因为大部分输入信息是无关的，可以被抽象出来。然而，现有的方法大多集中在固定大小记忆的循环模型或依赖完整上下文的Transformer上。在本文中，我们提出Memo，这是一种基于Transformer的架构和训练方法，用于在需要大量记忆、长时间跨度任务中的强化学习（RL）。Memo在训练过程中通过在模型输入中插入定期摘要标记，实现记忆的创建和检索。我们在网格世界元强化学习基准测试和逼真的室内多目标导航任务中验证了Memo的有效性。Memo在计算和存储效率方面优于简单的长上下文Transformer基线。此外，Memo在推理过程中对更长的上下文具有更好的泛化能力，并且在需要截断历史上下文以适应推理约束的流式设置中仍保持鲁棒性。</p>
<h3 id="ARM-FM-Automated-Reward-Machines-via-Foundation-Models-for-Compositional-Reinforcement-Learning"><a href="#ARM-FM-Automated-Reward-Machines-via-Foundation-Models-for-Compositional-Reinforcement-Learning" class="headerlink" title="ARM-FM: Automated Reward Machines via Foundation Models for Compositional Reinforcement Learning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.14176v2">ARM-FM: Automated Reward Machines via Foundation Models for Compositional Reinforcement Learning</a></h3><p><strong>Categories:</strong> Reinforcement Learning, Vision-Language Models</p>
<p>强化学习（RL）算法对奖励函数的定义高度敏感，这仍然是限制其广泛应用的核心挑战。我们提出ARM-FM：通过基础模型实现自动奖励机器（Automated Reward Machines via Foundation Models），这是一个用于RL中自动化、组合式奖励设计的框架，利用基础模型（FMs）的高层推理能力。奖励机器（RMs）——一种基于自动机的形式化奖励定义方法——被用作RL目标定义的机制，并通过使用FMs自动构建。RM的结构化形式化方法能够实现有效的任务分解，而使用FMs则使得目标定义可以使用自然语言进行表达。具体而言，我们（i）使用FMs从自然语言规范中自动生成RMs；（ii）将语言嵌入与每个RM自动机状态相关联，以实现跨任务的泛化；（iii）在一系列具有挑战性的多样化环境中提供了ARM-FM有效性的实证证据，包括零样本泛化的证据。</p>
<h3 id="Masked-Generative-Priors-Improve-World-Models-Sequence-Modelling-Capabilities"><a href="#Masked-Generative-Priors-Improve-World-Models-Sequence-Modelling-Capabilities" class="headerlink" title="Masked Generative Priors Improve World Models Sequence Modelling Capabilities"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2410.07836v6">Masked Generative Priors Improve World Models Sequence Modelling Capabilities</a></h3><p><strong>Categories:</strong> Reinforcement Learning, World Models</p>
<p>深度强化学习（RL）已成为在复杂环境中创建人工代理的主要方法。基于模型的方法，即具有世界模型（world models）的RL方法，能够预测环境动态，是提高数据效率最具前景的方向之一，是弥合研究与实际部署之间差距的关键步骤。特别是，世界模型通过在想象中学习来提升样本效率，这涉及以自监督的方式训练环境的生成序列模型。最近，掩码生成建模（Masked Generative Modelling）作为一种更高效且更优越的归纳偏差，被用于建模和生成标记序列。基于高效的随机Transformer世界模型（STORM）架构，我们用掩码生成先验（如MaskGIT先验）替代传统的MLP先验，并引入了GIT-STORM模型。我们在两个下游任务上评估了我们的模型：强化学习和视频预测。GIT-STORM在Atari 100k基准测试的强化学习任务中表现出显著的性能提升。此外，我们首次将基于Transformer的世界模型应用于连续动作环境，填补了先前研究中的一个重要空白。为实现这一点，我们采用了一种状态混合函数，将潜在状态表示与动作进行整合，使我们的模型能够处理连续控制任务。我们在DeepMind控制套件上通过定性和定量分析验证了这一方法，展示了基于Transformer的世界模型在这一新领域中的有效性。我们的结果突显了掩码GIT动态先验的多功能性和有效性，为构建更精确的世界模型和有效的强化学习策略铺平了道路。</p>
<h3 id="Estimating-Long-term-Heterogeneous-Dose-response-Curve-Generalization-Bound-Leveraging-Optimal-Transport-Weights"><a href="#Estimating-Long-term-Heterogeneous-Dose-response-Curve-Generalization-Bound-Leveraging-Optimal-Transport-Weights" class="headerlink" title="Estimating Long-term Heterogeneous Dose-response Curve: Generalization Bound Leveraging Optimal Transport Weights"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2406.19195v3">Estimating Long-term Heterogeneous Dose-response Curve: Generalization Bound Leveraging Optimal Transport Weights</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>长期治疗效果的估计在许多应用中是一个具有重要意义但具有挑战性的问题。现有的方法通常依赖于理想化的假设，例如没有未观测的混杂变量或二元治疗，以估计长期平均治疗效果。然而，在许多实际应用场景中，这些假设可能会被违反，而平均治疗效果对于个性化决策而言是不够的。本文针对一个更为一般的问题，即在考虑未观测混杂变量和连续治疗的情况下估计长期异质剂量反应曲线（HDRC）。具体而言，为了消除长期观察数据中的未观测混杂变量，我们引入了一个最优传输加权框架，将长期观察数据对齐到辅助的短期实验数据。此外，为了准确预测连续治疗的异质性效应，我们利用最优传输诱导的重加权分布，建立了反事实预测误差的泛化界限。最后，我们基于上述理论基础，开发了一个长期HDRC估计器。在合成数据集和半合成数据集上的大量实验验证了我们方法的有效性。</p>
<h3 id="Optimizing-the-Unknown-Black-Box-Bayesian-Optimization-with-Energy-Based-Model-and-Reinforcement-Learning"><a href="#Optimizing-the-Unknown-Black-Box-Bayesian-Optimization-with-Energy-Based-Model-and-Reinforcement-Learning" class="headerlink" title="Optimizing the Unknown: Black Box Bayesian Optimization with Energy-Based Model and Reinforcement Learning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19530v1">Optimizing the Unknown: Black Box Bayesian Optimization with Energy-Based Model and Reinforcement Learning</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>现有的贝叶斯优化（BO）方法通常通过在探索与利用之间进行平衡来优化代价高昂的目标函数。然而，这些方法往往存在显著的一步偏差，这可能导致收敛到局部最优解，并在复杂或高维任务中表现不佳。近年来，黑盒优化（BBO）在各种科学和工程领域取得了成功，尤其是在函数评估成本高昂且梯度不可用的情况下。受此启发，我们提出了用于贝叶斯优化的强化能量模型（REBMBO），它结合高斯过程（GP）进行局部引导，并利用能量模型（EBM）来捕捉全局结构信息。值得注意的是，我们将每一次贝叶斯优化迭代定义为马尔可夫决策过程（MDP），并采用近端策略优化（PPO）进行自适应多步前瞻，动态调整探索的深度和方向，从而有效克服传统BO方法的局限性。我们在合成和现实世界基准上进行了大量实验，验证了REBMBO的优越性能。此外，针对各种GP配置的进一步分析进一步突显了其适应性和鲁棒性。</p>
<h3 id="Universal-Quantitative-Abstraction-Categorical-Duality-and-Logical-Completeness-for-Probabilistic-Systems"><a href="#Universal-Quantitative-Abstraction-Categorical-Duality-and-Logical-Completeness-for-Probabilistic-Systems" class="headerlink" title="Universal Quantitative Abstraction: Categorical Duality and Logical Completeness for Probabilistic Systems"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19444v1">Universal Quantitative Abstraction: Categorical Duality and Logical Completeness for Probabilistic Systems</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>本文提出了一种定量抽象的统一理论，用于概率系统，将范畴论、最优传输和定量模态逻辑联系起来。该理论的核心是一个具有普遍性质的规范 $ \varepsilon $-商：在所有 $ \varepsilon $-抽象中，它是信息量最大的，同时保持了预设的价值损失上限。这种构造诱导了抽象与实现函子之间的伴随关系 $ (Q_{\varepsilon} \dashv R_{\varepsilon}) $，通过特殊伴随函子定理建立，揭示了度量结构与逻辑语义之间的范畴对偶性。行为伪度量被刻画为一种贝尔曼风格算子的唯一不动点，并在协代数框架中证明了其收缩性和利普希茨性质。引入了一种定量模态 $ \mu $-演算，并证明其在逻辑可表示系统中具有表达完全性，从而使得行为距离与最大逻辑偏差相一致。分析了接口细化下的组合性，阐明了抽象在系统边界间的交互方式。在有限马尔可夫决策过程中进行的精确验证套件验证了收缩性质、价值损失界限、扰动下的稳定性、对抗区分性和可扩展性，展示了该框架的鲁棒性和计算可行性。由此产生的框架为状态聚合和表示学习提供了有原则的目标，并在随机领域中为价值函数近似提供了数学精确的保证。</p>
<h3 id="ColorAgent-Building-A-Robust-Personalized-and-Interactive-OS-Agent"><a href="#ColorAgent-Building-A-Robust-Personalized-and-Interactive-OS-Agent" class="headerlink" title="ColorAgent: Building A Robust, Personalized, and Interactive OS Agent"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19386v1">ColorAgent: Building A Robust, Personalized, and Interactive OS Agent</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>随着硬件、软件和大型语言模型技术的进步，人与操作系统之间的交互方式已经从命令行界面演进到了迅速兴起的AI代理交互。构建一个能够执行用户指令并忠实遵循用户意图的操作系统（OS）代理，正逐渐成为现实。在本技术报告中，我们介绍了ColorAgent，这是一个设计用于与环境进行长期、稳健交互的OS代理，同时还能实现个性化和主动的用户交互。为了实现与环境的长期交互，我们通过分步强化学习和自进化训练来增强模型的能力，同时开发了一个定制的多代理框架，以确保通用性、一致性和鲁棒性。在用户交互方面，我们探索了个性化用户意图识别和主动参与，将OS代理不仅仅定位为一个自动化工具，而是作为一位温暖、协作的伙伴。我们在AndroidWorld和AndroidLab基准测试中评估了ColorAgent，分别取得了77.2%和50.7%的成功率，建立了新的行业标杆。然而，我们注意到当前的基准测试不足以全面评估OS代理，因此在未来的探索方向中，我们提出了进一步研究的领域，特别是在评估范式、代理协作和安全性方面。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/MadeAgents/mobile-use%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/MadeAgents/mobile-use获取。</a></p>
<h3 id="Coordinated-Strategies-in-Realistic-Air-Combat-by-Hierarchical-Multi-Agent-Reinforcement-Learning"><a href="#Coordinated-Strategies-in-Realistic-Air-Combat-by-Hierarchical-Multi-Agent-Reinforcement-Learning" class="headerlink" title="Coordinated Strategies in Realistic Air Combat by Hierarchical Multi-Agent Reinforcement Learning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.11474v2">Coordinated Strategies in Realistic Air Combat by Hierarchical Multi-Agent Reinforcement Learning</a></h3><p><strong>Categories:</strong> Reinforcement Learning, Robotics</p>
<p>在对空战进行现实模拟的环境中实现任务目标极具挑战性，因为存在不完全的情境感知和非线性的飞行动力学。在本研究中，我们引入了一种新颖的三维多智能体空战环境和一种分层多智能体强化学习框架，以应对这些挑战。我们的方法结合了异构智能体动力学、课程学习、联赛对战以及一种新适应的训练算法。为此，决策过程被组织为两个抽象层次：低层次策略学习精确的控制机动，而高层次策略则根据任务目标发布战术指令。实验结果表明，我们的分层方法在复杂空战场景中提高了学习效率和战斗性能。</p>
<h3 id="Balancing-Rewards-in-Text-Summarization-Multi-Objective-Reinforcement-Learning-via-HyperVolume-Optimization"><a href="#Balancing-Rewards-in-Text-Summarization-Multi-Objective-Reinforcement-Learning-via-HyperVolume-Optimization" class="headerlink" title="Balancing Rewards in Text Summarization: Multi-Objective Reinforcement Learning via HyperVolume Optimization"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19325v1">Balancing Rewards in Text Summarization: Multi-Objective Reinforcement Learning via HyperVolume Optimization</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>文本摘要是一项关键任务，需要同时优化多个目标，包括一致性、连贯性、相关性和流畅性，这带来了相当大的挑战。尽管大型语言模型（LLMs）在强化学习（RL）的增强下表现出色，但很少有研究专注于通过基于LLMs的强化学习来优化摘要的多目标问题。在本文中，我们引入了一种新颖的优化策略——超体积优化（HVO）。该方法通过使用超体积方法在强化学习的奖励过程中动态调整组之间的得分。这种方法引导模型优化逐步逼近帕累托前沿，从而在多个目标之间生成平衡的摘要。我们在多个代表性摘要数据集上的实验结果表明，我们的方法在总体得分上优于组相对策略优化（GRPO），并且在不同维度上表现出更均衡的性能。此外，通过HVO增强的7B基础模型在摘要任务中的表现与GPT-4相当，同时保持了更短的生成长度。我们的代码已公开在<a target="_blank" rel="noopener" href="https://github.com/ai4business-LiAuto/HVO.git%E3%80%82">https://github.com/ai4business-LiAuto/HVO.git。</a></p>
<h3 id="Continual-Knowledge-Adaptation-for-Reinforcement-Learning"><a href="#Continual-Knowledge-Adaptation-for-Reinforcement-Learning" class="headerlink" title="Continual Knowledge Adaptation for Reinforcement Learning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19314v1">Continual Knowledge Adaptation for Reinforcement Learning</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>强化学习使智能体能够通过与环境的交互学习最优行为。然而，现实世界环境通常是非平稳的，要求智能体持续适应新的任务和变化的条件。尽管持续强化学习（Continual Reinforcement Learning）能够跨多个任务进行学习，但现有方法常常面临灾难性遗忘和知识利用效率低的问题。为了解决这些挑战，我们提出了一种用于强化学习的持续知识适应方法（Continual Knowledge Adaptation for Reinforcement Learning，简称CKA-RL），该方法能够积累和有效利用历史知识。具体而言，我们引入了一种持续知识适应策略，该策略包括维护任务特定的知识向量池，并动态使用历史知识来适应新任务。这一过程通过保留和调整关键模型参数，缓解灾难性遗忘问题，并实现任务间的高效知识迁移。此外，我们还提出了一种自适应知识融合机制，用于合并相似的知识向量，以应对可扩展性挑战，从而减少内存需求，同时确保关键知识的保留。在三个基准测试中进行的实验表明，所提出的CKA-RL方法在整体性能上优于现有最先进的方法，分别提升了4.20%的整体性能和8.02%的正向迁移效果。源代码可在<a target="_blank" rel="noopener" href="https://github.com/Fhujinwu/CKA-RL%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Fhujinwu/CKA-RL获取。</a></p>
<h3 id="Learning-to-Make-Friends-Coaching-LLM-Agents-toward-Emergent-Social-Ties"><a href="#Learning-to-Make-Friends-Coaching-LLM-Agents-toward-Emergent-Social-Ties" class="headerlink" title="Learning to Make Friends: Coaching LLM Agents toward Emergent Social Ties"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19299v1">Learning to Make Friends: Coaching LLM Agents toward Emergent Social Ties</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>大型语言模型（LLM）代理能否再现人类在线行为中所体现的复杂社会动态——这些动态受到同质性、互惠性和社会认同的影响？以及哪些记忆和学习机制能够促成这些动态的出现？我们提出了一种多智能体LLM模拟框架，在该框架中，智能体通过上下文学习加速的指导信号反复进行互动、相互评估并调整自身行为。为了模拟人类社会行为，我们设计了行为奖励函数，以捕捉在线参与的核心驱动因素，包括社会互动、信息获取、自我呈现、协调以及情感支持。这些奖励机制使智能体的目标与实证观察到的用户动机相一致，从而能够研究网络结构和群体形成如何从个体决策中产生。我们的实验表明，经过指导的LLM代理能够发展出稳定的交互模式，并形成涌现的社会联系，从而产生与真实在线社区相似的网络结构。通过将行为奖励与上下文适应相结合，我们的框架建立了一个原理性的测试平台，用于研究LLM群体中的集体动态，并揭示人工代理如何模拟或偏离人类类社会行为。</p>
<h3 id="Social-World-Model-Augmented-Mechanism-Design-Policy-Learning"><a href="#Social-World-Model-Augmented-Mechanism-Design-Policy-Learning" class="headerlink" title="Social World Model-Augmented Mechanism Design Policy Learning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19270v1">Social World Model-Augmented Mechanism Design Policy Learning</a></h3><p><strong>Categories:</strong> Reinforcement Learning, World Models</p>
<p>设计适应性机制以协调个体与集体利益，仍然是人工智能社会智能领域中的核心挑战。现有方法在建模具有持续潜在特征（如技能、偏好）的异质性智能体以及处理复杂多智能体系统动态方面常常面临困难。这些挑战进一步被高样本效率需求所加剧，因为现实世界的交互成本很高。通过学习预测环境动态，世界模型（World Models）为增强异质性和复杂系统中的机制设计提供了一条有前景的途径。在本文中，我们提出了一种名为SWM-AP（Social World Model-Augmented Mechanism Design Policy Learning，社会世界模型增强的机制设计策略学习）的新方法。该方法通过分层建模智能体行为来学习社会世界模型，从而增强机制设计。具体而言，社会世界模型从智能体的交互轨迹中推断其特征，并学习基于特征的模型以预测智能体对部署机制的响应。机制设计策略通过与社会世界模型交互来收集大量训练轨迹，同时在现实交互过程中在线推断智能体的特征，从而进一步提升策略学习的效率。在多种应用场景（如税收政策设计、团队协作和设施选址）中的实验表明，SWM-AP在累积奖励和样本效率方面均优于现有的基于模型和无模型强化学习基准方法。</p>
<h3 id="Improved-Exploration-in-GFlownets-via-Enhanced-Epistemic-Neural-Networks"><a href="#Improved-Exploration-in-GFlownets-via-Enhanced-Epistemic-Neural-Networks" class="headerlink" title="Improved Exploration in GFlownets via Enhanced Epistemic Neural Networks"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2506.16313v2">Improved Exploration in GFlownets via Enhanced Epistemic Neural Networks</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>在GFlowNets中，高效识别用于训练的正确轨迹仍然是一个开放性问题。为了解决这一问题，必须优先在奖励分布尚未充分学习的状态空间区域进行探索。这就需要不确定性驱动的探索，换句话说，智能体应意识到自身知识的不足。这一属性可以通过联合预测来衡量，这对于组合型和序列决策问题尤为重要。在本研究中，我们将知识不确定性神经网络（ENN）与传统的GFlowNets架构相结合，以实现更高效的联合预测和更好的不确定性量化，从而提升探索效率和最优轨迹的识别能力。我们提出的算法ENN-GFN-Enhanced与GFlowNets中的基线方法进行了比较，并在网格环境以及多种设置下的结构化序列生成任务中进行了评估，验证了其有效性和高效性。</p>
<h3 id="PARCO-Parallel-AutoRegressive-Models-for-Multi-Agent-Combinatorial-Optimization"><a href="#PARCO-Parallel-AutoRegressive-Models-for-Multi-Agent-Combinatorial-Optimization" class="headerlink" title="PARCO: Parallel AutoRegressive Models for Multi-Agent Combinatorial Optimization"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2409.03811v3">PARCO: Parallel AutoRegressive Models for Multi-Agent Combinatorial Optimization</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>涉及多个智能体的组合优化问题由于其NP难性质以及需要有效的智能体协调，历来具有极大的挑战性。尽管基于学习的方法取得了进展，但现有的方法通常面临关键的局限性，包括次优的智能体协调、泛化能力差以及高计算延迟。为了解决这些问题，我们提出了PARCO（并行自回归组合优化），这是一种通用的强化学习框架，旨在高效地构建多智能体组合任务的高质量解决方案。为此，PARCO集成了三个关键的创新组件：(1) 基于Transformer的通信层，以在并行解构造过程中实现有效的智能体协作；(2) 多指针机制，用于实现低延迟的并行智能体决策；(3) 基于优先级的冲突解决器，通过学习到的优先级来解决决策冲突。我们在多智能体车辆路径与调度问题中评估了PARCO，结果表明我们的方法优于现有的学习方法，展现出强大的泛化能力和显著的计算效率。我们公开了PARCO的源代码以促进未来的研究：<a target="_blank" rel="noopener" href="https://github.com/ai4co/parco%E3%80%82">https://github.com/ai4co/parco。</a></p>
<h3 id="SPOT-Scalable-Policy-Optimization-with-Trees-for-Markov-Decision-Processes"><a href="#SPOT-Scalable-Policy-Optimization-with-Trees-for-Markov-Decision-Processes" class="headerlink" title="SPOT: Scalable Policy Optimization with Trees for Markov Decision Processes"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19241v1">SPOT: Scalable Policy Optimization with Trees for Markov Decision Processes</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>可解释的强化学习策略对于高风险决策至关重要，然而在马尔可夫决策过程（MDPs）中优化决策树策略仍然具有挑战性。我们提出了SPOT，一种用于计算决策树策略的新方法，该方法将优化问题建模为混合整数线性规划（MILP）问题。为了提高效率，我们采用了一种减少空间的分支定界方法，将MDP的动力学与树结构约束分离，从而实现高效的并行搜索。这显著提高了运行时间和可扩展性，相比之前的方法有明显提升。我们的方法确保每次迭代都能得到最优的决策树策略。在标准基准测试中的实验结果表明，SPOT实现了显著的速度提升，并能扩展到具有更多状态的更大MDPs。所得到的决策树策略具有可解释性和紧凑性，保持了透明度而不会影响性能。这些结果表明，我们的方法同时实现了可解释性和可扩展性，比现有方法快了一个数量级地提供了高质量的策略。</p>
<h3 id="Horizon-Reduction-Makes-RL-Scalable"><a href="#Horizon-Reduction-Makes-RL-Scalable" class="headerlink" title="Horizon Reduction Makes RL Scalable"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2506.04168v3">Horizon Reduction Makes RL Scalable</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>在本工作中，我们研究了离线强化学习（RL）算法的可扩展性。原则上，一个真正具有可扩展性的离线RL算法应在数据、计算能力和模型容量充足的情况下，能够解决任何复杂程度的问题。我们利用比典型离线RL数据集大1000倍的数据集，考察当前的离线RL算法在多样且具有挑战性、此前未被解决的任务上是否能够兑现这一承诺。我们观察到，尽管增加了数据量，许多现有的离线RL算法仍然表现出较差的可扩展性，其性能远未达到最大值。我们假设，任务的时域（horizon）是导致离线RL可扩展性差的主要原因。我们通过一系列分析实验验证了这一假设，表明长时域确实构成了离线RL扩展的一个根本性障碍。随后，我们展示了多种时域缩减技术在挑战性任务上显著提升了算法的可扩展性。基于我们的研究发现，我们还提出了一种简单但具有可扩展性的方法SHARSA，该方法有效地缩减了时域。SHARSA在我们评估的方法中表现出最佳的渐近性能和可扩展性，表明显式地缩减时域能够释放离线RL的可扩展潜力。代码：<a target="_blank" rel="noopener" href="https://github.com/seohongpark/horizon-reduction">https://github.com/seohongpark/horizon-reduction</a></p>
<h3 id="WebGraphEval-Multi-Turn-Trajectory-Evaluation-for-Web-Agents-using-Graph-Representation"><a href="#WebGraphEval-Multi-Turn-Trajectory-Evaluation-for-Web-Agents-using-Graph-Representation" class="headerlink" title="WebGraphEval: Multi-Turn Trajectory Evaluation for Web Agents using Graph Representation"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19205v1">WebGraphEval: Multi-Turn Trajectory Evaluation for Web Agents using Graph Representation</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>目前对网络代理的评估大多简化为二元成功度量或对单一参考轨迹的符合度，忽视了基准数据集中存在的结构多样性。我们提出了WebGraphEval框架，该框架将多个代理的轨迹抽象为一个统一的加权动作图。这种表示方式可以直接与WebArena等基准测试兼容，无需修改环境即可利用排行榜运行结果和新收集的轨迹。该框架规范地编码动作，合并重复行为，并应用包括奖励传播和加权成功边统计在内的结构分析。通过对六种网络代理数千条轨迹的评估表明，图抽象能够捕捉跨模型的规律性，突出冗余和低效之处，并识别出基于结果指标所忽略的关键决策点。通过将网络交互视为图结构数据，WebGraphEval建立了一种通用方法，用于多路径、跨代理且关注效率的网络代理评估。</p>
<h3 id="ROTATE-Regret-driven-Open-ended-Training-for-Ad-Hoc-Teamwork"><a href="#ROTATE-Regret-driven-Open-ended-Training-for-Ad-Hoc-Teamwork" class="headerlink" title="ROTATE: Regret-driven Open-ended Training for Ad Hoc Teamwork"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.23686v2">ROTATE: Regret-driven Open-ended Training for Ad Hoc Teamwork</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>在多智能体学习中，学会与之前未曾见过的合作伙伴协作是一项根本性的泛化挑战，被称为即兴团队合作（Ad Hoc Teamwork，简称AHT）。现有的AHT方法通常采用两阶段流程：首先，生成一个固定数量的队友，假设这些队友能代表部署时会遇到的队友；其次，训练一个AHT智能体，使其能够与这些队友良好协作。迄今为止，研究社区主要专注于为每个阶段分别设计算法。这种分离导致了生成的队友行为覆盖有限，且忽略了这些生成的队友是否便于AHT智能体学习。此外，用于训练AHT智能体的算法通常将训练队友集视为静态的，试图在不假设对训练队友集有任何控制的情况下，泛化到之前未曾见过的合作伙伴智能体。本文通过将问题重新表述为AHT智能体与对抗性队友生成器之间的开放学习过程，提出了一个统一的AHT框架。我们引入了ROTATE，这是一种基于遗憾驱动的、开放式的训练算法，它交替地改进AHT智能体并生成能够探测其不足的队友。在多种双人环境中进行的实验表明，ROTATE在泛化到未见过的评估队友集方面显著优于基线方法，从而建立了稳健且可泛化的团队合作的新标准。</p>
<h3 id="Imbalanced-Gradients-in-RL-Post-Training-of-Multi-Task-LLMs"><a href="#Imbalanced-Gradients-in-RL-Post-Training-of-Multi-Task-LLMs" class="headerlink" title="Imbalanced Gradients in RL Post-Training of Multi-Task LLMs"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19178v1">Imbalanced Gradients in RL Post-Training of Multi-Task LLMs</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>大语言模型（LLM）的多任务后训练通常通过混合来自不同任务的数据集并进行联合优化来实现。这种方法隐式地假设所有任务产生的梯度幅度相似；当这一假设不成立时，优化过程会偏向于产生较大梯度的任务。然而，在本文中，我们发现这一假设在强化学习（RL）的后训练中并不成立：某些任务会产生显著更大的梯度，从而导致更新偏向这些任务。这种梯度不平衡只有在较大的梯度意味着任务上更大的学习收益（即更大的性能提升）时才合理——但我们发现这一情况并不成立。产生较大梯度的任务可能实现与产生较小梯度的任务相似甚至更低的学习收益。进一步分析表明，这些梯度不平衡无法用常见的训练统计量（如训练奖励或优势值）来解释，这表明它们源于任务之间的本质差异。这警示我们不要简单地进行数据集混合，并呼吁未来对LLM的梯度级校正进行更深入的研究。</p>
<h3 id="News-Aware-Direct-Reinforcement-Trading-for-Financial-Markets"><a href="#News-Aware-Direct-Reinforcement-Trading-for-Financial-Markets" class="headerlink" title="News-Aware Direct Reinforcement Trading for Financial Markets"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19173v1">News-Aware Direct Reinforcement Trading for Financial Markets</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>金融市场以对新闻高度敏感而著称。因此，有效将新闻数据整合到量化交易中仍然是一个重要挑战。现有方法通常依赖于人工设计的规则和&#x2F;或手工构建的特征。在本工作中，我们直接使用由大型语言模型得出的新闻情感得分，结合原始价格和成交量数据，作为强化学习的可观测输入。这些输入通过序列模型（如循环神经网络或Transformer）进行处理，以做出端到端的交易决策。我们以加密货币市场为例进行实验，并评估了两种代表性强化学习算法：双重深度Q网络（DDQN）和群体相对策略优化（GRPO）。实验结果表明，我们提出的基于新闻的方案，不依赖于手工构建的特征或人工设计的规则，其表现优于市场基准。我们进一步强调了时间序列信息在这一过程中的关键作用。</p>
<h3 id="X-Ego-Acquiring-Team-Level-Tactical-Situational-Awareness-via-Cross-Egocentric-Contrastive-Video-Representation-Learning"><a href="#X-Ego-Acquiring-Team-Level-Tactical-Situational-Awareness-via-Cross-Egocentric-Contrastive-Video-Representation-Learning" class="headerlink" title="X-Ego: Acquiring Team-Level Tactical Situational Awareness via Cross-Egocentric Contrastive Video Representation Learning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19150v1">X-Ego: Acquiring Team-Level Tactical Situational Awareness via Cross-Egocentric Contrastive Video Representation Learning</a></h3><p><strong>Categories:</strong> Reinforcement Learning, Robotics</p>
<p>人类团队战术源于每位球员的个体视角及其预测、理解和适应队友意图的能力。尽管视频理解技术的进步提高了对体育比赛中团队互动的建模能力，但大多数现有研究依赖于第三人称的广播视角，忽视了多智能体学习中同步的、以自我为中心的特性。我们引入了X-Ego-CS数据集，该数据集包含45场专业级《反恐精英2》（Counter-Strike 2）电子竞技比赛的124小时游戏画面，旨在促进在复杂三维环境中多智能体决策研究。X-Ego-CS提供了跨自我中心视角的视频流，同步捕捉所有玩家的第一人称视角及状态-动作轨迹。基于这一资源，我们提出了交叉自我中心对比学习（Cross-Ego Contrastive Learning，CECL），通过对齐队友的自我中心视觉流，从个体视角提升团队战术情境感知能力。我们在队友与对手位置预测任务上评估了CECL，展示了其通过最先进的视频编码器，从单一第一人称视角推断队友与对手位置的有效性。总体而言，X-Ego-CS与CECL为电子竞技中的跨自我中心多智能体基准测试奠定了基础。更广泛地说，我们的工作将游戏理解定位为多智能体建模和战术学习的试验场，对虚拟和现实世界中的时空推理与人机团队协作具有重要意义。代码和数据集可在<a target="_blank" rel="noopener" href="https://github.com/HATS-ICT/x-ego%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/HATS-ICT/x-ego获取。</a></p>
<h3 id="Hummer-Towards-Limited-Competitive-Preference-Dataset"><a href="#Hummer-Towards-Limited-Competitive-Preference-Dataset" class="headerlink" title="Hummer: Towards Limited Competitive Preference Dataset"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2405.11647v4">Hummer: Towards Limited Competitive Preference Dataset</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>偏好数据集对于将人类偏好融入预训练语言模型至关重要，是基于人类反馈的强化学习（Reinforcement Learning from Human Feedback）成功的关键。然而，这些数据集常常表现出相互冲突的对齐目标，从而增加了对越狱攻击的脆弱性，并在适应下游任务时面临优先考虑特定对齐目标而不影响其他目标的挑战。在本工作中，我们引入了一种新颖的统计度量标准——对齐维度冲突（Alignment Dimension Conflict），用于量化偏好数据集中冲突的程度。随后，我们提出了创新的成对偏好数据集 \texttt{Hummer} 及其细粒度变体 \texttt{Hummer-F}，这些数据集具有降低冲突的对齐目标。\texttt{Hummer} 基于 UltraFeedback 构建，并通过 GPT-4 提供的 AI 反馈进行增强，是首个旨在减少对齐目标之间竞争的偏好数据集。此外，我们开发了奖励模型 HummerRM 和 HummerRM-F，它们采用混合采样方法，有效地平衡了多样化的对齐目标。这种采样方法使 HummerRM 成为适用于特定领域进一步微调和降低攻击脆弱性的理想模型。</p>
<h2 id="Robotics"><a href="#Robotics" class="headerlink" title="Robotics"></a>Robotics</h2><h3 id="Compositional-Coordination-for-Multi-Robot-Teams-with-Large-Language-Models"><a href="#Compositional-Coordination-for-Multi-Robot-Teams-with-Large-Language-Models" class="headerlink" title="Compositional Coordination for Multi-Robot Teams with Large Language Models"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2507.16068v3">Compositional Coordination for Multi-Robot Teams with Large Language Models</a></h3><p><strong>Categories:</strong> Robotics</p>
<p>多机器人协调传统上依赖于针对特定任务和专家驱动的流程，其中领域专家手动将自然语言任务描述转换为数学公式、算法设计和可执行代码。这种传统方法耗费大量人力，对非专家用户不友好，并且难以适应任务需求的变化。在这里，我们提出了LAN2CB（Language to Collective Behavior），一种新颖的框架，利用大型语言模型（LLMs）来简化和推广多机器人协调流程。LAN2CB通过两个核心模块将自然语言（NL）任务描述转换为多机器人系统的可执行Python代码：（1）任务分析模块，该模块将任务描述解析为行为树；（2）代码生成模块，该模块利用行为树和结构化知识库生成机器人控制代码。我们进一步引入了一个自然语言任务描述的数据集，以支持开发和基准测试。在仿真和真实环境中的实验表明，LAN2CB能够从自然语言实现稳健且灵活的多机器人协调，显著减少了人工工程工作量，并支持在多种任务类型中的广泛泛化。网站：<a target="_blank" rel="noopener" href="https://sites.google.com/view/lan-cb">https://sites.google.com/view/lan-cb</a></p>
<h3 id="Semantic-World-Models"><a href="#Semantic-World-Models" class="headerlink" title="Semantic World Models"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19818v1">Semantic World Models</a></h3><p><strong>Categories:</strong> Robotics,World Models,Vision-Language Models</p>
<p>使用世界模型进行规划为机器人控制提供了一种强大的范式。传统方法通过当前帧和动作来训练模型预测未来的帧，然后可以将这些预测用于规划。然而，预测未来像素的目标通常与实际的规划目标相冲突；强大的像素重建并不总是与良好的规划决策相关。本文认为，与其重建未来的帧作为像素，世界模型只需预测与任务相关的未来语义信息即可。为此，本文将世界建模视为一个关于未来帧中语义信息的视觉问答问题。这种视角使我们能够使用支撑视觉语言模型的相同工具来处理世界建模。因此，可以通过在图像-动作-文本数据上的监督微调训练，将视觉语言模型训练为“语义”世界模型，从而实现用于决策制定的规划，同时继承预训练视觉语言模型的许多泛化能力和鲁棒性。本文展示了如何利用这种语义世界模型对开放式机器人任务进行策略改进，从而在基于重建的行动条件世界建模的典型范式之上实现显著的泛化性能提升。网站地址为：<a target="_blank" rel="noopener" href="https://weirdlabuw.github.io/swm%E3%80%82">https://weirdlabuw.github.io/swm。</a></p>
<h3 id="Learning-Affordances-at-Inference-Time-for-Vision-Language-Action-Models"><a href="#Learning-Affordances-at-Inference-Time-for-Vision-Language-Action-Models" class="headerlink" title="Learning Affordances at Inference-Time for Vision-Language-Action Models"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19752v1">Learning Affordances at Inference-Time for Vision-Language-Action Models</a></h3><p><strong>Categories:</strong> Robotics, Vision-Language Models</p>
<p>解决复杂的现实世界控制任务通常需要多次尝试：如果第一次尝试失败，我们会反思哪里出了问题，并相应地调整策略以避免重复同样的错误。在机器人领域，视觉-语言-动作模型（Vision-Language-Action models, VLAs）为解决复杂控制任务提供了一条有前景的路径，但它们在任务未能完成时缺乏根据上下文和动态地调整行为的能力。在本工作中，我们引入了一种名为“推理时执行学习”（Learning from Inference-Time Execution, LITEN）的方法，该方法将一个VLA的低级策略与一个基于过往经验的高级视觉语言模型（VLM）连接起来，通过将这些经验包含在上下文中，使VLM能够学习低级VLA的可用性和能力。我们的方法在推理阶段生成并执行低级VLA的计划，而在评估阶段则反思执行结果并得出有用的结论，以供未来推理上下文使用。与非机器人领域中类似的自我优化方法不同，LITEN必须对无结构的现实世界机器人轨迹（例如原始视频）进行反思，这需要在评估过程中引入结构化的引导机制。我们的实验结果表明，LITEN能够有效地从过往经验中学习，生成使用高可用性指令的计划，以完成长期目标任务。</p>
<h3 id="From-Forecasting-to-Planning-Policy-World-Model-for-Collaborative-State-Action-Prediction"><a href="#From-Forecasting-to-Planning-Policy-World-Model-for-Collaborative-State-Action-Prediction" class="headerlink" title="From Forecasting to Planning: Policy World Model for Collaborative State-Action Prediction"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19654v1">From Forecasting to Planning: Policy World Model for Collaborative State-Action Prediction</a></h3><p><strong>Categories:</strong> Robotics, World Models</p>
<p>尽管在构建世界模型方面取得了显著进展，但其在自主系统中的潜力仍远未被充分挖掘：目前的世界模型主要用于世界模拟，并与轨迹规划相分离。虽然近期的研究尝试在一个统一框架中融合世界建模与规划，但世界建模对规划的协同促进机制仍需进一步探索。在本工作中，我们引入了一种新的驾驶范式，称为策略世界模型（Policy World Model, PWM）。该范式不仅在一个统一的架构中集成了世界建模与轨迹规划，而且还能通过所提出的无动作未来状态预测方案，利用学习到的世界知识来提升规划效果。通过协作的状态-动作预测，PWM可以模仿人类类似的预判感知，从而实现更可靠的规划性能。为了提升视频预测的效率，我们进一步引入了一种动态增强的并行标记生成机制，该机制配备了基于上下文的标记器和自适应动态焦点损失。尽管仅利用前向摄像头输入，我们的方法在性能上与依赖多视角和多模态输入的最先进方法相匹配甚至超越了它们。代码和模型权重将在 <a target="_blank" rel="noopener" href="https://github.com/6550Zhao/Policy-World-Model">https://github.com/6550Zhao/Policy-World-Model</a> 上发布。</p>
<h3 id="Follow-the-STARs-Dynamic-ω-Regular-Shielding-of-Learned-Policies"><a href="#Follow-the-STARs-Dynamic-ω-Regular-Shielding-of-Learned-Policies" class="headerlink" title="Follow the STARs: Dynamic $ω$-Regular Shielding of Learned Policies"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.14689v3">Follow the STARs: Dynamic $ω$-Regular Shielding of Learned Policies</a></h3><p><strong>Categories:</strong> Robotics,Reinforcement Learning</p>
<p>本文提出了一种新颖的动态后屏蔽框架，该框架能够在预先计算的概率策略上强制执行完整的$\omega$-正则正确性属性。这标志着从主流的安全屏蔽设置（即确保任何不良事件都不会发生）向一种额外强制执行活性（即确保最终会发生有益事件）的屏蔽过程的范式转变。核心在于，我们的方法使用了基于策略模板的自适应运行时屏蔽（STARs），该方法利用宽容策略模板，以最小的干扰实现屏蔽。STARs的主要特点是引入了一种动态控制干扰的机制，允许一个可调节的强制执行参数在运行时平衡形式化义务与任务特定行为。这使得在需要时可以触发更加强烈的强制执行，而在其他情况下则允许优化的策略选择。此外，STARs支持在运行时适应变化的规范或执行器故障，使其特别适用于网络物理系统应用。我们在移动机器人基准测试中评估了STARs，以展示其在强制执行（逐步更新的）$\omega$-正则正确性属性时的可控干扰能力。</p>
<h3 id="RoboGPT-R1-Enhancing-Robot-Planning-with-Reinforcement-Learning"><a href="#RoboGPT-R1-Enhancing-Robot-Planning-with-Reinforcement-Learning" class="headerlink" title="RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.14828v2">RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning</a></h3><p><strong>Categories:</strong> Robotics,Reinforcement Learning,Vision-Language Models</p>
<p>提升具身智能体的推理能力对于机器人成功完成长期操作任务中的复杂人类指令至关重要。尽管基于监督微调（SFT）的大型语言模型和视觉语言模型在规划任务中取得了成功，但它们在复杂现实环境中的长期操作任务中仍面临挑战，原因在于其受限的常识和推理能力。考虑到通过监督微调将通用视觉语言模型对齐到机器人规划任务时存在泛化能力差和物理理解不足的问题，我们提出了RoboGPT-R1，这是一种用于具身规划的两阶段微调框架。在该框架中，通过专家序列进行监督训练以获取基础知识，随后利用强化学习（RL）来弥补模型在视觉空间理解和推理方面的不足。为了在多步推理任务中实现物理理解与动作序列的一致性，我们设计了一种基于规则的奖励函数，同时考虑了长期表现和环境中的动作约束。在Qwen2.5-VL-3B上训练的推理模型在EmbodiedBench基准测试中显著优于更大规模的模型GPT-4o-mini，性能提升了21.33%，并且在Qwen2.5-VL-7B上训练的其他工作中表现也优于其20.33%。</p>
<h3 id="RoboMemory-A-Brain-inspired-Multi-memory-Agentic-Framework-for-Interactive-Environmental-Learning-in-Physical-Embodied-Systems"><a href="#RoboMemory-A-Brain-inspired-Multi-memory-Agentic-Framework-for-Interactive-Environmental-Learning-in-Physical-Embodied-Systems" class="headerlink" title="RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Interactive Environmental Learning in Physical Embodied Systems"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.01415v5">RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Interactive Environmental Learning in Physical Embodied Systems</a></h3><p><strong>Categories:</strong> Robotics,Reinforcement Learning</p>
<p>具身智能体在现实环境中的应用面临持续的挑战，包括部分可观测性、有限的空间推理能力和高延迟的多记忆集成。我们提出了RoboMemory，这是一个受大脑启发的框架，通过并行化架构将空间、时间、事件和语义记忆统一起来，以实现高效的长时规划和交互式环境学习。一个动态的空间知识图谱（KG）确保了可扩展且一致的记忆更新，而带有批评模块的闭环规划器则支持在动态环境中进行自适应决策。在EmbodiedBench上的实验表明，基于Qwen2.5-VL-72B-Ins构建的RoboMemory，其平均成功率比基线模型提高了25%，并且超越了闭源的最先进模型Gemini-1.5-Pro 3%。现实世界测试进一步验证了其累积学习能力，其在重复任务中的表现持续提升。这些结果突显了RoboMemory作为增强型具身智能可扩展基础的重要性，弥合了认知神经科学与机器人自主性之间的鸿沟。</p>
<h3 id="Using-Non-Expert-Data-to-Robustify-Imitation-Learning-via-Offline-Reinforcement-Learning"><a href="#Using-Non-Expert-Data-to-Robustify-Imitation-Learning-via-Offline-Reinforcement-Learning" class="headerlink" title="Using Non-Expert Data to Robustify Imitation Learning via Offline Reinforcement Learning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19495v1">Using Non-Expert Data to Robustify Imitation Learning via Offline Reinforcement Learning</a></h3><p><strong>Categories:</strong> Robotics,Reinforcement Learning</p>
<p>模仿学习已被证明在利用专家人类示范训练机器人执行复杂任务方面是有效的。然而，其性能受到对高质量、任务特定数据的依赖性限制，这制约了其适应现实世界中多样化的物体配置和场景的能力。相比之下，非专家数据（如游戏数据、次优示范、部分任务完成或次优策略的执行轨迹）可以提供更广泛的覆盖范围和更低的数据采集成本。然而，传统的模仿学习方法无法有效利用这些数据。为了解决这些挑战，我们认为通过合理的策略设计，离线强化学习可以作为一种工具，用于利用非专家数据来增强模仿学习策略的性能。我们表明，虽然标准的离线强化学习方法在现实世界中通常遇到的稀疏数据覆盖条件下难以有效利用非专家数据，但通过简单的算法修改，可以在不引入显著额外假设的前提下利用这些数据。我们的方法表明，扩大策略分布的支持范围，可以让结合离线强化学习的模仿算法稳健地解决任务，并表现出显著增强的恢复和泛化能力。在操作任务中，这些创新显著提高了在引入非专家数据后，学习策略成功所需的初始条件范围。此外，我们还表明这些方法能够利用所有采集到的数据，包括部分或次优的示范，以提升任务导向策略的性能。这凸显了在机器人领域利用非专家数据进行稳健策略学习的算法技术的重要性。</p>
<h3 id="VideoAgentTrek-Computer-Use-Pretraining-from-Unlabeled-Videos"><a href="#VideoAgentTrek-Computer-Use-Pretraining-from-Unlabeled-Videos" class="headerlink" title="VideoAgentTrek: Computer Use Pretraining from Unlabeled Videos"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19488v1">VideoAgentTrek: Computer Use Pretraining from Unlabeled Videos</a></h3><p><strong>Categories:</strong> Robotics,Reinforcement Learning</p>
<p>训练计算机使用代理需要大量的图形用户界面（GUI）交互数据，但大规模地手动标注动作轨迹成本极高。我们提出了VideoAgentTrek，这是一个可扩展的流程，能够自动从公开的屏幕录制视频中挖掘训练数据，规模达到网络级别，从而消除了对人工标注的依赖。我们的方法解决了关键挑战：原始视频包含隐式演示，但缺乏显式的动作标签。为了解决这一问题，我们开发了Video2Action，这是一个逆动力学模块（IDM），包含两个组成部分：（1）一个视频定位模型，可以检测并精确定位GUI动作，具有精确的时间边界和上下文信息；（2）一个动作内容识别器，能够以高保真度提取结构化的参数，例如点击坐标和输入的文本。将我们的流程应用于39,000个YouTube教程视频后，自动生成了152万次交互步骤。我们通过持续预训练和随后的监督微调利用这些数据。在OSWorld-Verified数据集上，我们的方法将任务成功率从仅使用SFT基线的9.3%提升至15.8%，相对提升70%。在AgentNetBench上，步骤准确率从64.1%提升至69.3%。我们的结果表明，被动的互联网视频可以转化为高质量的监督信号，为计算机使用代理提供一种可扩展的替代方案，以取代昂贵的人工标注。</p>
<h3 id="NeSyPr-Neurosymbolic-Proceduralization-For-Efficient-Embodied-Reasoning"><a href="#NeSyPr-Neurosymbolic-Proceduralization-For-Efficient-Embodied-Reasoning" class="headerlink" title="NeSyPr: Neurosymbolic Proceduralization For Efficient Embodied Reasoning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19429v1">NeSyPr: Neurosymbolic Proceduralization For Efficient Embodied Reasoning</a></h3><p><strong>Categories:</strong> Robotics</p>
<p>我们探讨了在动态环境中采用语言模型（LMs）进行具身任务所面临的挑战，其中由于延迟、连接性和资源限制，无法在线访问大规模推理引擎或符号规划器。为此，我们提出了NeSyPr，一个新颖的具身推理框架，通过神经符号过程化来编译知识，从而为基于LM的智能体赋予结构化、自适应且及时的推理能力。在NeSyPr中，首先利用符号工具根据其声明性知识显式生成任务特定的计划。随后，这些计划被转换为可组合的过程表示，编码了计划的隐含生成规则，从而使得生成的组合过程能够无缝地融入LM的推理过程中。这种神经符号过程化将多步符号结构化路径查找和推理抽象并泛化为单步LM推理，类似于人类知识的编译。它支持在测试时高效推理，而无需依赖外部符号引导，因此非常适合部署在延迟敏感且资源受限的物理系统中。我们在具身基准PDDLGym、VirtualHome和ALFWorld上评估了NeSyPr，展示了其在大规模推理模型和符号规划器上的高效推理能力，同时使用更紧凑的LM。</p>
<h3 id="Open-World-Drone-Active-Tracking-with-Goal-Centered-Rewards"><a href="#Open-World-Drone-Active-Tracking-with-Goal-Centered-Rewards" class="headerlink" title="Open-World Drone Active Tracking with Goal-Centered Rewards"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2412.00744v2">Open-World Drone Active Tracking with Goal-Centered Rewards</a></h3><p><strong>Categories:</strong> Robotics,Reinforcement Learning</p>
<p>无人机视觉主动跟踪旨在通过基于视觉观测控制运动系统，实现对目标物体的自主跟随，为在动态环境中实现有效的跟踪提供更加实用的解决方案。然而，由于缺乏统一的基准测试平台以及开放世界环境中频繁干扰所带来的复杂性，使用强化学习实现精确的无人机视觉主动跟踪仍面临诸多挑战。为了解决这些问题，我们率先提出了一套系统性的解决方案。首先，我们提出了DAT，这是首个开放世界无人机主动空地跟踪基准测试平台。该基准涵盖24个城市级场景，具有类似人类行为的目标物体以及高保真度的动态模拟。DAT还提供了数字孪生工具，用于无限生成场景。此外，我们提出了一种新的强化学习方法，称为GC-VAT，旨在提升无人机在复杂场景中跟踪目标的性能。具体来说，我们设计了一种以目标为中心的奖励机制，为智能体提供跨视角的精确反馈，使其能够通过无限制的视角扩展感知和运动范围。受课程学习的启发，我们引入了一种基于课程的训练策略，逐步增强复杂环境下的跟踪性能。此外，我们在模拟器和真实世界图像上的实验表明，GC-VAT表现出卓越的性能，在模拟器上实现了约72%的跟踪成功率。该基准测试平台和代码可在<a target="_blank" rel="noopener" href="https://github.com/SHWplus/DAT_Benchmark%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/SHWplus/DAT_Benchmark上获取。</a></p>
<h3 id="Traffic-R1-Reinforced-LLMs-Bring-Human-Like-Reasoning-to-Traffic-Signal-Control-Systems"><a href="#Traffic-R1-Reinforced-LLMs-Bring-Human-Like-Reasoning-to-Traffic-Signal-Control-Systems" class="headerlink" title="Traffic-R1: Reinforced LLMs Bring Human-Like Reasoning to Traffic Signal Control Systems"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.02344v2">Traffic-R1: Reinforced LLMs Bring Human-Like Reasoning to Traffic Signal Control Systems</a></h3><p><strong>Categories:</strong> Robotics,Reinforcement Learning</p>
<p>我们介绍Traffic-R1，这是一个具有类人推理能力的30亿参数基础模型，用于交通信号控制（TSC）。该模型是在模拟交通环境中，通过专家指导下的大语言模型（LLM）的自我探索和迭代强化训练而成。与传统的强化学习方法和近期基于LLM的方法相比，Traffic-R1具有三大优势：一是零样本泛化能力，通过利用内部交通控制策略和推理，能够无缝迁移至新的道路网络和分布外事件；二是紧凑的30亿参数设计，支持在移动芯片上进行实时推理，适用于边缘部署；三是可解释的TSC过程，通过通信和异步通信网络实现多交叉口协调。大量基准测试表明，Traffic-R1在性能上优于强大的基线模型和训练密集型强化学习控制器。在实际部署中，该模型目前每天管理影响超过55,000名驾驶员的信号灯，平均排队长度减少了5%以上，并将操作员的工作量减半。我们的模型可在<a target="_blank" rel="noopener" href="https://huggingface.co/Season998/Traffic-R1">https://huggingface.co/Season998/Traffic-R1</a> 获取。</p>
<h2 id="Vision-Language-Models"><a href="#Vision-Language-Models" class="headerlink" title="Vision-Language Models"></a>Vision-Language Models</h2><h3 id="Surfer-2-The-Next-Generation-of-Cross-Platform-Computer-Use-Agents"><a href="#Surfer-2-The-Next-Generation-of-Cross-Platform-Computer-Use-Agents" class="headerlink" title="Surfer 2: The Next Generation of Cross-Platform Computer Use Agents"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19949v1">Surfer 2: The Next Generation of Cross-Platform Computer Use Agents</a></h3><p><strong>Categories:</strong> Vision-Language Models, World Models</p>
<p>在网页、桌面和移动环境中实现具有通用性的操作代理仍然是一个开放性挑战，因为以往的系统依赖于特定环境的接口，这限制了跨平台部署。我们提出了Surfer 2，这是一种统一的架构，完全基于视觉观察进行操作，其在所有三种环境中的表现均达到最先进的水平。Surfer 2集成了层次化上下文管理、解耦的规划与执行机制以及带有自验证和自适应恢复能力的系统，从而实现了长时间任务中的可靠运行。我们的系统在WebVoyager上达到97.1%的准确率，在WebArena上达到69.6%，在OSWorld上达到60.1%，在AndroidWorld上达到87.1%，在无需任务特定微调的情况下，优于所有先前系统。通过多次尝试，Surfer 2在所有基准测试中均超过了人类表现。这些结果表明，系统化的协调能够放大基础模型的能力，并通过视觉交互实现通用计算机控制，同时呼吁开发下一代视觉语言模型以实现帕累托最优的成本效率。</p>
<h3 id="Context-Aware-Pseudo-Label-Scoring-for-Zero-Shot-Video-Summarization"><a href="#Context-Aware-Pseudo-Label-Scoring-for-Zero-Shot-Video-Summarization" class="headerlink" title="Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.17501v3">Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>我们提出了一种基于评分标准引导、伪标签和提示驱动的零样本视频摘要框架，将大型语言模型与结构化的语义推理相结合。一小部分人工标注被转换为高置信度的伪标签，并根据数据集的特性组织成定义清晰评估维度的评分标准，例如主题相关性、动作细节和叙事进展。在推理过程中，边界场景（包括开头和结尾部分）会根据其自身的描述独立评分，而中间场景则结合相邻片段的简洁摘要，以评估叙事的连贯性和冗余性。这种设计使语言模型能够在无需参数调优的情况下，平衡局部显著性与全局一致性。在三个基准数据集上，所提出的方法实现了稳定且具有竞争力的结果，其F1分数分别为：SumMe上为57.58，TVSum上为63.05，QFVS上为53.79，分别比零样本基线高出0.85、0.84和0.37。这些结果表明，结合评分标准引导的伪标签与上下文提示的方法，能够有效稳定基于大语言模型的评分，并为通用型和查询聚焦型视频摘要建立了通用、可解释且无需训练的范式。</p>
<h3 id="Can-They-Dixit-Yes-they-Can-Dixit-as-a-Playground-for-Multimodal-Language-Model-Capabilities"><a href="#Can-They-Dixit-Yes-they-Can-Dixit-as-a-Playground-for-Multimodal-Language-Model-Capabilities" class="headerlink" title="Can They Dixit? Yes they Can! Dixit as a Playground for Multimodal Language Model Capabilities"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19892v1">Can They Dixit? Yes they Can! Dixit as a Playground for Multimodal Language Model Capabilities</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>多模态大语言模型（MLMs）通常在静态、独立的基准测试上进行评估——这些测试无法在单个任务中全面评估MLM的能力——或者依赖于人类或模型之间的成对比较——这种方法高度主观、成本高昂，并且允许模型利用表面的捷径（例如冗长）来提高胜率。为了解决这些问题，我们提出了基于游戏的评估方法，以全面评估MLM的能力。游戏要求玩家具备多种能力才能获胜，本质上具有竞争性，并且由固定、客观的规则所支配，使评估更具吸引力，为应对上述挑战提供了一个稳固的框架。我们通过Dixit这一幻想卡牌游戏来具体实现这种评估，该游戏要求玩家为一张卡牌生成描述，使部分但并非所有玩家误以为该卡牌被播放。我们与五种MLM进行的定量实验表明，Dixit中的胜率排名与主流MLM基准测试中的排名完全一致，而Dixit中人类与MLM玩家之间的游戏则揭示了代理策略之间的差异以及MLM推理方面的改进空间。</p>
<h3 id="ModServe-Modality-and-Stage-Aware-Resource-Disaggregation-for-Scalable-Multimodal-Model-Serving"><a href="#ModServe-Modality-and-Stage-Aware-Resource-Disaggregation-for-Scalable-Multimodal-Model-Serving" class="headerlink" title="ModServe: Modality- and Stage-Aware Resource Disaggregation for Scalable Multimodal Model Serving"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2502.00937v3">ModServe: Modality- and Stage-Aware Resource Disaggregation for Scalable Multimodal Model Serving</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>大型多模态模型（LMMs）在理解图像、视频和音频等非文本内容方面展现出令人印象深刻的能力。然而，由于其复杂的架构以及多阶段推理流水线中异构的特性，将LMMs高效地部署在生产环境中面临着显著的挑战。我们首次对两种主流的LMM架构（仅解码器架构和交叉注意力架构）进行了系统性的分析，覆盖了六个具有代表性的开源模型，揭示了关键的系统设计启示。此外，我们还对生产环境中的LMM推理轨迹进行了深入分析，发现了独特的负载特征，包括可变的、重尾分布的请求模式以及突发性的流量模式。基于这些洞察，我们提出了ModServe，一个模块化的LMM服务系统，它将各个阶段解耦，以便进行独立优化和自适应扩展。ModServe动态地重新配置各个阶段，并通过模态感知的调度和自动扩展来应对突发流量，以满足尾延迟的服务等级协议（SLO）同时最小化成本。在包含生产轨迹的128-GPU集群上，ModServe实现了3.3至5.5倍更高的吞吐量（从而带来25至41.3%的成本节省），同时满足SLO要求。</p>
<h3 id="I-Spy-With-My-Model’s-Eye-Visual-Search-as-a-Behavioural-Test-for-MLLMs"><a href="#I-Spy-With-My-Model’s-Eye-Visual-Search-as-a-Behavioural-Test-for-MLLMs" class="headerlink" title="I Spy With My Model’s Eye: Visual Search as a Behavioural Test for MLLMs"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19678v1">I Spy With My Model’s Eye: Visual Search as a Behavioural Test for MLLMs</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>多模态大语言模型（MLLMs）在视觉-语言任务中表现出色，但其视觉处理过程却具有一定的不透明性。大多数黑箱评估仅关注任务准确率，而未能揭示其内部机制。借鉴认知心理学的研究方法，我们采用经典视觉搜索范式——最初用于研究人类感知的范式——来检验MLLMs是否表现出“突出效应”（pop-out effect），即在干扰项数量不同的情况下，显著的视觉特征能够被独立检测出来。通过针对颜色、大小和光照特征进行受控实验，我们发现先进的MLLMs在基于颜色或大小的析取性（单一特征）搜索中表现出类似人类的突出效应，而在基于多个特征的联合性搜索中则表现出一定的容量限制。我们还发现证据表明，MLLMs像人类一样，会将自然场景的先验知识（如光照方向）整合到物体表征中。我们通过有针对性的微调和机制可解释性分析进一步验证了这些发现。我们的研究展示了如何利用视觉搜索作为认知基础的诊断工具，用于评估MLLMs的感知能力。</p>
<h3 id="AgentSense-LLMs-Empower-Generalizable-and-Explainable-Web-Based-Participatory-Urban-Sensing"><a href="#AgentSense-LLMs-Empower-Generalizable-and-Explainable-Web-Based-Participatory-Urban-Sensing" class="headerlink" title="AgentSense: LLMs Empower Generalizable and Explainable Web-Based Participatory Urban Sensing"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19661v1">AgentSense: LLMs Empower Generalizable and Explainable Web-Based Participatory Urban Sensing</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>基于网络的参与式城市感知已成为现代城市治理的关键方法，通过利用移动个体作为分布式传感器。然而，现有的城市感知系统在应对多样化城市场景时泛化能力有限，在决策过程中可解释性较差。在本研究中，我们提出了AgentSense，这是一个混合型、无需训练的框架，通过多智能体进化系统将大型语言模型（LLMs）集成到参与式城市感知中。AgentSense首先使用经典规划器生成基准解决方案，然后通过迭代优化，使其适应动态的城市环境和异构工作者的偏好，同时生成自然语言解释，以提升透明度和信任度。在两个大规模移动数据集和七种动态干扰类型的广泛实验中，结果表明AgentSense在适应性和可解释性方面相比传统方法具有明显优势。此外，与单智能体LLM基线方法相比，我们的方法在性能和鲁棒性方面表现更优，同时提供了更加合理和透明的解释。这些结果将AgentSense定位为在互联网上部署适应性与可解释性城市感知系统的重要进展。</p>
<h3 id="Style-Attack-Disguise-When-Fonts-Become-a-Camouflage-for-Adversarial-Intent"><a href="#Style-Attack-Disguise-When-Fonts-Become-a-Camouflage-for-Adversarial-Intent" class="headerlink" title="Style Attack Disguise: When Fonts Become a Camouflage for Adversarial Intent"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19641v1">Style Attack Disguise: When Fonts Become a Camouflage for Adversarial Intent</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>随着社交媒体的发展，用户开始使用风格化的字体和类似字体的emoji来表达个性，创造出既美观又可读的文本。然而，这些字体在自然语言处理（NLP）模型中引入了隐藏的漏洞：虽然人类可以轻松阅读风格化文本，但模型会将这些字符视为不同的标记，从而导致干扰。我们识别了人类与模型之间在感知上的差异，并提出了一种基于风格的攻击方法——风格攻击伪装（Style Attack Disguise，简称SAD）。我们设计了两种版本：轻量版用于提高查询效率，而强版则用于实现更出色的攻击效果。我们在传统模型、大型语言模型（LLMs）和商业服务上进行的实验表明，SAD具有强大的攻击能力。此外，我们还展示了SAD对包括文本到图像和文本到语音生成等多模态任务的潜在威胁。</p>
<h3 id="Learning-from-Videos-for-3D-World-Enhancing-MLLMs-with-3D-Vision-Geometry-Priors"><a href="#Learning-from-Videos-for-3D-World-Enhancing-MLLMs-with-3D-Vision-Geometry-Priors" class="headerlink" title="Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision Geometry Priors"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.24625v3">Learning from Videos for 3D World: Enhancing MLLMs with 3D Vision Geometry Priors</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>先前的研究已经探讨了将多模态大语言模型（MLLMs）应用于理解三维场景的方法，这些方法通常将三维场景解释为视频进行处理。这些方法通常依赖于全面的三维数据输入，如点云或重建的鸟瞰图（BEV）地图。在我们的研究中，我们通过增强MLLMs直接从视频数据中理解并推理三维空间的能力，从而推动了这一领域的发展，而无需额外的三维输入数据。我们提出了一种新颖且高效的方法，称为视频-三维几何大语言模型（Video-3D Geometry Large Language Model，简称VG LLM）。我们的方法利用三维视觉几何编码器，从视频序列中提取三维先验信息，然后将这些信息与视觉标记结合并输入到MLLM中。大量实验表明，我们的方法在与三维场景理解和空间推理相关的各种任务中取得了显著的性能提升，且所有能力都是直接从视频数据中学习得到的。令人印象深刻的是，我们的4B模型在不依赖显式三维数据输入的情况下，其性能与现有的最先进方法相媲美，甚至在VSI-Bench评估中超过了Gemini-1.5-Pro。</p>
<h3 id="Human-Agent-Collaborative-Paper-to-Page-Crafting-for-Under-0-1"><a href="#Human-Agent-Collaborative-Paper-to-Page-Crafting-for-Under-0-1" class="headerlink" title="Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19600v1">Human-Agent Collaborative Paper-to-Page Crafting for Under $0.1</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>在追求科学进步的过程中，研究成果的传播与发现本身同样重要。然而，研究人员常常被构建项目网页这一繁琐、重复的任务所分心，以便让他们的内容密集型论文更容易被访问。虽然自动化技术已经解决了静态幻灯片和海报的制作，但网页的动态性和交互性仍是一个未被解决的挑战。为弥合这一差距，我们将问题重新定义，认为解决方案并不在于单一的命令，而在于一个协作、分层的过程。我们引入了 $\textbf{AutoPage}$，这是一种全新的多智能体系统，体现了这一理念。AutoPage将从论文到网页的创建过程分解为一个由叙事规划到多模态内容生成再到交互式渲染的粗到细的流程。为对抗AI幻觉，专门的“检查者”智能体会将每一步与原始论文进行比对，同时可选的人工检查点确保最终成果完全符合作者的愿景，从而将系统从一个简单的工具转变为强大的协作助手。为了严格验证我们的方法，我们还构建了 $\textbf{PageBench}$，这是首个针对该新任务的基准测试集。实验表明，AutoPage不仅能生成高质量、视觉吸引力强的网页，而且在不到15分钟的时间内，成本低于0.1美元就能完成。代码和数据集将在 $\href{<a target="_blank" rel="noopener" href="https://mqleet.github.io/AutoPage_ProjectPage/%7D%7BWebpage%7D$">https://mqleet.github.io/AutoPage_ProjectPage/}{Webpage}$</a> 上发布。</p>
<h3 id="XBench-A-Comprehensive-Benchmark-for-Visual-Language-Explanations-in-Chest-Radiography"><a href="#XBench-A-Comprehensive-Benchmark-for-Visual-Language-Explanations-in-Chest-Radiography" class="headerlink" title="XBench: A Comprehensive Benchmark for Visual-Language Explanations in Chest Radiography"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19599v1">XBench: A Comprehensive Benchmark for Visual-Language Explanations in Chest Radiography</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>视觉-语言模型（VLMs）在医学图像理解方面最近表现出显著的零样本性能，但其基础能力——即文本概念与视觉证据之间的对齐程度——仍处于探索阶段。然而，在医学领域，可靠的基础能力对于可解释性和临床应用至关重要。在本研究中，我们提出了首个系统性基准，用于评估七种CLIP风格的VLM变体在胸部X光图像中的跨模态可解释性。我们利用交叉注意力和基于相似度的定位图生成视觉解释，并定量评估其与放射科医生标注区域在多种病理情况下的对齐程度。我们的分析揭示了以下几点：（1）尽管所有VLM变体在大且明确的病灶上都表现出合理的定位能力，但它们在小或弥漫性病灶上的性能显著下降；（2）在胸部X光专用数据集上预训练的模型相比在通用领域数据上训练的模型表现出更强的对齐能力；（3）模型的整体识别能力和基础能力之间存在强相关性。这些发现表明，尽管当前VLMs在识别能力方面表现出色，但在临床可靠的基础能力方面仍存在不足，强调在医学实践中部署前需要建立针对性的可解释性基准。XBench代码可在<a target="_blank" rel="noopener" href="https://github.com/Roypic/Benchmarkingattention%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Roypic/Benchmarkingattention获取。</a></p>
<h3 id="Detecting-Latin-in-Historical-Books-with-Large-Language-Models-A-Multimodal-Benchmark"><a href="#Detecting-Latin-in-Historical-Books-with-Large-Language-Models-A-Multimodal-Benchmark" class="headerlink" title="Detecting Latin in Historical Books with Large Language Models: A Multimodal Benchmark"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19585v1">Detecting Latin in Historical Books with Large Language Models: A Multimodal Benchmark</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>本文提出了一种新颖的任务，即从具有多种排版形式的混合语言历史文献中提取拉丁文片段。我们基于一个包含724页标注数据的多模态数据集，对大型基础模型的性能进行了基准测试与评估。实验结果表明，使用现代模型可以实现可靠的拉丁文检测。本研究首次对该任务中这些模型的能力和局限性进行了全面分析。</p>
<h3 id="A-Matter-of-Time-Revealing-the-Structure-of-Time-in-Vision-Language-Models"><a href="#A-Matter-of-Time-Revealing-the-Structure-of-Time-in-Vision-Language-Models" class="headerlink" title="A Matter of Time: Revealing the Structure of Time in Vision-Language Models"></a><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19559v1">A Matter of Time: Revealing the Structure of Time in Vision-Language Models</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>像CLIP这样的大规模视觉-语言模型（VLM）因其通用且表达性强的多模态表示而受到欢迎。通过利用具有多样化文本元数据的大规模训练数据，VLM获得了开放词汇能力，从而能够解决超出其训练范围的任务。本文研究了VLM的时间感知能力，评估它们将视觉内容定位在时间上的能力。我们引入了TIME10k，这是一个包含超过10,000张图像且带有时间真实值的基准数据集，并通过一种新颖的方法评估了37个VLM的时间感知能力。我们的研究发现，时间信息在VLM嵌入空间中沿着一个低维的非线性流形结构化。基于这一发现，我们提出了一种从嵌入空间中推导出显式“时间线”表示的方法。这些表示建模了时间及其时间顺序，从而有助于进行时间推理任务。与基于提示的基线方法相比，我们的时间线方法在准确率上具有竞争力，甚至更优，同时计算效率更高。所有代码和数据均可在<a target="_blank" rel="noopener" href="https://tekayanidham.github.io/timeline-page/%E8%8E%B7%E5%8F%96%E3%80%82">https://tekayanidham.github.io/timeline-page/获取。</a></p>
<h3 id="CARES-Context-Aware-Resolution-Selector-for-VLMs"><a href="#CARES-Context-Aware-Resolution-Selector-for-VLMs" class="headerlink" title="CARES: Context-Aware Resolution Selector for VLMs"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19496v1">CARES: Context-Aware Resolution Selector for VLMs</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>大型视觉-语言模型（VLMs）通常以原生或高分辨率处理图像，以在各种任务中保持有效性。这使得视觉标记通常占总标记的97%-99%，即使低分辨率图像就已足够，也会导致计算成本和延迟较高。我们引入了CARES——一种上下文感知的分辨率选择器（Context-Aware Resolution Selector），这是一个轻量级的预处理模块，它根据图像和查询对预测出最小的足够输入分辨率。CARES使用一个紧凑型VLM（3.5亿参数）提取特征，并预测目标预训练VLM的响应是否已收敛到其正确回答的最大能力。尽管CARES是在一组可选分辨率上训练为离散分类器的，但在推理过程中，它会对连续分辨率进行插值，以实现更精细的控制。在涵盖文档和自然图像的五个多模态基准测试中，以及多种目标VLMs上，CARES在保持任务性能的同时，计算成本最多可降低80%。</p>
<h3 id="EgoBlind-Towards-Egocentric-Visual-Assistance-for-the-Blind"><a href="#EgoBlind-Towards-Egocentric-Visual-Assistance-for-the-Blind" class="headerlink" title="EgoBlind: Towards Egocentric Visual Assistance for the Blind"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2503.08221v3">EgoBlind: Towards Egocentric Visual Assistance for the Blind</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>我们提出了EgoBlind，这是首个从盲人个体日常生活中收集的视角中心视频问答（VideoQA）数据集，用于评估当前多模态大语言模型（MLLMs）在辅助功能方面的表现。EgoBlind包含1,392段第一视角视频，记录了盲人和视觉障碍者的日常生活。该数据集还包含5,311个问题，这些问题由盲人直接提出或验证，以反映他们在实际情境中对视觉辅助的需求。每个问题平均有3个手动标注的参考答案，以减少主观性。通过EgoBlind，我们全面评估了16种先进的MLLMs，并发现所有模型的表现都不理想。表现最好的模型准确率接近60%，远低于人类87.4%的水平。为了指导未来的发展，我们识别并总结了现有MLLMs在盲人视角中心视觉辅助中的主要局限性，并探索了改进的启发式解决方案。通过这些努力，我们希望EgoBlind能成为开发有效AI助手的基础，以提升盲人和视觉障碍者的独立性。数据和代码可在<a target="_blank" rel="noopener" href="https://github.com/doc-doc/EgoBlind%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/doc-doc/EgoBlind获取。</a></p>
<h3 id="MUG-V-10B-High-efficiency-Training-Pipeline-for-Large-Video-Generation-Models"><a href="#MUG-V-10B-High-efficiency-Training-Pipeline-for-Large-Video-Generation-Models" class="headerlink" title="MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.17519v2">MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>近年来，用于视觉内容（例如图像、视频和3D对象&#x2F;场景）的大规模生成模型取得了显著进展。然而，由于跨模态的文本-视频对齐、涉及的长序列以及复杂的时空依赖关系，训练大规模视频生成模型仍然特别具有挑战性且资源消耗巨大。为了解决这些挑战，我们提出了一种训练框架，优化了四个核心支柱：（i）数据处理，（ii）模型架构，（iii）训练策略，以及（iv）大规模视频生成模型的基础设施。这些优化在数据预处理、视频压缩、参数扩展、基于课程的预训练以及以对齐为中心的后训练等所有阶段都带来了显著的效率提升和性能改进。我们的最终模型MUG-V 10B在整体上与最新的视频生成模型相当，并在面向电子商务的视频生成任务中，通过人类评估超越了领先的开源基线模型。更重要的是，我们开源了完整的系统栈，包括模型权重、基于Megatron-Core的大规模训练代码，以及用于视频生成和增强的推理流程。据我们所知，这是首个利用Megatron-Core实现高训练效率和近线性多节点扩展的大规模视频生成训练代码的公开发布，详情请参见<a target="_blank" rel="noopener" href="https://github.com/Shopee-MUG/MUG-V%E3%80%82">https://github.com/Shopee-MUG/MUG-V。</a></p>
<h3 id="Training-Free-Label-Space-Alignment-for-Universal-Domain-Adaptation"><a href="#Training-Free-Label-Space-Alignment-for-Universal-Domain-Adaptation" class="headerlink" title="Training-Free Label Space Alignment for Universal Domain Adaptation"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2509.17452v2">Training-Free Label Space Alignment for Universal Domain Adaptation</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>通用领域自适应（Universal Domain Adaptation，UniDA）是从有标签的源域向无标签的目标域迁移知识，其中标签空间可能不同，目标域可能包含私有类别。以往的UniDA方法主要集中在视觉空间对齐上，但由于内容差异，常常难以处理视觉歧义，这限制了其鲁棒性和泛化能力。为了解决这一问题，我们提出了一种新的方法，利用近期视觉-语言基础模型（VLMs）如CLIP所具备的强“零样本能力”，专注于标签空间对齐，以增强自适应的稳定性。CLIP可以根据仅有的标签名称生成任务相关的分类器。然而，将CLIP应用于UniDA具有挑战性，因为标签空间在事先并不完全已知。在本研究中，我们首先利用生成式视觉-语言模型来识别目标域中的未知类别。在发现的标签中，噪声和语义歧义（例如与源域标签相似的类别，如同义词、上义词、下义词）会使得标签对齐变得复杂。为了解决这一问题，我们提出了一种无需训练的UniDA标签空间对齐方法（\ours）。我们的方法通过过滤和精炼两个域之间的噪声标签，对齐标签空间，而不是视觉空间。随后，我们构建了一个“通用分类器”，整合了共享知识和目标私有类信息，从而在领域变化下提升泛化能力。实验结果表明，所提出的方法在关键的DomainBed基准测试中显著优于现有的UniDA技术，H-score平均提升\textcolor{blue}{+7.9%}，H$^3$-score平均提升\textcolor{blue}{+6.1%}。此外，结合自训练进一步提升了性能，并在H-和H$^3$-score中分别实现了额外的\textcolor{blue}{+1.6%}的提升。</p>
<h3 id="With-Limited-Data-for-Multimodal-Alignment-Let-the-STRUCTURE-Guide-You"><a href="#With-Limited-Data-for-Multimodal-Alignment-Let-the-STRUCTURE-Guide-You" class="headerlink" title="With Limited Data for Multimodal Alignment, Let the STRUCTURE Guide You"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2506.16895v2">With Limited Data for Multimodal Alignment, Let the STRUCTURE Guide You</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>多模态模型在需要多模态对齐的复杂任务中展现出强大的能力，包括零样本分类和跨模态检索。然而，现有的模型通常依赖数百万对多模态样本，而在许多领域中获取这些样本既昂贵又不可行。在本工作中，我们探索了在仅有少量配对数据的情况下构建多模态模型的可行性，方法是通过对齐预训练的单模态基础模型。我们展示了仅需数万个配对样本（通常用于该领域的数据量的不到1%）即可实现高质量的对齐。为了实现这一目标，我们引入了STRUCTURE，这是一种有效的正则化技术，能够保持单模态编码器潜在空间的邻域几何结构。此外，我们还表明对齐最后一层通常是次优的，并展示了对齐跨模态表示相似性最高的层所带来的优势。这两种组件可以轻松集成到现有的对齐方法中，在24个零样本图像分类和检索基准测试中取得了显著的提升，分类任务的平均相对提升达51.6%，检索任务的平均相对提升达91.8%。我们的结果突显了该框架在有限样本多模态学习中的有效性和广泛适用性，并为资源受限领域提供了一条有前景的发展路径。</p>
<h3 id="Towards-Enhanced-Image-Generation-Via-Multi-modal-Chain-of-Thought-in-Unified-Generative-Models"><a href="#Towards-Enhanced-Image-Generation-Via-Multi-modal-Chain-of-Thought-in-Unified-Generative-Models" class="headerlink" title="Towards Enhanced Image Generation Via Multi-modal Chain of Thought in Unified Generative Models"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2503.01298v2">Towards Enhanced Image Generation Via Multi-modal Chain of Thought in Unified Generative Models</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>统一生成模型在文本和图像生成任务中表现出色。对于图像合成任务，它们通常采用直接的文本到图像（T2I）生成方法。然而，直接的T2I生成方式限制了模型处理复杂组合指令的能力，而这类指令在现实场景中非常常见。尽管这一问题至关重要，但现有研究主要集中在提升模型的基本图像生成能力上。虽然这些改进在一定程度上有帮助，但仍然无法充分解决该问题。受“思维链”（Chain of Thought, CoT）逐步解决复杂问题的启发，本工作旨在将CoT引入统一生成模型，以应对直接T2I生成无法有效解决的复杂图像生成挑战，从而增强模型的图像生成能力。为实现这一目标，我们首先提出了功能导向专家（Functionality-oriented eXperts, FoXperts），这是我们模型FoX中的专家并行架构，根据功能分配专家。FoXperts解耦了主流模态导向设计中潜在的冲突，并为CoT提供了坚实的基础。在引入CoT时，首要问题是：如何为复杂的图像生成设计CoT？为此，我们模拟了类似人类的艺术创作流程——规划、执行、反思与修正，并提出了多模态思维链（Multimodal Chain of Thought, MCoT）方法，因为数据涉及文本和图像两种模态。为应对后续挑战——设计有效的MCoT训练范式，我们开发了一种多任务联合训练方案，以解耦的方式为模型赋予完成每个MCoT步骤所需的所有能力。该范式避免了收集一致多步骤数据元组的困难。大量实验表明，FoX在各种T2I基准测试中持续优于现有的统一模型，在复杂图像生成任务中表现出显著的性能提升。</p>
<h3 id="Merge-then-Realign-Simple-and-Effective-Modality-Incremental-Continual-Learning-for-Multimodal-LLMs"><a href="#Merge-then-Realign-Simple-and-Effective-Modality-Incremental-Continual-Learning-for-Multimodal-LLMs" class="headerlink" title="Merge then Realign: Simple and Effective Modality-Incremental Continual Learning for Multimodal LLMs"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2503.07663v2">Merge then Realign: Simple and Effective Modality-Incremental Continual Learning for Multimodal LLMs</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>多模态大语言模型（MLLMs）的最新进展增强了其多功能性，因为它们整合了越来越多的模态。考虑到训练MLLMs的成本较高，通过模态增量持续学习（MCL）重用现有模型并扩展至更多模态是一种高效的方法。目前，对MCL的探索仍处于早期阶段。在本研究中，我们深入探讨了MCL中性能下降的原因。我们发现，MCL不仅会像传统持续学习一样出现遗忘问题，还会出现模态无关组件与模态相关组件之间的不一致问题。为此，我们提出了一种名为“先合并后重新对齐”（MErge then ReAlign，简称MERA）的优雅且简单的MCL范式，以解决遗忘和不一致问题。MERA无需引入额外的模型预算或修改模型结构，因此易于部署且在MLLM社区中具有高度可重用性。大量实验表明，MERA表现出色，在扩展至四种模态时，平均保持率高达99.84%，实现了几乎无损失的MCL性能。我们的研究突显了MCL中的不一致问题。更广泛地来看，我们的工作展示了如何在持续学习过程中调整MLLMs的不同组件。</p>
<h3 id="PULSE-Practical-Evaluation-Scenarios-for-Large-Multimodal-Model-Unlearning"><a href="#PULSE-Practical-Evaluation-Scenarios-for-Large-Multimodal-Model-Unlearning" class="headerlink" title="PULSE: Practical Evaluation Scenarios for Large Multimodal Model Unlearning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2507.01271v3">PULSE: Practical Evaluation Scenarios for Large Multimodal Model Unlearning</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>近年来，去学习技术（unlearning techniques）作为一种方法，用于诱导模型“遗忘”先前学习到的信息，受到了广泛关注，因其可以解决大型语言模型（LLMs）和大型多模态模型（LMMs）中的隐私和版权问题。虽然已有多个去学习基准用于LLMs，但针对LMMs的去学习评估框架仍较少被探索。具体而言，现有的LMMs去学习基准仅考虑了模型通过单次去学习操作来去除微调知识的场景。在本研究中，我们引入了PULSE协议，从两个关键视角出发，为LMMs的现实去学习场景提供支持：(i) 预训练知识去学习，用于分析在不同知识获取阶段的效果；(ii) 长期可持续性评估，以应对连续请求。随后，我们沿着这些维度评估现有的去学习方法。研究结果表明，尽管某些技术能够成功去除通过微调获得的知识，但它们在消除预训练过程中学到的信息方面存在困难。此外，那些在单次操作中能够有效去除一批目标数据的方法，在将相同数据分批并按顺序进行去除时，会表现出显著的性能下降。</p>
<h3 id="See-Think-Act-Online-Shopper-Behavior-Simulation-with-VLM-Agents"><a href="#See-Think-Act-Online-Shopper-Behavior-Simulation-with-VLM-Agents" class="headerlink" title="See, Think, Act: Online Shopper Behavior Simulation with VLM Agents"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19245v1">See, Think, Act: Online Shopper Behavior Simulation with VLM Agents</a></h3><p><strong>Categories:</strong> Vision-Language Models,Reinforcement Learning</p>
<p>近年来，大语言模型（LLMs）在模拟在线购物者行为方面展现出强大的潜力。以往的研究通过在动作轨迹上应用监督微调（SFT）并结合LLM生成的推理理由，以及利用强化学习（RL）进一步提升推理能力，从而改善了动作预测。尽管取得了这些进展，当前的方法仍然依赖于基于文本的输入，忽视了视觉感知在网页GUI交互过程中对人类决策的关键作用。在本文中，我们通过视觉语言模型（VLMs）探索将视觉信息（特别是网页截图）整合到行为模拟中的方法，并利用OPERA数据集进行研究。通过在文本和视觉模态上对智能体的决策进行锚定，我们旨在缩小合成智能体与真实用户之间的差距，从而实现更符合人类认知的在线购物行为模拟。</p>
<p>具体而言，我们采用SFT进行联合动作预测和理由生成，并基于完整的交互上下文进行条件建模，该上下文包括动作历史、过去的HTML观察以及当前网页截图。为了进一步增强推理能力，我们结合强化学习，并引入一种基于难度感知因子的层次化奖励结构，优先处理具有挑战性的决策点。从实验结果来看，我们的研究表明，引入视觉锚定带来了显著的提升：结合文本和图像输入的方法，相较于仅使用文本输入，准确匹配率提高了超过6%。这些结果表明，多模态锚定不仅提升了预测的准确性，还增强了在视觉复杂环境中的模拟保真度，能够捕捉到文本型智能体通常忽略的人类注意力和决策的细微差别。最后，我们重新审视行为模拟框架的设计空间，识别出关键的方法论局限，并提出了构建高效且有效的人类行为模拟器的未来研究方向。</p>
<h3 id="Grasp-Any-Region-Towards-Precise-Contextual-Pixel-Understanding-for-Multimodal-LLMs"><a href="#Grasp-Any-Region-Towards-Precise-Contextual-Pixel-Understanding-for-Multimodal-LLMs" class="headerlink" title="Grasp Any Region: Towards Precise, Contextual Pixel Understanding for Multimodal LLMs"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18876v2">Grasp Any Region: Towards Precise, Contextual Pixel Understanding for Multimodal LLMs</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>尽管多模态大语言模型（MLLMs）在整体理解方面表现出色，但在捕捉复杂场景下的密集世界时却面临困难，需要对细节和物体之间的关系进行细致的分析。区域级的MLLMs已经是一个有前景的进展。然而，以往的尝试通常专注于理解孤立的区域，忽略了关键的全局上下文。为了解决这一问题，我们引入了“Grasp Any Region”（GAR），以实现全面的区域级视觉理解。通过一种有效的RoI对齐特征重放技术，GAR支持以下功能：(1) 利用必要的全局上下文实现精确感知；(2) 建模多个提示之间的交互。由此，它自然地实现了 (3) 高级的组合推理能力，从而回答关于任意区域的特定自由形式问题，将范式从被动描述转变为积极对话。此外，我们构建了GAR-Bench，它不仅提供了对单区域理解更准确的评估，更重要的是，还能够衡量多个区域之间的交互和复杂推理。大量实验表明，GAR-1B不仅保持了最先进的描述能力（例如在DLC-Bench上优于DAM-3B +4.5），还擅长建模多个提示之间的关系，具备高级的理解能力，甚至在GAR-Bench-VQA上超越了InternVL3-78B。更重要的是，我们的零样本GAR-8B在VideoRefer-BenchQ上甚至优于同领域的VideoRefer-7B，这表明其强大的能力可以轻松迁移到视频中。</p>
<h3 id="Chiron-o1-Igniting-Multimodal-Large-Language-Models-towards-Generalizable-Medical-Reasoning-via-Mentor-Intern-Collaborative-Search"><a href="#Chiron-o1-Igniting-Multimodal-Large-Language-Models-towards-Generalizable-Medical-Reasoning-via-Mentor-Intern-Collaborative-Search" class="headerlink" title="Chiron-o1: Igniting Multimodal Large Language Models towards Generalizable Medical Reasoning via Mentor-Intern Collaborative Search"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2506.16962v2">Chiron-o1: Igniting Multimodal Large Language Models towards Generalizable Medical Reasoning via Mentor-Intern Collaborative Search</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>多模态大语言模型（MLLMs）已经开始在一般任务中展现出强大的推理能力，但其在医疗领域的应用仍处于早期阶段。构建链式推理（Chain-of-Thought，CoT）训练数据对于增强医疗MLLMs的推理能力至关重要。然而，现有方法在提供全面的搜索和评估有效推理路径以实现关键诊断的框架方面存在不足。为了解决这一挑战，我们提出了Mentor-Intern协作搜索（MICS），这是一种新颖的推理路径搜索方案，用于生成严谨且有效的医疗CoT数据。MICS首先利用导师模型逐步初始化推理过程，然后提示每个实习生模型沿着这些初始路径继续推理，最后根据多个实习生模型的整体推理表现选择最优的推理路径。推理表现由MICS-Score评估，该评分机制衡量生成推理路径的质量。最终，我们构建了MMRP，这是一个具有难度分级的多任务医疗推理数据集，以及Chiron-o1，这是通过课程学习策略设计的新一代医疗MLLM，具备强大的视觉问答能力和可泛化的推理能力。大量实验表明，基于MICS构建的CoT数据集进行训练的Chiron-o1，在一系列医疗视觉问答和推理基准测试中均取得了最先进的性能。代码可在<a target="_blank" rel="noopener" href="https://github.com/manglu097/Chiron-o1%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/manglu097/Chiron-o1获取。</a></p>
<h3 id="Probing-Perceptual-Constancy-in-Large-Vision-Language-Models"><a href="#Probing-Perceptual-Constancy-in-Large-Vision-Language-Models" class="headerlink" title="Probing Perceptual Constancy in Large Vision-Language Models"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2502.10273v2">Probing Perceptual Constancy in Large Vision-Language Models</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>知觉恒常性是指在感觉输入发生变化（如距离、角度或光照变化）的情况下，保持对物体稳定知觉的能力。这种能力对于在动态世界中进行视觉理解至关重要。在本研究中，我们探讨了当前视觉语言模型（Vision Language Models，VLMs）的这种能力。本研究在三个领域（颜色、大小和形状恒常性）中通过236个实验评估了155个VLMs。这些实验包括经典认知任务的单图像和视频版本，以及在真实环境条件下的新任务。我们发现，在这些领域中，VLMs的表现存在显著差异，其中形状恒常性的表现明显与颜色和大小恒常性表现相分离。</p>
<h3 id="PruneHal-Reducing-Hallucinations-in-Multi-modal-Large-Language-Models-through-Adaptive-KV-Cache-Pruning"><a href="#PruneHal-Reducing-Hallucinations-in-Multi-modal-Large-Language-Models-through-Adaptive-KV-Cache-Pruning" class="headerlink" title="PruneHal: Reducing Hallucinations in Multi-modal Large Language Models through Adaptive KV Cache Pruning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19183v1">PruneHal: Reducing Hallucinations in Multi-modal Large Language Models through Adaptive KV Cache Pruning</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>尽管多模态大语言模型（MLLMs）近年来取得了显著进展，但幻觉问题仍然是一个主要的挑战。为了缓解这一现象，现有解决方案要么引入额外的数据进行进一步训练，要么在推理过程中结合外部或内部信息。然而，这些方法不可避免地增加了额外的计算成本。在本文中，我们观察到，MLLMs中的幻觉现象与对视觉标记分配的注意力不足密切相关。特别是，冗余的视觉标记会分散模型的注意力，使其无法专注于最具信息量的标记。因此，关键的视觉线索常常被忽视，这反过来加剧了幻觉的发生。基于这一观察，我们提出了一种名为PruneHal的方法，这是一种无需训练、简单而有效的方法，利用自适应KV缓存剪枝来增强模型对关键视觉信息的注意力，从而减轻幻觉问题。据我们所知，我们是第一个在MLLMs中应用标记剪枝以缓解幻觉的团队。值得注意的是，我们的方法不需要额外的训练，几乎不增加推理成本。此外，PruneHal是模型无关的，可以无缝集成到不同的解码策略中，包括专门为缓解幻觉设计的策略。我们在四个主流MLLMs上评估了PruneHal在多个广泛使用的幻觉评估基准上的表现，取得了稳健且出色的结果，突显了我们方法的有效性和优越性。我们的代码将公开发布。</p>
<h3 id="Text-or-Pixels-It-Takes-Half-On-the-Token-Efficiency-of-Visual-Text-Inputs-in-Multimodal-LLMs"><a href="#Text-or-Pixels-It-Takes-Half-On-the-Token-Efficiency-of-Visual-Text-Inputs-in-Multimodal-LLMs" class="headerlink" title="Text or Pixels? It Takes Half: On the Token Efficiency of Visual Text Inputs in Multimodal LLMs"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18279v2">Text or Pixels? It Takes Half: On the Token Efficiency of Visual Text Inputs in Multimodal LLMs</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>大语言模型（LLMs）及其多模态变体现在可以处理视觉输入，包括文本图像。这引发了一个引人入胜的问题：我们能否通过将文本输入作为图像输入，以减少令牌使用的同时保持性能不变？在本文中，我们展示了视觉文本表示是解码器LLMs的一种实用且出人意料的有效输入压缩形式。我们利用了将长文本输入渲染为单个图像并直接提供给模型的思路。这导致解码器所需的令牌数量大幅减少，提供了一种新的输入压缩方式。通过在两个不同的基准测试RULER（长上下文检索）和CNN&#x2F;DailyMail（文档摘要）上的实验，我们证明了这种将文本作为图像的方法能够带来显著的令牌节省（通常接近一半），而不会影响任务性能。</p>
<h3 id="FairGen-Controlling-Sensitive-Attributes-for-Fair-Generations-in-Diffusion-Models-via-Adaptive-Latent-Guidance"><a href="#FairGen-Controlling-Sensitive-Attributes-for-Fair-Generations-in-Diffusion-Models-via-Adaptive-Latent-Guidance" class="headerlink" title="FairGen: Controlling Sensitive Attributes for Fair Generations in Diffusion Models via Adaptive Latent Guidance"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2503.01872v2">FairGen: Controlling Sensitive Attributes for Fair Generations in Diffusion Models via Adaptive Latent Guidance</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>文本到图像的扩散模型通常会对特定的人口群体表现出偏差，例如在被提示生成工程师图像时，倾向于生成更多男性而非女性，这引发了伦理问题并限制了其应用。本文中，我们应对了在扩散模型中减轻任何目标属性值（例如“性别”中的“男性”）生成偏差的挑战，同时保持生成质量。我们提出了FairGen，这是一种自适应潜在空间引导机制，它在推理过程中控制生成分布。在FairGen中，一个潜在空间引导模块动态调整扩散过程以强制特定属性，而一个记忆模块跟踪生成统计信息，并引导潜在空间引导以与目标属性值的公平分布对齐。此外，我们通过引入全面偏差评估（Holistic Bias Evaluation，HBE）基准来解决现有数据集的局限性，该基准涵盖多样化的领域，并结合复杂的提示以更全面地评估偏差。在HBE和Stable Bias数据集上的广泛评估表明，FairGen优于现有的偏差缓解方法，实现了显著的偏差减少（例如，在Stable Diffusion 2中性别偏差减少了68.5%）。消融研究突显了FairGen能够灵活控制输出分布的能力，以任何用户指定的粒度进行调整，确保适应性和针对性的偏差缓解。</p>
<h2 id="World-Models"><a href="#World-Models" class="headerlink" title="World Models"></a>World Models</h2><h3 id="A-Tutorial-on-Cognitive-Biases-in-Agentic-AI-Driven-6G-Autonomous-Networks"><a href="#A-Tutorial-on-Cognitive-Biases-in-Agentic-AI-Driven-6G-Autonomous-Networks" class="headerlink" title="A Tutorial on Cognitive Biases in Agentic AI-Driven 6G Autonomous Networks"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19973v1">A Tutorial on Cognitive Biases in Agentic AI-Driven 6G Autonomous Networks</a></h3><p><strong>Categories:</strong> World Models</p>
<p>6G网络实现更高自主性的路径，不仅在于对关键性能指标（KPIs）的单纯优化。尽管KPIs在TM Forum的第1至第3级中实现了自动化收益，但它们仍然是数值上的抽象，仅作为通信网络真实本质的代理：无缝连接、公平性、适应性与弹性。真正的自主性要求能够感知并推理当前网络环境的实际情况。这种进展可通过“自主AI”（agentic AI）实现，其中基于大型语言模型（LLM）的智能代理能够感知多模态遥测数据、利用记忆进行推理、跨领域进行协商，并通过API执行操作，以实现多目标任务。然而，部署此类代理会引入来自人类设计的固有认知偏差的挑战，这些偏差可能扭曲推理、协商、工具使用和执行操作。本文在神经科学与人工智能之间，提供了一个关于一些已知偏差的教程，包括其分类、定义、数学表达、在电信系统中的出现以及对相关自主组件的影响。教程还提出了针对每种偏差的多种缓解策略。文章最后提供了两个实际用例，探讨了6G切片间与跨域管理中某些著名偏差的出现、影响及缓解效果。特别地，引入了锚定随机化、时间衰减和转折奖励等技术，专门针对锚定偏差、时间偏差和确认偏差。这避免了代理固守初始的高资源分配方案或最近的、确认先验假设的决策。通过基于更丰富和公平的过去经验来做出决策，例如在第二个用例中，自主协议的质量和勇气实现了延迟降低5倍、能耗降低约40%的效果。</p>
<h3 id="SparseWorld-A-Flexible-Adaptive-and-Efficient-4D-Occupancy-World-Model-Powered-by-Sparse-and-Dynamic-Queries"><a href="#SparseWorld-A-Flexible-Adaptive-and-Efficient-4D-Occupancy-World-Model-Powered-by-Sparse-and-Dynamic-Queries" class="headerlink" title="SparseWorld: A Flexible, Adaptive, and Efficient 4D Occupancy World Model Powered by Sparse and Dynamic Queries"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.17482v2">SparseWorld: A Flexible, Adaptive, and Efficient 4D Occupancy World Model Powered by Sparse and Dynamic Queries</a></h3><p><strong>Categories:</strong> World Models, Robotics</p>
<p>语义占用（Semantic occupancy）作为一种强大的表示方法，在世界模型中崭露头角，因其能够捕捉丰富的空间语义。然而，大多数现有的占用世界模型依赖于静态且固定的嵌入或网格结构，这在本质上限制了感知的灵活性。此外，它们对网格进行“原地分类”（in-place classification）的方法，可能与现实场景中动态且连续的特性存在潜在的不匹配。在本文中，我们提出了一种名为SparseWorld的新颖4D占用世界模型，该模型具有灵活性、自适应性和高效性，其核心是通过稀疏且动态的查询进行驱动。我们提出了一个范围自适应感知模块（Range-Adaptive Perception module），其中可学习的查询通过自身车辆状态进行调制，并结合时空关联信息，以实现远距离感知。为了有效捕捉场景的动态特性，我们设计了一个状态条件预测模块（State-Conditioned Forecasting module），该模块用回归引导的公式替代了基于分类的预测方法，从而精确地将动态查询与4D环境的连续性对齐。此外，我们专门设计了一种时序感知的自调度训练策略（Temporal-Aware Self-Scheduling training strategy），以实现平滑且高效的训练。大量实验表明，SparseWorld在感知、预测和规划任务中均取得了最先进的性能。全面的可视化结果和消融研究进一步验证了SparseWorld在灵活性、适应性和效率方面的优势。代码可在<a target="_blank" rel="noopener" href="https://github.com/MSunDYY/SparseWorld%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/MSunDYY/SparseWorld获取。</a></p>
<h3 id="Rethinking-Driving-World-Model-as-Synthetic-Data-Generator-for-Perception-Tasks"><a href="#Rethinking-Driving-World-Model-as-Synthetic-Data-Generator-for-Perception-Tasks" class="headerlink" title="Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19195v1">Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks</a></h3><p><strong>Categories:</strong> World Models</p>
<p>最近在驾驶世界模型方面的进展使得生成高质量的RGB视频或多模态视频成为可能。现有方法主要关注生成质量与可控性的相关指标。然而，它们常常忽视了对下游感知任务的评估，而这对于自动驾驶性能至关重要。现有方法通常采用一种训练策略：首先在合成数据上进行预训练，然后在真实数据上进行微调，导致训练轮次是基线方法（仅使用真实数据）的两倍。当我们增加基线方法的训练轮次时，合成数据所带来的优势变得微乎其微。为了充分展示合成数据的优势，我们引入了Dream4Drive，这是一种新颖的合成数据生成框架，旨在增强下游感知任务。Dream4Drive首先将输入视频分解为多个具有3D感知的引导图，随后将3D资产渲染到这些引导图上。最后，通过微调驾驶世界模型，生成经过编辑的、多视角的逼真视频，可用于训练下游感知模型。Dream4Drive在大规模生成多视角极端场景方面提供了前所未有的灵活性，显著提升了自动驾驶中的极端场景感知能力。为了促进未来的研究，我们还贡献了一个大规模的3D资产数据集DriveObj3D，涵盖驾驶场景中的典型类别，支持多样化的3D感知视频编辑。我们进行了全面的实验，证明Dream4Drive可以在各种训练轮次下有效提升下游感知模型的性能。项目链接：$\href{<a target="_blank" rel="noopener" href="https://wm-research.github.io/Dream4Drive/%7D%7Bthis/">https://wm-research.github.io/Dream4Drive/}{this\</a> https\ URL}$</p>
<h3 id="Multi-Agent-Design-Assistant-for-the-Simulation-of-Inertial-Fusion-Energy"><a href="#Multi-Agent-Design-Assistant-for-the-Simulation-of-Inertial-Fusion-Energy" class="headerlink" title="Multi-Agent Design Assistant for the Simulation of Inertial Fusion Energy"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.17830v2">Multi-Agent Design Assistant for the Simulation of Inertial Fusion Energy</a></h3><p><strong>Categories:</strong> World Models</p>
<p>惯性聚变能源如果能够实现，将有望提供几乎无限的清洁能源。然而，聚变系统的研发和工程设计需要在极端的能量和时间尺度上控制和操控物质；在这些条件下，决定物质行为的冲击物理和辐射传输现象非常复杂，需要开发、校准和使用预测性多物理场代码，以应对高度非线性和多方面的设计挑战。我们假设可以通过将人工智能推理模型与物理代码和模拟器结合，实现对聚变燃料胶囊的自主设计。在本文中，我们构建了一个多智能体系统，利用自然语言来探索与聚变能源相关的复杂物理领域。该智能体系统能够执行高阶多物理场惯性聚变计算代码。我们展示了多智能体设计助手的能力，它既能够协作又能够自主地操控、导航和优化胶囊几何结构，同时考虑高保真度的物理效应，最终通过逆向设计实现模拟点火。</p>
<h2 id="Rejected-Papers"><a href="#Rejected-Papers" class="headerlink" title="Rejected Papers"></a>Rejected Papers</h2><ul>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.02855v2">Constraint Satisfaction Approaches to Wordle: Novel Heuristics and Cross-Lexicon Validation</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.14449v2">Feature Selection and Regularization in Multi-Class Classification: An Empirical Study of One-vs-Rest Logistic Regression with Gradient Descent Optimization and L1 Sparsity Constraints</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.20075v1">LLMs can hide text in other text of the same length.ipynb</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.15001v2">VaultGemma: A Differentially Private Gemma Model</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2507.00418v2">Serving LLMs in HPC Clusters: A Comparative Study of Qualcomm Cloud AI 100 Ultra and NVIDIA Data Center GPUs</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.20061v1">Ask What Your Country Can Do For You: Towards a Public Red Teaming Model</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.06377v2">Relational Transformer: Toward Zero-Shot Foundation Models for Relational Data</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.16171v2">Bridging Symmetry and Robustness: On the Role of Equivariance in Enhancing Adversarial Robustness</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2503.15984v2">DIPLI: Deep Image Prior Lucky Imaging for Blind Astronomical Image Restoration</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.20039v1">Beyond One-Way Influence: Bidirectional Opinion Dynamics in Multi-Turn Human-LLM Interactions</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.01618v3">Don’t be lazy: CompleteP enables compute-efficient deep transformers</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.22601v2">Machine Unlearning under Overparameterization</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.20028v1">The Temporal Graph of Bitcoin Transactions</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2502.15090v3">ExpertLens: Activation steering features are highly interpretable</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.21364v2">Towards Interpretability Without Sacrifice: Faithful Dense Layer Decomposition with Mixture of Decoders</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.20020v1">Optimized Distortion in Linear Social Choice</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.20002v1">Forging GEMs: Advancing Greek NLP through Quality-Based Corpus Curation and Specialized Pre-training</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.20001v1">Beyond MedQA: Towards Real-world Clinical Decision Making in the Era of LLMs</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2504.01002v3">Token embeddings violate the manifold hypothesis</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19997v1">A Framework for the Adoption and Integration of Generative AI in Midsize Organizations and Enterprises (FAIGMOE)</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2410.05056v5">Transition of $α$-mixing in Random Iterations with Applications in Queuing Theory</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.14827v3">Text Generation Beyond Discrete Token Sampling</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19988v1">LLM-Augmented Symbolic NLU System for More Reliable Continuous Causal Statement Interpretation</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19975v1">Revisiting Zeroth-Order Optimization: Minimum-Variance Two-Point Estimators and Directionally Aligned Perturbations</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19967v1">LyriCAR: A Difficulty-Aware Curriculum Reinforcement Learning Framework For Controllable Lyric Translation</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19964v1">AI-Driven Personalized Learning: Predicting Academic Per-formance Through Leadership Personality Traits</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19957v1">A new wave of vehicle insurance fraud fueled by generative AI</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2509.19271v2">WolBanking77: Wolof Banking Speech Intent Classification Dataset</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19954v1">RELATE: A Schema-Agnostic Perceiver Encoder for Multimodal Relational Graphs</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19953v1">On the Optimal Construction of Unbiased Gradient Estimators for Zeroth-Order Optimization</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2501.13133v2">Graph Representation Learning with Diffusion Generative Models</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19897v1">Learning from Supervision with Semantic and Episodic Memory: A Reflective Approach to Agent Adaptation</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.24379v3">Unlearned but Not Forgotten: Data Extraction after Exact Unlearning in LLM</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19895v1">Large Language Model enabled Mathematical Modeling</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19799v1">Integrating Transparent Models, LLMs, and Practitioner-in-the-Loop: A Case of Nonprofit Program Evaluation</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19792v1">On Controlled Change: Generative AI’s Impact on Professional Authority in Journalism</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2506.08800v2">Measuring Data Science Automation: A Survey of Evaluation Tools for AI Assistants and Agents</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19779v1">AdaSPEC: Selective Knowledge Distillation for Efficient Speculative Decoders</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19771v1">Beyond Reactivity: Measuring Proactive Problem Solving in LLM Agents</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19767v1">SmartSwitch: Advancing LLM Reasoning by Overcoming Underthinking via Promoting Deeper Thought Exploration</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.08450v2">gLSTM: Mitigating Over-Squashing by Increasing Storage Capacity</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19889v1">From Optimization to Prediction: Transformer-Based Path-Flow Estimation to the Traffic Assignment Problem</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19738v1">Misalignment Bounty: Crowdsourcing AI Agent Misbehavior</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19728v1">Enabling Granular Subgroup Level Model Evaluations by Generating Synthetic Medical Time Series</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.15020v2">The Coverage Principle: How Pre-Training Enables Post-Training</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2503.18129v2">GeoBenchX: Benchmarking LLMs in Agent Solving Multistep Geospatial Tasks</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.07364v3">Base Models Know How to Reason, Thinking Models Learn When</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19698v1">RLIE: Rule Generation with Logistic Regression, Iterative Refinement, and Evaluation for Large Language Models</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19694v1">Do Prompts Reshape Representations? An Empirical Study of Prompting Effects on Embeddings</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19692v1">Toward Agentic Software Engineering Beyond Code: Framing Vision, Values, and Vocabulary</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2507.05101v2">PRING: Rethinking Protein-Protein Interaction Prediction from Pairs to Graphs</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19689v1">Serverless GPU Architecture for Enterprise HR Analytics: A Production-Scale BDaaS Implementation</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19687v1">Are Large Language Models Sensitive to the Motives Behind Communication?</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19685v1">Directive, Metacognitive or a Blend of Both? A Comparison of AI-Generated Feedback Types on Student Engagement, Confidence, and Outcomes</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.16853v2">Agentic Inequality</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.02511v2">Test-time Prompt Intervention</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2506.21374v2">Pay Attention to Small Weights</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19675v1">Study of Training Dynamics for Memory-Constrained Fine-Tuning</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19671v1">Explainable e-sports win prediction through Machine Learning classification in streaming</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.06293v2">BlockGPT: Spatio-Temporal Modelling of Rainfall via Frame-Level Autoregression</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19668v1">Unraveling Emotions with Pre-Trained Models</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19666v1">A Graph Engine for Guitar Chord-Tone Soloing Education</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2503.12730v5">TinySQL: A Progressive Text-to-SQL Dataset for Mechanistic Interpretability Research</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2410.10101v3">Learning Linear Attention in Polynomial Time</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.08666v3">dInfer: An Efficient Inference Framework for Diffusion Language Models</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2410.06019v2">Unveiling Transformer Perception by Exploring Input Manifolds</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19631v1">HSCodeComp: A Realistic and Expert-level Benchmark for Deep Search Agents in Hierarchical Rule Application</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2507.05916v3">On the Effectiveness of Methods and Metrics for Explainable AI in Remote Sensing Image Scene Classification</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2502.02197v3">An Efficient Local Search Approach for Polarized Community Discovery in Signed Networks</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2509.25434v2">The Open Syndrome Definition</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2507.15268v2">IM-Chat: A Multi-agent LLM Framework Integrating Tool-Calling and Diffusion Modeling for Knowledge Transfer in Injection Molding Industry</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19882v1">Quantifying Feature Importance for Online Content Moderation</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.16206v2">The Right to Be Remembered: Preserving Maximally Truthful Digital Memory in the Age of AI</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2312.10107v3">Towards Context-Aware Domain Generalization: Understanding the Benefits and Limits of Marginal Transfer Learning</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19593v1">A Goal-Driven Survey on Root Cause Analysis</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.19955v3">MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.12829v2">Mathematics with large language models as provers and verifiers</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19579v1">Multi-modal Co-learning for Earth Observation: Enhancing single-modality models via modality collaboration</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2509.05346v2">Benchmarking Large Language Models for Personalized Guidance in AI-Enhanced Learning</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2310.18608v3">Embedding in Recommender Systems: A Survey</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19544v1">Demonstrating Real Advantage of Machine-Learning-Enhanced Monte Carlo for Combinatorial Optimization</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19535v1">Insights into the Unknown: Federated Data Diversity Analysis on Molecular Data</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2509.21567v2">EEG-Based Consumer Behaviour Prediction: An Exploration from Classical Machine Learning to Graph Neural Networks</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19514v1">From Prototypes to Sparse ECG Explanations: SHAP-Driven Counterfactuals for Multivariate Time-Series Multi-class Classification</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.18817v2">High-order Equivariant Flow Matching for Density Functional Theory Hamiltonian Prediction</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2506.06407v3">TimeWak: Temporal Chained-Hashing Watermark for Time Series Data</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19497v1">Modeling realistic human behavior using generative agents in a multimodal transport system: Software architecture and Application to Toulouse</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2506.15591v3">One-Step Diffusion for Detail-Rich and Temporally Consistent Video Super-Resolution</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.01687v3">Explaining Time Series Classifiers with PHAR: Rule Extraction and Fusion from Post-hoc Attributions</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19484v1">KnowMol: Advancing Molecular Large Language Models with Multi-Level Chemical Knowledge</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19479v1">Graph Unlearning Meets Influence-aware Negative Preference Optimization</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19476v1">A Concrete Roadmap towards Safety Cases based on Chain-of-Thought Monitoring</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.21589v5">Middo: Model-Informed Dynamic Data Optimization for Enhanced LLM Fine-Tuning via Closed-Loop Learning</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19470v1">HybridEP: Scaling Expert Parallelism to Cross-Datacenter Scenario via Hybrid Expert&#x2F;Data Transmission</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2504.09909v2">Quantum Natural Language Processing: A Comprehensive Review of Models, Methods, and Applications</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.07711v2">Merging Embedded Topics with Optimal Transport for Online Topic Modeling on Data Streams</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2507.14909v2">The Endless Tuning. An Artificial Intelligence Design To Avoid Human Replacement and Trace Back Responsibilities</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2506.18167v4">Understanding Reasoning in Thinking Language Models via Steering Vectors</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.15727v2">Invoice Information Extraction: Methods and Performance Evaluation</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19425v1">Neural Variational Dropout Processes</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19423v1">MSC-Bench: A Rigorous Benchmark for Multi-Server Tool Orchestration</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19421v1">FairNet: Dynamic Fairness Correction without Performance Loss via Contrastive Conditional LoRA</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19420v1">Monitoring LLM-based Multi-Agent Systems Against Corruptions via Node Evaluation</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19875v1">Stream: Scaling up Mechanistic Interpretability to Long Context in LLMs via Sparse Attention</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19414v1">EchoFake: A Replay-Aware Dataset for Practical Speech Deepfake Detection</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19410v1">ToMMeR – Efficient Entity Mention Detection from Large Language Models</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.06098v2">MeanAudio: Fast and Faithful Text-to-Audio Generation with Mean Flows</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2509.26433v2">ACT: Agentic Classification Tree</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.16062v2">Can LLMs Correct Themselves? A Benchmark of Self-Correction in LLMs</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2412.01322v3">Explainable fault and severity classification for rolling element bearings using Kolmogorov-Arnold networks</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.16708v2">Natural Language Processing for Cardiology: A Narrative Review</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19365v1">The Massive Legal Embedding Benchmark (MLEB)</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.17950v2">Evaluating NLP Embedding Models for Handling Science-Specific Symbolic Expressions in Student Texts</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19361v1">AgenticMath: Enhancing LLM Reasoning via Agentic-based Math Data Generation</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19873v1">From Large to Small: Transferring CUDA Optimization Expertise via Reasoning Graph</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19358v1">M3-SLU: Evaluating Speaker-Attributed Reasoning in Multimodal Large Language Models</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19347v1">A New Type of Adversarial Examples</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.08727v2">Memorization-Compression Cycles Improve Generalization</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19345v1">Foundation Model Forecasts: Form and Function</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19342v1">To Use or to Refuse? Re-Centering Student Agency with Generative AI in Engineering Design Education</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2507.07780v2">Where are we with calibration under dataset shift in image classification?</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19334v1">Metadata Extraction Leveraging Large Language Models</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19329v1">Seabed-Net: A multi-task network for joint bathymetry estimation and seabed classification from remote sensing imagery in shallow waters</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19327v1">SORA-ATMAS: Adaptive Trust Management and Multi-LLM Aligned Governance for Future Smart Cities</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19322v1">Enabling Reconfiguration-Communication Overlap for Collective Communication in Optical Networks</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19321v1">Online Handwritten Signature Verification Based on Temporal-Spatial Graph Attention Transformer</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19303v1">Collaborative penetration testing suite for emerging generative AI algorithms</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19298v1">Knowledge and Common Knowledge of Strategies</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.14489v2">Reasoning Models Better Express Their Confidence</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19282v1">Enhancing Early Alzheimer Disease Detection through Big Data and Ensemble Few-Shot Learning</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2509.20214v2">Q-Palette: Fractional-Bit Quantizers Toward Optimal Bit Allocation for Efficient LLM Deployment</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.14623v3">LeapFactual: Reliable Visual Counterfactual Explanation Using Conditional Flow Matching</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2410.10481v4">Model-based Large Language Model Customization as Service</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19264v1">LAPRAD: LLM-Assisted PRotocol Attack Discovery</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19263v1">An Argumentative Explanation Framework for Generalized Reason Model with Inconsistent Precedents</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19261v1">ChatGPT Unveils Its Limits: Principles of Law Deliver Checkmate</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19257v1">FnRGNN: Distribution-aware Fairness in Graph Neural Network</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18221v2">The Emergence of Complex Behavior in Large-Scale Ecological Environments</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18193v2">FST.ai 2.0: An Explainable AI Ecosystem for Fair, Fast, and Inclusive Decision-Making in Olympic and Paralympic Taekwondo</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.20749v3">Can Agents Fix Agent Issues?</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18431v2">ScaleNet: Scaling up Pretrained Neural Networks with Incremental Parameters</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19212v1">No Intelligence Without Statistics: The Invisible Backbone of Artificial Intelligence</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2406.18851v2">LICO: Large Language Models for In-Context Molecular Optimization</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.17064v2">A Brain Cell Type Resource Created by Large Language Models and a Multi-Agent AI System for Collaborative Community Annotation</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2506.13579v2">Flexible-length Text Infilling for Discrete Diffusion Models</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19202v1">An Active Diffusion Neural Network for Graphs</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2504.03931v3">NAACL2025 Tutorial: Adaptation of Large Language Models</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2501.04961v4">Demystifying Domain-adaptive Post-training for Financial LLMs</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19866v1">An Evaluation of the Pedagogical Soundness and Usability of AI-Generated Lesson Plans Across Different Models and Prompt Frameworks in High-School Physics</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.15831v2">Who’s Asking? Investigating Bias Through the Lens of Disability Framed Queries in LLMs</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2509.18942v2">Data Efficient Adaptation in Large Language Models via Continuous Low-Rank Fine-Tuning</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19181v1">Interpretable Question Answering with Knowledge Graphs</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2503.12499v2">PTFA: An LLM-based Agent that Facilitates Online Consensus Building through Parallel Thinking</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19176v1">The Zero-Step Thinking: An Empirical Study of Mode Selection as Harder Early Exit in Reasoning Models</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2506.10946v2">GUARD: Guided Unlearning and Retention via Data Attribution for Large Language Models</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.15297v2">VERA-MH Concept Paper</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19172v1">When Facts Change: Probing LLMs on Evolving Knowledge with evolveQA</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.16309v2">MedRule-KG: A Knowledge-Graph–Steered Scaffold for Mathematical Reasoning with a Lightweight Verifier</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.17947v2">PLAGUE: Plug-and-play framework for Lifelong Adaptive Generation of Multi-turn Exploits</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.16705v2">An Analysis of Concept Bottleneck Models: Measuring, Understanding, and Mitigating the Impact of Noisy Annotations</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.07897v3">LongCodeBench: Evaluating Coding LLMs at 1M Context Windows</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2509.23426v2">Democratizing AI scientists using ToolUniverse</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.09160v2">A Multi-Task Foundation Model for Wireless Channel Representation Using Contrastive and Masked Autoencoder Learning</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.09660v3">Learning What Matters: Steering Diffusion via Spectrally Anisotropic Forward Noise</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19139v1">A Multi-faceted Analysis of Cognitive Abilities: Evaluating Prompt Methods with Large Language Models on the CONSORT Checklist</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19138v1">InvarGC: Invariant Granger Causality for Heterogeneous Interventional Time Series under Latent Confounding</a></li>
</ul>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=https://fan-jj24.github.io/2025/10/24/2025-10-22/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>  
    </footer>
  </div>

   
  
   
    
    <script src="https://cdn.staticfile.org/twikoo/1.4.18/twikoo.all.min.js"></script>
    <div id="twikoo" class="twikoo"></div>
    <script>
        twikoo.init({
            envId: ""
        })
    </script>
 
</article>

</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2025
        <i class="ri-heart-fill heart_icon"></i> Pop Fan
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>Visitors:<span id="busuanzi_value_site_uv"></span></span>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>Views:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>    
    </main>
    <div class="float_btns">
      <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

    </div>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="Instant Papers"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-3.6.0.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->
 
<script src="/js/tocbot.min.js"></script>

<script>
  tocbot.init({
    tocSelector: ".tocbot",
    contentSelector: ".article-entry",
    headingSelector: "h1, h2, h3, h4, h5, h6",
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: "main",
    positionFixedSelector: ".tocbot",
    positionFixedClass: "is-position-fixed",
    fixedSidebarOffset: "auto",
  });
</script>

<script src="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.staticfile.org/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/default-skin/default-skin.min.css">
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.js"></script>
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->

<!-- Katex -->

<!-- busuanzi  -->
 
<script src="/js/busuanzi-2.3.pure.min.js"></script>
 
<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.staticfile.org/clipboard.js/2.0.10/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->

<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
    

  </div>
</body>

</html>