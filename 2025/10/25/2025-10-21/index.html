<!DOCTYPE html>


<html lang="en">
  

    <head>
      <meta charset="utf-8" />
       
      <meta name="keywords" content="Papers, Instant, Customize, Reading" />
       
      <meta name="description" content="Customize your instant paper reading experience" />
      
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title>2025-10-21 |  Instant Papers</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="/favicon.ico" />
       
<link rel="stylesheet" href="/dist/main.css">

      
<link rel="stylesheet" href="/css/fonts/remixicon.css">

      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.staticfile.org/pace/1.2.4/pace.min.js"></script>
       
 

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"
      />
      <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script>

      <!-- mermaid -->
      
      <style>
        .swal2-styled.swal2-confirm {
          font-size: 1.6rem;
        }
      </style>
    <link rel="alternate" href="/atom.xml" title="Instant Papers" type="application/atom+xml">
</head>
  </html>
</html>


<body>
  <div id="app">
    
      
    <main class="content on">
      <section class="outer">
  <article
  id="post-2025-10-21"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h1 class="article-title sea-center" style="border-left:0" itemprop="name">
  2025-10-21
</h1>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2025/10/25/2025-10-21/" class="article-date">
  <time datetime="2025-10-25T03:13:49.276Z" itemprop="datePublished">2025-10-25</time>
</a>   
<div class="word_count">
    <span class="post-time">
        <span class="post-meta-item-icon">
            <i class="ri-quill-pen-line"></i>
            <span class="post-meta-item-text"> Word count:</span>
            <span class="post-count">71.9k</span>
        </span>
    </span>

    <span class="post-time">
        &nbsp; | &nbsp;
        <span class="post-meta-item-icon">
            <i class="ri-book-open-line"></i>
            <span class="post-meta-item-text"> Reading time≈</span>
            <span class="post-count">258 min</span>
        </span>
    </span>
</div>
 
    </div>
      
    <div class="tocbot"></div>




  
    <div class="article-entry" itemprop="articleBody">
       
  <h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>今日共搜索AI论文: 257<br>请注意，我只想阅读指定类别的论文。如果您需要其他类别，请与我联系。<br>那些不符合要求的论文将被拒绝，并在文本末尾显示。<br>目前的筛选类别<br>Reinforcement Learning, Robotics, Vision-Language Models, World Models, Large Language Models</p>
<p>今日共筛选论文: 188</p>
<h2 id="Large-Language-Models"><a href="#Large-Language-Models" class="headerlink" title="Large Language Models"></a>Large Language Models</h2><h3 id="That’s-Deprecated-Understanding-Detecting-and-Steering-Knowledge-Conflicts-in-Language-Models-for-Code-Generation"><a href="#That’s-Deprecated-Understanding-Detecting-and-Steering-Knowledge-Conflicts-in-Language-Models-for-Code-Generation" class="headerlink" title="That’s Deprecated! Understanding, Detecting, and Steering Knowledge Conflicts in Language Models for Code Generation"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19116v1">That’s Deprecated! Understanding, Detecting, and Steering Knowledge Conflicts in Language Models for Code Generation</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>本文探讨了大型语言模型（LLMs）在面对其参数知识与提示中包含的冲突信息之间的差异时的行为。基于先前的问题回答（QA）研究，我们将知识冲突的分析扩展到了代码生成领域。我们提出了一种与领域无关的框架，用于构建和解释此类冲突，并提出了针对代码冲突场景的新型评估方法和数据集。我们的实验表明，足够大的LLMs在其参数中编码了知识冲突的概念，使我们能够以高达**80.65%<strong>的准确率检测知识冲突。基于这些见解，我们展示了在激活层层面进行引导（steering）可以比随机基线提高高达</strong>12.6%**的引导成功率。然而，有效性在很大程度上取决于模型规模、任务领域和引导方向之间的平衡。实验代码和数据将在论文被接受后公开。</p>
<h3 id="Improving-Metacognition-and-Uncertainty-Communication-in-Language-Models"><a href="#Improving-Metacognition-and-Uncertainty-Communication-in-Language-Models" class="headerlink" title="Improving Metacognition and Uncertainty Communication in Language Models"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.05126v2">Improving Metacognition and Uncertainty Communication in Language Models</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>大语言模型（LLMs）在决策情境中的应用日益增多，但当它们在不表明低置信度的情况下提供答案时，用户可能会在不知情的情况下采取错误的输出。先前的研究表明，LLMs内部存在不确定性信号，但其表达的置信度常常校准不良，并且在区分正确与错误答案方面表现不佳。我们研究监督微调是否能提升模型表达不确定性的能力，以及这种改进是否能在不同任务和领域中推广。我们使用涵盖一般知识、数学和开放式趣味问题的数据集对LLMs进行微调，并评估两个元认知任务：(1) 单题置信度估计，即模型为其答案赋予一个数值的确定性；(2) 成对置信度比较，即模型选择两个答案中更可能正确回答的那个。我们评估了模型在未见过的领域（如医疗和法律推理）中的泛化能力。结果显示，微调在不同领域内和跨领域中提高了校准度（即陈述的置信度与实际准确度的一致性）和区分度（正确回答比错误回答具有更高的置信度）。然而，这些提升是任务特定的：单题校准训练无法迁移到成对比较任务，反之亦然。多任务微调则带来了更广泛的提升，在跨领域评估中降低了校准误差并增强了区分度。这表明，LLMs的不确定性表达是可训练的，但需要多任务训练才能有效泛化。</p>
<h3 id="What-Makes-a-Good-Curriculum-Disentangling-the-Effects-of-Data-Ordering-on-LLM-Mathematical-Reasoning"><a href="#What-Makes-a-Good-Curriculum-Disentangling-the-Effects-of-Data-Ordering-on-LLM-Mathematical-Reasoning" class="headerlink" title="What Makes a Good Curriculum? Disentangling the Effects of Data Ordering on LLM Mathematical Reasoning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19099v1">What Makes a Good Curriculum? Disentangling the Effects of Data Ordering on LLM Mathematical Reasoning</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>课程学习（Curriculum Learning, CL）——按照从简单到困难的顺序组织训练数据——已成为提升大型语言模型（LLMs）推理能力的一种流行策略。然而，以往的研究采用了多种不同的难度度量标准和训练设置，留下了一些基本问题尚未解决：何时课程学习会有帮助？正向课程（forward curriculum）和反向课程（reverse curriculum）哪种更好？答案是否依赖于我们所测量的内容？我们通过一个统一的离线评估框架来回答这些问题，该框架将课程难度分解为五个互补的维度：问题难度、模型惊讶度（Model Surprisal）、置信度边界（Confidence Margin）、预测不确定性（Predictive Uncertainty）和决策可变性（Decision Variability）。通过在数学推理基准测试上使用Llama3.1-8B、Mistral-7B和Gemma3-4B进行受控的后训练实验，我们发现：（i）没有任何一种课程策略能普遍占优——正向与反向课程学习的相对有效性取决于模型的能力和任务的复杂性；（ii）即使在同一度量标准下，不同难度级别的样本也会根据任务需求产生不同的收益；（iii）任务对齐的课程侧重于塑造模型的最终表示和泛化能力，而内部状态课程则调节模型的内部状态，如置信度和不确定性。我们的研究结果挑战了存在通用课程策略的观念，并为不同模型和任务范围提供了可操作的指导，某些度量指标还表明，优先处理决策不确定性较高的样本可以进一步提升学习效果。</p>
<h3 id="Concept-Guided-Interpretability-via-Neural-Chunking"><a href="#Concept-Guided-Interpretability-via-Neural-Chunking" class="headerlink" title="Concept-Guided Interpretability via Neural Chunking"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.11576v3">Concept-Guided Interpretability via Neural Chunking</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>神经网络通常被描述为“黑箱”，这反映了理解其内部工作机制和交互的显著挑战。我们提出了一种不同的观点，挑战这一主流看法：神经网络在其原始群体活动模式中表现出与训练数据中规律性相似的模式。我们将这一观点称为“反射假设”，并在简单循环神经网络（RNNs）和复杂的大语言模型（LLMs）中提供了这一现象的证据。基于这一见解，我们提出利用人类的认知倾向——将信息分块（chunking）——将高维的神经群体动态分割为可解释的单元，这些单元能够反映潜在的概念。我们提出了三种方法，以在神经群体层面上提取重复出现的分块，这些方法根据标签的可用性和神经数据的维度进行互补。离散序列分块（DSC）在一个低维神经空间中学习实体的字典；群体平均（PA）提取与已知标签相对应的重复实体；而无监督分块发现（UCD）则在标签缺失时使用。我们展示了这些方法在提取与模型架构无关的概念编码实体方面的有效性。这些概念可以是具体的（如词语），抽象的（如词性标签），或结构性的（如叙事结构）。此外，我们还表明，提取出的分块在神经网络行为中发挥因果作用，因为将它们植入模型中会导致其行为产生可控且可预测的变化。我们的工作指出了可解释性研究的一个新方向，这一方向结合了认知原理和自然数据的结构，揭示复杂学习系统中的隐藏计算，逐渐将这些系统从“黑箱”转变为我们可以开始理解的系统。</p>
<h3 id="AgentTTS-Large-Language-Model-Agent-for-Test-time-Compute-optimal-Scaling-Strategy-in-Complex-Tasks"><a href="#AgentTTS-Large-Language-Model-Agent-for-Test-time-Compute-optimal-Scaling-Strategy-in-Complex-Tasks" class="headerlink" title="AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.00890v2">AgentTTS: Large Language Model Agent for Test-time Compute-optimal Scaling Strategy in Complex Tasks</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>测试时扩展（Test-time scaling, TTS）通过在推理过程中分配额外的计算资源，从而提升大型语言模型（LLMs）的性能。然而，现有研究主要集中在单阶段任务中的TTS；而许多现实世界问题则是多阶段的复杂任务，由一系列异构子任务组成，每个子任务都需要具备特定能力的LLM。因此，我们研究了一个新问题：多阶段复杂任务中的测试时计算最优扩展，旨在为每个子任务选择合适的模型并分配预算，以最大化整体性能。在多阶段任务中进行TTS，带来了两个根本性的挑战：(i) 模型和预算分配的组合搜索空间，加上推理的高成本，使得穷举搜索不可行；(ii) 子任务之间最优的模型和预算分配相互依赖，增加了计算最优搜索的复杂性。为了解决这一问题，我们在六个数据集上的四个任务上进行了广泛的试点实验，得出了三个经验性见解，揭示了LLM在多阶段复杂任务中的行为特征。基于这些见解，我们提出了AgentTTS，这是一个基于LLM代理的框架，通过与执行环境进行迭代的反馈驱动交互，自主地搜索计算最优的分配方案。实验结果表明，AgentTTS在搜索效率方面显著优于传统方法和其他基于LLM的基线方法，并且在训练集大小变化时表现出更强的鲁棒性，同时具备更高的可解释性。</p>
<h3 id="CtrlDiff-Boosting-Large-Diffusion-Language-Models-with-Dynamic-Block-Prediction-and-Controllable-Generation"><a href="#CtrlDiff-Boosting-Large-Diffusion-Language-Models-with-Dynamic-Block-Prediction-and-Controllable-Generation" class="headerlink" title="CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block Prediction and Controllable Generation"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.14455v2">CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block Prediction and Controllable Generation</a></h3><p><strong>Categories:</strong> Large Language Models,Reinforcement Learning</p>
<p>尽管自回归模型在近年来的语言建模中占据主导地位，但人们开始对传统“下一个词预测”框架的替代范式表现出日益浓厚的兴趣。基于扩散的语言模型因其强大的并行生成能力和内在的可编辑性，成为一种有吸引力的替代方案。然而，这些模型通常受到固定长度生成的限制。一个有前景的方向是结合两种范式的优点，将序列分割为块，同时在块之间建模自回归依赖关系，并利用离散扩散来估计给定先前上下文的每个块的条件分布。然而，其实际应用常常受到两个关键限制的阻碍：刚性的固定长度输出和缺乏灵活的控制机制。在本工作中，我们解决了当前大型扩散语言模型中固定粒度和弱可控性这两个关键限制。我们提出了CtrlDiff，这是一种动态且可控的半自回归框架，它利用强化学习根据局部语义自适应地确定每个生成块的大小。此外，我们引入了一种针对离散扩散的分类器引导控制机制，该机制在不重新训练的情况下显著降低了计算开销，并促进了高效的后处理条件生成。大量实验表明，CtrlDiff在混合扩散模型中设定了新的标准，缩小了与最先进自回归方法之间的性能差距，并在多种任务中实现了有效的条件文本生成。</p>
<h3 id="The-MUSE-Benchmark-Probing-Music-Perception-and-Auditory-Relational-Reasoning-in-Audio-LLMS"><a href="#The-MUSE-Benchmark-Probing-Music-Perception-and-Auditory-Relational-Reasoning-in-Audio-LLMS" class="headerlink" title="The MUSE Benchmark: Probing Music Perception and Auditory Relational Reasoning in Audio LLMS"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19055v1">The MUSE Benchmark: Probing Music Perception and Auditory Relational Reasoning in Audio LLMS</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>多模态大语言模型（MLLMs）在音频理解方面展现出了能力，但当前的评估可能掩盖了关系推理中的基本缺陷。我们引入了音乐理解与结构评估（MUSE）基准测试，这是一个包含10项任务的开源资源，旨在探究基础的音乐感知能力。我们对四个最先进的模型（Gemini Pro和Flash、Qwen2.5-Omni以及Audio-Flamingo 3）进行了评估，并与一个大规模人类基线（N&#x3D;200）进行比较。我们的结果揭示了这些最先进模型能力的广泛差异，并且与人类专家之间仍存在持续的差距。虽然Gemini Pro在基础感知任务上表现良好，但Qwen和Audio Flamingo 3的性能接近随机水平，暴露出严重的感知缺陷。此外，我们还发现链式思维（Chain-of-Thought，CoT）提示方法带来的结果往往不一致，甚至具有负面影响。我们的工作为评估不变的音乐表示提供了一个关键工具，并推动了更鲁棒人工智能系统的开发。</p>
<h3 id="ALHD-A-Large-Scale-and-Multigenre-Benchmark-Dataset-for-Arabic-LLM-Generated-Text-Detection"><a href="#ALHD-A-Large-Scale-and-Multigenre-Benchmark-Dataset-for-Arabic-LLM-Generated-Text-Detection" class="headerlink" title="ALHD: A Large-Scale and Multigenre Benchmark Dataset for Arabic LLM-Generated Text Detection"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.03502v2">ALHD: A Large-Scale and Multigenre Benchmark Dataset for Arabic LLM-Generated Text Detection</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>我们介绍了ALHD，这是首个大规模的综合性阿拉伯语数据集，专门设计用于区分人类生成文本和大型语言模型（LLM）生成文本。ALHD涵盖三个文体（新闻、社交媒体和评论），包括现代标准阿拉伯语（MSA）和方言阿拉伯语，包含由三个领先的LLM生成的超过40万个平衡样本，这些样本来源于多个人类来源，从而支持对阿拉伯语LLM生成文本检测的泛化能力研究。我们提供了严谨的预处理、丰富的标注和标准化的平衡划分，以支持可重复性。此外，我们使用该新数据集进行了基准实验，并分析和讨论了实验结果，从而识别出研究中的不足之处并提出了未来的研究方向。通过在传统分类器、基于BERT的模型和LLM（零样本和少样本）之间进行基准测试，我们发现微调后的BERT模型表现具有竞争力，优于基于LLM的模型。然而，结果并不总是保持一致，因为我们观察到在不同文体之间进行泛化时存在挑战；事实上，当模型需要在跨文体设置中处理未见过的模式时，它们往往难以泛化，而这些挑战在处理新闻文章时尤为突出，因为在新闻文章中，LLM生成的文本在风格上与人类文本非常相似，这为未来的研究开辟了新的方向。ALHD为阿拉伯语LLM检测相关的研究以及缓解虚假信息、学术不端行为和网络威胁的风险奠定了基础。</p>
<h3 id="CLiVR-Conversational-Learning-System-in-Virtual-Reality-with-AI-Powered-Patients"><a href="#CLiVR-Conversational-Learning-System-in-Virtual-Reality-with-AI-Powered-Patients" class="headerlink" title="CLiVR: Conversational Learning System in Virtual Reality with AI-Powered Patients"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19031v1">CLiVR: Conversational Learning System in Virtual Reality with AI-Powered Patients</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>模拟是医学和护理教育中的基本组成部分，传统上采用标准化病人（SP）和高保真模拟人偶来培养临床推理和沟通技能。然而，这些方法需要大量资源，限制了其可及性和可扩展性。在本研究中，我们介绍了CLiVR，这是一个结合大型语言模型（LLMs）、语音处理和3D虚拟形象的虚拟现实（VR）对话学习系统，用于模拟真实的医患互动。CLiVR是在Unity平台上开发并部署在Meta Quest 3平台上的，使学员能够与虚拟患者进行自然对话。每个模拟场景均从综合征-症状数据库中动态生成，并结合情感分析以提供沟通语气的反馈。通过一项涉及医学系教师（n&#x3D;13）的专家用户研究，我们评估了系统的可用性、真实性和感知的教育影响。结果显示，用户接受度高，对教育潜力充满信心，并提供了有价值的改进建议。CLiVR为基于标准化病人的培训提供了一种可扩展且沉浸式的补充方式。</p>
<h3 id="FlexiDataGen-An-Adaptive-LLM-Framework-for-Dynamic-Semantic-Dataset-Generation-in-Sensitive-Domains"><a href="#FlexiDataGen-An-Adaptive-LLM-Framework-for-Dynamic-Semantic-Dataset-Generation-in-Sensitive-Domains" class="headerlink" title="FlexiDataGen: An Adaptive LLM Framework for Dynamic Semantic Dataset Generation in Sensitive Domains"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19025v1">FlexiDataGen: An Adaptive LLM Framework for Dynamic Semantic Dataset Generation in Sensitive Domains</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>在机器学习中，数据集的可用性和质量仍然是关键挑战，尤其是在数据稀缺、获取成本高昂或受隐私法规限制的领域。医疗、生物医学研究和网络安全等领域经常面临高昂的数据获取成本、有限的标注数据访问权限以及关键事件的稀有性或敏感性。这些问题统称为“数据集挑战”，阻碍了在这些高风险领域中开发准确且具有广泛适用性的机器学习模型。为了解决这一问题，我们引入了FlexiDataGen，这是一种适用于敏感领域动态语义数据集生成的自适应大型语言模型（LLM）框架。FlexiDataGen能够自主生成丰富、语义连贯且语言多样化的定制化数据集，以满足特定领域的需要。该框架集成了四个核心组件：（1）句法-语义分析；（2）检索增强生成；（3）动态元素注入；（4）结合语义验证的迭代改写。这些组件共同确保生成高质量且符合领域需求的数据。实验结果表明，FlexiDataGen有效缓解了数据短缺和标注瓶颈，从而实现了可扩展且精确的机器学习模型开发。</p>
<h3 id="Prior-informed-optimization-of-treatment-recommendation-via-bandit-algorithms-trained-on-large-language-model-processed-historical-records"><a href="#Prior-informed-optimization-of-treatment-recommendation-via-bandit-algorithms-trained-on-large-language-model-processed-historical-records" class="headerlink" title="Prior-informed optimization of treatment recommendation via bandit algorithms trained on large language model-processed historical records"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19014v1">Prior-informed optimization of treatment recommendation via bandit algorithms trained on large language model-processed historical records</a></h3><p><strong>Categories:</strong> Large Language Models, Reinforcement Learning</p>
<p>当前的医疗实践依赖于标准化的治疗框架和实证方法，这些方法忽视了个体患者的差异，导致健康结局不佳。我们开发了一个综合系统，整合了大型语言模型（LLMs）、条件表格生成对抗网络（CTGAN）、T-学习者反事实模型以及上下文多臂老虎机方法，以提供定制化、数据驱动的临床建议。该方法利用LLM将非结构化的医疗叙述转化为结构化数据集（准确率为93.2%），使用CTGAN生成逼真的合成患者数据（通过两样本验证准确率为55%），部署T-学习者来预测患者特定的治疗反应（准确率为84.3%），并整合了基于先验知识的上下文多臂老虎机方法，通过有效平衡探索新可能性与利用现有知识来提升在线治疗选择。在III期结肠癌数据集上的测试显示，我们的KernelUCB方法在5000轮测试中获得了平均奖励分数0.60-0.61，超过了其他参考方法。该综合系统克服了在线学习环境中的冷启动限制，提高了计算效率，并在个性化医疗方面取得了显著进展，能够适应特定患者的特征。</p>
<h3 id="Benchmarking-Large-Language-Models-with-Integer-Sequence-Generation-Tasks"><a href="#Benchmarking-Large-Language-Models-with-Integer-Sequence-Generation-Tasks" class="headerlink" title="Benchmarking Large Language Models with Integer Sequence Generation Tasks"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2411.04372v2">Benchmarking Large Language Models with Integer Sequence Generation Tasks</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>我们提出了一种新的基准测试，旨在严格评估大型语言模型（LLMs）在数学推理和算法代码合成任务中的能力。该基准测试包含来自在线整数序列百科全书（OEIS）的整数序列生成任务，用于测试LLMs在不使用查找表的情况下准确且高效地生成计算这些序列的Python代码的能力。我们的全面评估涵盖了来自OpenAI（包括专门用于推理的o系列模型）、Anthropic、Meta和Google的领先模型，在一组精心挑选的1000个OEIS序列上进行测试，这些序列被分为“简单”和“困难”两类。其中一半是早期OEIS中的经典序列，另一半是最近添加的，以避免与模型训练数据产生污染。为了避免模型利用记忆中的序列值，我们引入了一种自动作弊检测机制，通过检测查找表的使用情况，并通过与人类专家评估进行对比来验证其有效性。实验结果表明，专门用于推理的模型（如OpenAI的o3、o3-mini、o4-mini，以及Google的Gemini 2.5-pro）在准确率上显著优于非推理模型，尤其是在更复杂的任务上。然而，模型在困难序列上的整体表现仍然较差，突显了算法推理方面仍然存在的持续挑战。我们的基准测试为当前最先进的LLMs的能力和局限性提供了重要的见解，特别是强调了进一步提升以可靠地解决复杂数学推理任务的必要性。</p>
<h3 id="Can-Reasoning-Models-Obfuscate-Reasoning-Stress-Testing-Chain-of-Thought-Monitorability"><a href="#Can-Reasoning-Models-Obfuscate-Reasoning-Stress-Testing-Chain-of-Thought-Monitorability" class="headerlink" title="Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought Monitorability"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19851v1">Can Reasoning Models Obfuscate Reasoning? Stress-Testing Chain-of-Thought Monitorability</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>最近的研究表明，对齐不当的模型可能会表现出欺骗性行为，这引发了人们对输出可信度的担忧。链式思维（Chain-of-thought, CoT）是一种有前景的对齐监控工具：当模型忠实地表达其推理过程时，监控系统可以检测并缓解有害行为，从而在不良结果发生之前加以干预。然而，一个关键的不确定性是：模型是否能够通过模糊其链式思维来追求隐藏的对抗性目标，同时逃避检测？为了解答这一问题并从而对链式思维的监控能力进行压力测试，我们开发了一种可组合且可量化的提示分类体系，以引出链式思维的模糊化行为。我们在 SHADE-Arena 中使用玩具任务和更现实的环境对内部链式思维（推理轨迹）和外部链式思维（输出中被提示引发的推理）进行了评估。我们的结果显示：(i) 在没有模糊化压力的情况下，链式思维监控能够准确且高效地运行；(ii) 在强模糊化压力下，一些模型能够成功完成对抗性任务，同时规避检测；(iii) 在提示压力下，模型对内部链式思维的模糊化程度不如对外部链式思维的模糊化程度高。这些结果表明，尽管链式思维在良性环境中提供了有价值的监督作用，但其稳健部署仍需要针对模型特性的监控能力压力测试。</p>
<h3 id="Test-time-Verification-via-Optimal-Transport-Coverage-ROC-Sub-optimality"><a href="#Test-time-Verification-via-Optimal-Transport-Coverage-ROC-Sub-optimality" class="headerlink" title="Test-time Verification via Optimal Transport: Coverage, ROC, &amp; Sub-optimality"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18982v1">Test-time Verification via Optimal Transport: Coverage, ROC, &amp; Sub-optimality</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>尽管在测试时通过验证进行扩展（test-time scaling with verification）在提升大语言模型（LLMs）性能方面展现出一定前景，但验证器的作用及其不完美之处仍缺乏深入研究。验证效果通过三个量的相互作用体现出来：(i) 生成器的覆盖范围，(ii) 验证器的收敛区域（ROC），以及 (iii) 采样算法的次优性。虽然近期研究已经涵盖了这些因素的部分子集，但缺乏一个统一的框架来量化它们之间的几何相互作用。我们将可验证的测试时扩展建模为一个传输问题。这有助于刻画覆盖范围、ROC 和次优性之间的相互作用，并揭示次优性-覆盖曲线表现出三种不同的状态。第一种是传输状态——次优性随覆盖范围增加而上升；第二种是策略改进状态——次优性可能随着覆盖范围的增加而减少，这取决于验证器的ROC；第三种是饱和状态——次优性趋于平稳，不再受覆盖范围影响。我们进一步提出了并分析了两类采样算法——顺序采样和批量采样，并探讨了它们的计算复杂性如何影响这些权衡。使用Qwen、Llama和Gemma模型进行的实验证实了我们的理论发现。</p>
<h3 id="ProfBench-Multi-Domain-Rubrics-requiring-Professional-Knowledge-to-Answer-and-Judge"><a href="#ProfBench-Multi-Domain-Rubrics-requiring-Professional-Knowledge-to-Answer-and-Judge" class="headerlink" title="ProfBench: Multi-Domain Rubrics requiring Professional Knowledge to Answer and Judge"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18941v1">ProfBench: Multi-Domain Rubrics requiring Professional Knowledge to Answer and Judge</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>评估大型语言模型（LLMs）的进步通常受到验证响应的挑战所限制，使得评估仅限于数学、编程和简短问答等任务。然而，许多实际应用场景要求对LLMs在处理专业文档、综合信息以及根据用户查询生成全面报告等方面的能力进行评估。为此，我们引入了ProfBench：一个包含超过7000个由具有物理学博士、化学博士、金融MBA和咨询MBA专业知识的人类专家评估的响应标准对的基准测试集。我们通过减轻自我增强偏差并降低评估成本2-3个数量级，构建了稳健且经济高效的LLM-Judges来评估ProfBench的评分标准，从而使该基准测试更加公平且易于广泛社区使用。我们的研究结果表明，即使对于最先进的LLMs，ProfBench也提出了显著的挑战，例如表现最好的模型GPT-5-high仅达到65.9%的整体性能。此外，我们还发现了专有模型与开源模型之间存在显著的性能差异，并提供了关于扩展性思维在解决复杂、专业领域任务中所起作用的见解。数据集：<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/nvidia/ProfBench">https://huggingface.co/datasets/nvidia/ProfBench</a> 代码：<a target="_blank" rel="noopener" href="https://github.com/NVlabs/ProfBench">https://github.com/NVlabs/ProfBench</a></p>
<h3 id="NeuroAda-Activating-Each-Neuron’s-Potential-for-Parameter-Efficient-Fine-Tuning"><a href="#NeuroAda-Activating-Each-Neuron’s-Potential-for-Parameter-Efficient-Fine-Tuning" class="headerlink" title="NeuroAda: Activating Each Neuron’s Potential for Parameter-Efficient Fine-Tuning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18940v1">NeuroAda: Activating Each Neuron’s Potential for Parameter-Efficient Fine-Tuning</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>现有的参数高效微调（PEFT）方法主要分为两大类：基于添加的方法和选择性原位适应方法。前者，如LoRA，通过引入额外模块来适应下游任务，提供了强大的内存效率。然而，其表示能力通常受到限制，因此在细粒度适应方面表现不佳。相反，后者直接对原始模型参数中精心选择的一小部分进行微调，从而实现更精确和有效的适应，但代价是内存消耗显著增加。为了缓解这一权衡，我们提出了一种新的PEFT方法——NeuroAda，它能够在保持高内存效率的同时实现细粒度模型微调。我们的方法首先识别出重要参数（即网络中的连接），如同选择性适应方法一样，然后为这些选定的参数引入旁路连接。在微调过程中，仅更新旁路连接，而原始模型参数保持冻结不变。在涵盖自然语言生成和理解的23多个任务上的实验证明，NeuroAda仅需至多 <strong>0.02%</strong> 可训练参数即可达到最先进的性能，同时可将CUDA内存使用量减少高达60%。我们在此发布代码：<a target="_blank" rel="noopener" href="https://github.com/FightingFighting/NeuroAda.git%E3%80%82">https://github.com/FightingFighting/NeuroAda.git。</a></p>
<h3 id="How-Do-LLMs-Use-Their-Depth"><a href="#How-Do-LLMs-Use-Their-Depth" class="headerlink" title="How Do LLMs Use Their Depth?"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18871v1">How Do LLMs Use Their Depth?</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>越来越多的证据表明，大型语言模型（LLMs）在使用深度（即模型层数）时并非均匀分布，但目前我们仍缺乏对其逐层预测动态的细致理解。在本文中，我们追踪了多个开源模型在推理过程中的中间表示，并揭示了深度使用的一种结构化且细致的方式。具体而言，我们提出了一种“猜测—精炼”框架，用于解释LLMs如何内部组织计算以进行预测。我们首先表明，在早期LLM层中排名最高的预测主要由高频词组成，这些词由于缺乏适当的上下文信息，早期被模型作为统计性猜测提出。随着上下文信息在模型中逐渐深入，这些初始猜测会被精炼为符合上下文的词。即使早期层中高频词的预测也有超过70%的概率被精炼，这表明正确的词预测并非“一次性完成”。随后，我们超越基于频率的预测，通过三个案例研究探讨了层深度的动态使用情况。首先，词性分析表明，功能词平均而言是最早被正确预测的词。其次，事实回忆任务分析表明，在一个多词答案中，第一个词需要比其余词更多的计算深度。第三，多项选择任务分析表明，模型在前半部分层中即可识别响应格式，但最终确定响应则要等到模型接近末尾。综上所述，我们的研究结果为LLMs中深度使用的提供了详细视角，揭示了成功预测背后逐层计算的机制，并为未来提高基于Transformer模型的计算效率提供了见解。</p>
<h3 id="LightMem-Lightweight-and-Efficient-Memory-Augmented-Generation"><a href="#LightMem-Lightweight-and-Efficient-Memory-Augmented-Generation" class="headerlink" title="LightMem: Lightweight and Efficient Memory-Augmented Generation"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18866v1">LightMem: Lightweight and Efficient Memory-Augmented Generation</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>尽管大型语言模型（LLMs）具有显著的能力，但在动态和复杂的环境中，它们往往难以有效地利用历史交互信息。记忆系统通过引入持久的信息存储、检索和利用机制，使LLMs能够超越无状态交互。然而，现有的记忆系统通常会带来显著的时间和计算开销。为此，我们引入了一种新的记忆系统LightMem，在记忆系统的性能和效率之间取得了平衡。受Atkinson-Shiffrin人类记忆模型的启发，LightMem将记忆组织为三个互补阶段。首先，受认知启发的感官记忆通过轻量级压缩快速过滤无关信息，并根据主题对信息进行分组。其次，主题感知的短期记忆整合这些基于主题的组，对内容进行组织和总结，以实现更结构化的访问。最后，具有睡眠时间更新的长期记忆采用一种离线过程，将巩固过程与在线推理分离。在LongMemEval数据集上使用GPT和Qwen作为基础模型的实验表明，LightMem在准确率上优于强大的基线模型（最高提升10.9%），同时将令牌使用量降低了高达117倍，API调用量降低了高达159倍，运行时间减少了超过12倍。代码可在<a target="_blank" rel="noopener" href="https://github.com/zjunlp/LightMem%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/zjunlp/LightMem获取。</a></p>
<h3 id="Correct-Detect-Balancing-Performance-and-Ambiguity-Through-the-Lens-of-Coreference-Resolution-in-LLMs"><a href="#Correct-Detect-Balancing-Performance-and-Ambiguity-Through-the-Lens-of-Coreference-Resolution-in-LLMs" class="headerlink" title="Correct-Detect: Balancing Performance and Ambiguity Through the Lens of Coreference Resolution in LLMs"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2509.14456v2">Correct-Detect: Balancing Performance and Ambiguity Through the Lens of Coreference Resolution in LLMs</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>大型语言模型（LLMs）旨在反映人类的语言能力。然而，人类可以接触到广泛且具身化的上下文，这对于检测和解决语言歧义至关重要，即使是在孤立的文本片段中也是如此。语义歧义的基本案例体现在指代消解任务中：代词与前面的人称提及之间有何关联？这种能力隐含在几乎所有下游任务中，而在这个层次上存在歧义，会显著影响模型表现。我们表明，LLMs在指代消解以及检测指代歧义方面，仅需少量提示即可取得良好的表现，然而它们无法同时完成这两项任务。我们提出了CORRECT-DETECT的权衡关系：虽然模型具备这两种能力并隐式地加以应用，但成功平衡这两种能力的表现仍然难以实现。</p>
<h3 id="NEXUS-Network-Exploration-for-eXploiting-Unsafe-Sequences-in-Multi-Turn-LLM-Jailbreaks"><a href="#NEXUS-Network-Exploration-for-eXploiting-Unsafe-Sequences-in-Multi-Turn-LLM-Jailbreaks" class="headerlink" title="NEXUS: Network Exploration for eXploiting Unsafe Sequences in Multi-Turn LLM Jailbreaks"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.03417v2">NEXUS: Network Exploration for eXploiting Unsafe Sequences in Multi-Turn LLM Jailbreaks</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>大型语言模型（LLMs）已经彻底改变了自然语言处理，但它们仍然容易受到“越狱”攻击，特别是多轮“越狱”攻击，这种攻击将恶意意图分散在看似无害的对话中，并绕过对齐机制。现有的方法通常对对抗空间探索不足，依赖人工设计的启发式方法，或缺乏系统性的查询优化。我们提出了NEXUS（用于探索危险序列的网络探索框架），这是一个模块化框架，用于构建、优化和执行多轮攻击。NEXUS包括以下三个组成部分：(1) ThoughtNet，它将有害意图分层扩展为一个结构化的语义网络，包含主题、实体和查询链；(2) 一个基于反馈的模拟器，通过攻击者-受害者-裁判LLM之间的协作，利用有害性和语义相似性基准，迭代地优化和修剪这些查询链；(3) 一个网络遍历器，能够自适应地在优化后的查询空间中进行实时攻击。该流程能够发现隐藏且成功率高的对抗路径。在多个闭源和开源LLM上，NEXUS相比现有方法将攻击成功率提高了2.1%至19.4%。代码：<a target="_blank" rel="noopener" href="https://github.com/inspire-lab/NEXUS">https://github.com/inspire-lab/NEXUS</a></p>
<h3 id="Prompt-Decorators-A-Declarative-and-Composable-Syntax-for-Reasoning-Formatting-and-Control-in-LLMs"><a href="#Prompt-Decorators-A-Declarative-and-Composable-Syntax-for-Reasoning-Formatting-and-Control-in-LLMs" class="headerlink" title="Prompt Decorators: A Declarative and Composable Syntax for Reasoning, Formatting, and Control in LLMs"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19850v1">Prompt Decorators: A Declarative and Composable Syntax for Reasoning, Formatting, and Control in LLMs</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>大语言模型（LLMs）在推理、写作和决策支持等流程中起着核心作用，但用户对模型如何进行推理以及如何表达输出缺乏一致的控制。传统的提示工程依赖于冗长的自然语言指令，这限制了其可重复性、模块性和可解释性。本文引入了“提示装饰器”（Prompt Decorators），这是一种声明式的、可组合的语法，通过紧凑的控制标记（如+++Reasoning、+++Tone(style&#x3D;formal)、+++Import(topic&#x3D;”Systems Thinking”)）来调控LLM的行为。每个装饰器都修改一个行为维度，如推理风格、结构或语气，而不会改变任务内容。该框架将二十个核心装饰器组织成两个功能族（认知与生成，以及表达与系统性），每个功能族进一步分解为子类别，分别控制推理、交互、表达和会话控制。它定义了一种统一的语法、作用范围模型和确定性处理流程，从而实现可预测且可审计的行为组合。通过将任务意图与执行行为解耦，提示装饰器为提示设计创建了一个可重用且可解释的接口。示例用例展示了推理透明度的提高、提示复杂度的降低以及跨领域的模型行为标准化。本文最后讨论了对互操作性、行为一致性以及可扩展AI系统声明式接口开发的影响。</p>
<h3 id="How-Transformers-Learn-In-Context-Recall-Tasks-Optimality-Training-Dynamics-and-Generalization"><a href="#How-Transformers-Learn-In-Context-Recall-Tasks-Optimality-Training-Dynamics-and-Generalization" class="headerlink" title="How Transformers Learn In-Context Recall Tasks? Optimality, Training Dynamics and Generalization"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.15009v3">How Transformers Learn In-Context Recall Tasks? Optimality, Training Dynamics and Generalization</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>我们研究了在上下文回忆任务上训练的Transformer模型的近似能力、收敛速度以及收敛过程中的行为。这类任务要求模型从上下文示例中识别出两个标记之间的<strong>位置关联</strong>。目前的理论结果仅关注了Transformer在完成<strong>一次梯度下降更新</strong>后所表现出的上下文推理行为。然而，关于通过梯度下降训练的Transformer在收敛过程中的行为以及收敛速度仍不清楚。此外，关于Transformer在单步上下文推理中的泛化能力尚未进行正式研究。本工作旨在填补这些空白。我们首先证明，一类具有线性、ReLU或softmax注意力机制的Transformer模型，在上下文回忆任务中是可证明的贝叶斯最优的。当使用梯度下降训练时，我们通过有限样本分析表明，期望损失以线性速率收敛至贝叶斯风险。此外，我们还证明了训练后的Transformer模型能够实现<strong>分布外（OOD）泛化</strong>，即能够泛化到超出总体分布的样本。我们的理论发现得到了大量实证验证的支持，结果显示：<strong>在没有适当参数化的情况下</strong>，表达能力更强的模型在经过梯度下降训练后，<strong>出人意料地无法实现分布外泛化</strong>。</p>
<h3 id="Understanding-In-Context-Learning-on-Structured-Manifolds-Bridging-Attention-to-Kernel-Methods"><a href="#Understanding-In-Context-Learning-on-Structured-Manifolds-Bridging-Attention-to-Kernel-Methods" class="headerlink" title="Understanding In-Context Learning on Structured Manifolds: Bridging Attention to Kernel Methods"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2506.10959v2">Understanding In-Context Learning on Structured Manifolds: Bridging Attention to Kernel Methods</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>尽管上下文学习（ICL）在自然语言和视觉领域取得了显著的成功，但其理论理解——尤其是在结构化几何数据的背景下——仍处于未探索阶段。本文首次对在流形上对Hölder函数进行回归的ICL进行了理论研究。我们建立了一种新颖的连接，将注意力机制与经典核方法联系起来，证明了变换器（transformer）通过与提示的交互，在新的查询点上能够有效地执行基于核的预测。这种连接通过数值实验得到了验证，结果表明，对于Hölder函数，所学习的查询-提示得分与高斯核高度相关。基于这一洞察，我们推导出了一般化误差界，该误差界以提示长度和训练任务数量为参数。当观察到足够多的训练任务时，变换器在流形上实现Hölder函数的最小最大回归速率，该速率随着流形的内禀维度呈指数增长，而不是随着环境空间维度增长。我们的结果还描述了一般化误差如何随训练任务数量变化，从而揭示了变换器作为上下文内核算法学习器的复杂性。我们的发现为理解几何在ICL中的作用提供了基础性的见解，并为研究非线性模型的ICL提供了新的工具。</p>
<h3 id="FALCON-Fine-grained-Activation-Manipulation-by-Contrastive-Orthogonal-Unalignment-for-Large-Language-Model"><a href="#FALCON-Fine-grained-Activation-Manipulation-by-Contrastive-Orthogonal-Unalignment-for-Large-Language-Model" class="headerlink" title="FALCON: Fine-grained Activation Manipulation by Contrastive Orthogonal Unalignment for Large Language Model"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2502.01472v3">FALCON: Fine-grained Activation Manipulation by Contrastive Orthogonal Unalignment for Large Language Model</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>大型语言模型已被广泛应用于各种场景，但它们可能会无意中编码敏感或有害的信息，从而引发重大的安全问题。为了解决这一问题，机器“反学习”（machine unlearning）技术应运而生；然而，现有的训练时反学习方法依赖于粗粒度的损失组合，这在精确分离知识和在去除效果与模型实用性之间取得平衡方面存在局限。相比之下，我们提出了一种名为“基于对比正交去对齐的精细激活操控”（Fine-grained Activation manipuLation by Contrastive Orthogonal uNalignment，FALCON）的新颖表示引导型反学习方法。该方法利用信息论指导实现高效的参数选择，通过对比机制增强表示分离，并将冲突梯度投影到正交子空间中，以解决遗忘与保留目标之间的冲突。大量实验表明，FALCON在保持模型实用性的同时实现了卓越的反学习效果，并表现出对知识恢复尝试的强大抵抗力。</p>
<h3 id="Fine-Tuned-Thoughts-Leveraging-Chain-of-Thought-Reasoning-for-Industrial-Asset-Health-Monitoring"><a href="#Fine-Tuned-Thoughts-Leveraging-Chain-of-Thought-Reasoning-for-Industrial-Asset-Health-Monitoring" class="headerlink" title="Fine-Tuned Thoughts: Leveraging Chain-of-Thought Reasoning for Industrial Asset Health Monitoring"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18817v1">Fine-Tuned Thoughts: Leveraging Chain-of-Thought Reasoning for Industrial Asset Health Monitoring</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>小型语言模型（SLMs）因其效率高、计算需求低以及能够针对特定领域任务进行微调，而逐渐在工业应用等专业领域中受到青睐，从而提供准确且经济高效的解决方案。然而，在工业4.0等专业领域中，使用SLMs进行复杂推理仍然面临挑战。在本文中，我们提出了一种用于工业资产健康的知识蒸馏框架，通过链式思维（Chain-of-Thought, CoT）蒸馏的方式，将大型语言模型（LLMs）的推理能力转移到更小、更高效的模型（SLMs）上。我们讨论了使用多选问答（MCQA）提示进行LLMs蒸馏的优势及其过程，以增强推理能力并优化决策过程。我们还通过上下文学习来验证生成知识的质量，并将使用生成知识微调后的SLMs与常用的LLMs进行性能基准测试。实验结果表明，经过CoT推理微调的SLMs在性能上显著优于基础模型，缩小了与LLMs之间的差距。我们的代码已开源，地址为：<a target="_blank" rel="noopener" href="https://github.com/IBM/FailureSensorIQ%E3%80%82">https://github.com/IBM/FailureSensorIQ。</a></p>
<h3 id="Online-SFT-for-LLM-Reasoning-Surprising-Effectiveness-of-Self-Tuning-without-Rewards"><a href="#Online-SFT-for-LLM-Reasoning-Surprising-Effectiveness-of-Self-Tuning-without-Rewards" class="headerlink" title="Online SFT for LLM Reasoning: Surprising Effectiveness of Self-Tuning without Rewards"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18814v1">Online SFT for LLM Reasoning: Surprising Effectiveness of Self-Tuning without Rewards</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>我们提出了一种简单、自助式的在线监督微调（OSFT）范式，用于大语言模型（LLM）的推理。在这一范式中，模型会生成自己的响应，并立即基于这些自生成的数据进行微调。OSFT是一种高效的LLM推理训练策略，因为它无需奖励机制，默认情况下仅使用一次rollout（即一次模拟运行）。实验结果表明，OSFT在具有挑战性的数学推理任务上的下游性能与基于可验证奖励的强化学习（RLVR）方法（如GRPO）相当。我们的消融研究进一步证明了OSFT的高效性和鲁棒性。OSFT的核心机制在于促进模型利用预训练阶段所学到的自身偏好（潜在知识），从而实现推理能力的提升。我们认为，OSFT为更复杂、基于奖励的训练范式提供了一种高效且有前景的替代方案。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/ElementQi/OnlineSFT%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/ElementQi/OnlineSFT获取。</a></p>
<h3 id="Is-Implicit-Knowledge-Enough-for-LLMs-A-RAG-Approach-for-Tree-based-Structures"><a href="#Is-Implicit-Knowledge-Enough-for-LLMs-A-RAG-Approach-for-Tree-based-Structures" class="headerlink" title="Is Implicit Knowledge Enough for LLMs? A RAG Approach for Tree-based Structures"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.10806v2">Is Implicit Knowledge Enough for LLMs? A RAG Approach for Tree-based Structures</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>大型语言模型（LLMs）擅长根据其上下文中的信息生成响应。虽然这种能力对于与结构化数据（如代码文件）进行交互非常有用，但另一种流行的方法——检索增强生成（RAG）——通过检索相关文档来增强模型的上下文学习能力。然而，目前尚不明确如何最佳地表示检索到的知识，以在结构化数据（尤其是树形等层次结构）上生成响应。在本研究中，我们提出了一种新颖的自底向上的方法，通过在每一层生成隐式的聚合摘要，将树状结构（如GitHub仓库）中的知识线性化。这种方法使知识能够存储在知识库中，并直接用于RAG。随后，我们将该方法与直接在原始、非结构化代码上使用RAG进行比较，评估生成响应的准确性和质量。我们的实验结果表明，虽然两种方法在响应质量上相当，但我们的方法在检索器中生成的文档数量减少了68%以上，这在效率上取得了显著提升。这一发现表明，利用隐式线性化知识可能是一种处理复杂层次结构数据的高效且可扩展的策略。</p>
<h3 id="Counterfactual-reasoning-an-analysis-of-in-context-emergence"><a href="#Counterfactual-reasoning-an-analysis-of-in-context-emergence" class="headerlink" title="Counterfactual reasoning: an analysis of in-context emergence"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2506.05188v2">Counterfactual reasoning: an analysis of in-context emergence</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>大规模神经语言模型在上下文学习中表现出色：即能够实时地学习并推理输入上下文。本研究探讨了语言模型中的上下文反事实推理能力，即预测假设情境后果的能力。我们专注于一个定义明确的合成线性回归任务，该任务需要噪声推断。准确预测依赖于两个方面：(1) 推断未观测到的潜在概念，以及 (2) 从实际观测中复制上下文噪声。我们证明了语言模型具备反事实推理的能力。此外，我们增强了现有的可识别性结果，并将一类广泛函数的反事实推理归约为对上下文观测的转换。在Transformer模型中，我们发现自注意力机制、模型深度和预训练数据的多样性驱动了性能。此外，我们提供了机制性证据，表明潜在概念在线性残差流中以线性方式表示，并引入了专门的 \textit{噪声推断头}（noise abduction heads），这些头是执行反事实推理的关键。最后，我们的发现扩展到SDE动力学下的反事实推理，表明Transformer模型可以对序列数据执行噪声推断，为反事实故事生成的潜力提供了初步证据。我们的代码可在 <a target="_blank" rel="noopener" href="https://github.com/mrtzmllr/iccr">https://github.com/mrtzmllr/iccr</a> 上获取。</p>
<h3 id="Can-Large-Language-Models-Adequately-Perform-Symbolic-Reasoning-Over-Time-Series"><a href="#Can-Large-Language-Models-Adequately-Perform-Symbolic-Reasoning-Over-Time-Series" class="headerlink" title="Can Large Language Models Adequately Perform Symbolic Reasoning Over Time Series?"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.03963v3">Can Large Language Models Adequately Perform Symbolic Reasoning Over Time Series?</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>从时间序列数据中揭示隐藏的符号性规律，这一追求可以追溯到开普勒发现行星运动定律的时代，仍然是科学发现和人工智能领域中的核心挑战。尽管大型语言模型（LLMs）在结构化推理任务中展现出潜力，但它们从时间序列数据中推断出可解释且与上下文一致的符号结构的能力仍处于探索阶段。为了系统地评估这种能力，我们引入了SymbolBench，这是一个全面的基准测试工具，用于评估在真实世界时间序列数据上进行符号推理的能力，涵盖三项任务：多变量符号回归、布尔网络推断和因果发现。与以往仅限于简单代数方程的尝试不同，SymbolBench涵盖了多种复杂程度各异的符号形式。我们进一步提出了一种统一框架，将大型语言模型（LLMs）与遗传编程相结合，形成一个闭环的符号推理系统，其中LLMs既充当预测器，也充当评估器。我们的实验结果揭示了当前模型的关键优势与局限性，强调了结合领域知识、上下文对齐以及推理结构对于提升LLMs在自动化科学发现中的性能的重要性。</p>
<h3 id="InternLM2-5-StepProver-Advancing-Automated-Theorem-Proving-via-Critic-Guided-Search"><a href="#InternLM2-5-StepProver-Advancing-Automated-Theorem-Proving-via-Critic-Guided-Search" class="headerlink" title="InternLM2.5-StepProver: Advancing Automated Theorem Proving via Critic-Guided Search"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2410.15700v2">InternLM2.5-StepProver: Advancing Automated Theorem Proving via Critic-Guided Search</a></h3><p><strong>Categories:</strong> Large Language Models, Reinforcement Learning</p>
<p>大型语言模型（LLMs）在数学定理证明中已成为一种强大的工具，尤其是在使用形式语言（如LEAN）时。一种常见的证明方法是LLM证明器通过逐步构建证明策略来迭代地生成证明，通常遵循最佳优先搜索方案。然而，这种方法往往忽略了现有策略轨迹中的关键偏好信息，从而阻碍了对更深层次证明的搜索。我们提出了一种直观而有效的方案，利用一个批评模型来捕捉偏好信息，并在运行时引导证明器模型的搜索过程。基于证明器-批评模型框架，我们进一步应用了超过20,000 CPU天数的大规模专家迭代，以进一步微调证明器和批评模型。训练后的InternLM2.5-StepProver批评模型显著提升了证明器模型的性能（从59.4%提升到65.9%）。我们还分析了批评模型在专家迭代过程中对定理证明各个环节的影响，从而揭示了其有效性。我们开源了模型和搜索到的证明，详见<a target="_blank" rel="noopener" href="https://github.com/InternLM/InternLM-Math%E5%92%8Chttps://huggingface.co/datasets/internlm/Lean-Workbook%E3%80%82">https://github.com/InternLM/InternLM-Math和https://huggingface.co/datasets/internlm/Lean-Workbook。</a></p>
<h3 id="Static-Sandboxes-Are-Inadequate-Modeling-Societal-Complexity-Requires-Open-Ended-Co-Evolution-in-LLM-Based-Multi-Agent-Simulations"><a href="#Static-Sandboxes-Are-Inadequate-Modeling-Societal-Complexity-Requires-Open-Ended-Co-Evolution-in-LLM-Based-Multi-Agent-Simulations" class="headerlink" title="Static Sandboxes Are Inadequate: Modeling Societal Complexity Requires Open-Ended Co-Evolution in LLM-Based Multi-Agent Simulations"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.13982v3">Static Sandboxes Are Inadequate: Modeling Societal Complexity Requires Open-Ended Co-Evolution in LLM-Based Multi-Agent Simulations</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>如果人工代理不仅能够交流，还能进化、适应，并以我们无法完全预测的方式重塑它们所处的世界，那会怎样呢？如今，大型语言模型（LLM）正在驱动多智能体系统和社会模拟，我们正见证着对开放、不断变化环境建模的新可能性。然而，大多数当前的模拟仍局限于静态沙盒环境中，这些环境具有预定义的任务、有限的动态性和僵化的评估标准。这些限制使它们无法捕捉现实社会的复杂性。在本文中，我们认为静态、任务特定的基准测试从根本上是不充分的，必须重新思考。我们批判性地回顾了将LLM与多智能体动态相结合的新兴架构，强调了诸如平衡稳定性与多样性、评估意外行为以及扩展到更高复杂性等关键挑战，并提出了一种针对这一快速发展的领域的新分类体系。最后，我们提出了一项以开放性、持续共进化以及构建具有韧性和社会对齐能力的AI生态系统为核心的研究路线图。我们呼吁社区超越静态范式，共同塑造下一代适应性强、具有社会意识的多智能体模拟。</p>
<h3 id="Verifiable-Accuracy-and-Abstention-Rewards-in-Curriculum-RL-to-Alleviate-Lost-in-Conversation"><a href="#Verifiable-Accuracy-and-Abstention-Rewards-in-Curriculum-RL-to-Alleviate-Lost-in-Conversation" class="headerlink" title="Verifiable Accuracy and Abstention Rewards in Curriculum RL to Alleviate Lost-in-Conversation"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18731v1">Verifiable Accuracy and Abstention Rewards in Curriculum RL to Alleviate Lost-in-Conversation</a></h3><p><strong>Categories:</strong> Large Language Models, Reinforcement Learning</p>
<p>大语言模型在单轮指令遵循方面表现出强大的能力，但在多轮对话设置中，由于信息逐步揭示而出现“对话中迷失”（Lost-in-Conversation，LiC）的问题，导致性能下降。受当前可验证奖励强化学习（Reinforcement Learning with Verifiable Rewards，RLVR）进展的启发，我们提出了可验证准确性和回避奖励的课程强化学习框架（Curriculum Reinforcement Learning with Verifiable Accuracy and Abstention Rewards，RLAAR）。该框架鼓励模型不仅生成正确的答案，还能够在多轮对话环境中判断问题的可解性。我们的方法采用了一种能力门控课程机制，逐步增加对话难度（以指令片段为衡量标准），在稳定训练的同时提升模型的可靠性。通过多轮、基于策略的 rollout（滚动展开）和混合奖励系统，RLAAR 教导模型在解决问题与有根据地回避之间取得平衡，从而减少导致 LiC 的过早回答行为。在 LiC 基准测试中，RLAAR 显著缓解了 LiC 性能下降（从 62.6% 提高到 75.1%），并提高了校准后的回避率（从 33.5% 提高到 73.4%）。这些结果共同提供了一种实用的方法，用于构建可靠且值得信赖的多轮对话大语言模型。</p>
<h3 id="HarmNet-A-Framework-for-Adaptive-Multi-Turn-Jailbreak-Attacks-on-Large-Language-Models"><a href="#HarmNet-A-Framework-for-Adaptive-Multi-Turn-Jailbreak-Attacks-on-Large-Language-Models" class="headerlink" title="HarmNet: A Framework for Adaptive Multi-Turn Jailbreak Attacks on Large Language Models"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18728v1">HarmNet: A Framework for Adaptive Multi-Turn Jailbreak Attacks on Large Language Models</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>大型语言模型（LLMs）仍然容易受到多轮越狱攻击。我们引入了HarmNet，这是一个模块化的框架，包括一个分层语义网络ThoughtNet、一个基于反馈的模拟器用于迭代查询优化，以及一个网络遍历器用于实时自适应攻击执行。HarmNet系统地探索并优化对抗空间，以发现隐蔽且成功率高的攻击路径。在闭源和开源的LLM上进行的实验表明，HarmNet优于最先进的方法，实现了更高的攻击成功率。例如，在Mistral-7B上，HarmNet的攻击成功率达到99.4%，比最佳基线高出13.9%。<br>关键词：越狱攻击；大型语言模型；对抗框架；查询优化。</p>
<h3 id="Language-Models-are-Injective-and-Hence-Invertible"><a href="#Language-Models-are-Injective-and-Hence-Invertible" class="headerlink" title="Language Models are Injective and Hence Invertible"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.15511v3">Language Models are Injective and Hence Invertible</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>像非线性激活函数和归一化这样的Transformer组件本质上是非单射的，这意味着不同的输入可能映射到相同的输出，从而防止从模型表示中精确恢复输入。在本文中，我们挑战这一观点。首先，我们数学上证明了Transformer语言模型将离散输入序列映射到其对应的连续表示序列是单射的，因此是无损的。这一性质在初始化时确立，并在训练过程中得以保持。其次，我们通过在六个最先进的语言模型上进行数十亿次碰撞测试，实证验证了这一结果，并未观察到任何碰撞。第三，我们将单射性概念具体化：我们引入了SipIt，这是首个能够以可证明且高效的方式从隐藏激活中精确重建输入文本的算法，提供了线性时间保证，并在实践中展示了精确可逆性。总体而言，我们的工作确立了单射性作为语言模型的基本且可利用的属性，对透明性、可解释性和安全部署具有直接的启示意义。</p>
<h3 id="Lightweight-Baselines-for-Medical-Abstract-Classification-DistilBERT-with-Cross-Entropy-as-a-Strong-Default"><a href="#Lightweight-Baselines-for-Medical-Abstract-Classification-DistilBERT-with-Cross-Entropy-as-a-Strong-Default" class="headerlink" title="Lightweight Baselines for Medical Abstract Classification: DistilBERT with Cross-Entropy as a Strong Default"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.10025v2">Lightweight Baselines for Medical Abstract Classification: DistilBERT with Cross-Entropy as a Strong Default</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>该研究评估了轻量级医学摘要分类方法，在财务预算限制下评估其最大性能潜力。我们在公开的医学摘要语料库上，使用相同的分词、序列长度、优化器和调度策略，对BERT base和DistilBERT模型进行微调，使用三种目标函数：交叉熵（CE）、类别加权CE和焦点损失（focal loss）。使用普通CE的DistilBERT在原始argmax权衡上表现最佳，但通过后处理操作点选择（验证校准，类别级阈值）显著提升了部署性能；在该调优模式下，焦点损失效果最为显著。我们报告了准确率（Accuracy）、宏F1（Macro F1）和加权F1（Weighted F1）指标，发布了评估相关工具和数据，还包含混淆矩阵分析以澄清错误结构。实际应用的启示是：应首先使用紧凑的编码器和CE，当部署需要更高的宏平衡时，再添加轻量级校准或阈值处理。</p>
<h3 id="Exploring-Membership-Inference-Vulnerabilities-in-Clinical-Large-Language-Models"><a href="#Exploring-Membership-Inference-Vulnerabilities-in-Clinical-Large-Language-Models" class="headerlink" title="Exploring Membership Inference Vulnerabilities in Clinical Large Language Models"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18674v1">Exploring Membership Inference Vulnerabilities in Clinical Large Language Models</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>随着大型语言模型（LLMs）在临床决策支持、文档记录和患者信息系统中日益广泛地嵌入，确保其隐私性和可信度已成为医疗行业的一项紧迫挑战。在敏感的电子健康记录（EHR）数据上对LLMs进行微调，虽然可以提高领域对齐度，但也增加了通过模型行为泄露患者信息的风险。在本项进行中的研究中，我们开展了一项探索性的实证研究，探讨临床LLMs中的成员推理漏洞，重点在于攻击者是否能够推断出特定患者记录是否曾被用于模型训练。我们使用最先进的临床问答模型Llemr，评估了传统的基于损失的攻击方法，以及一种基于领域动机的改写扰动策略，该策略更真实地反映了临床对抗环境中的情况。我们的初步发现表明，存在有限但可衡量的成员信息泄露，这表明当前的临床LLMs虽然具有一定的抗性，但仍可能受到细微隐私风险的影响，从而削弱临床AI应用的信任度。这些结果促使我们继续开发具有上下文感知和领域特定的隐私评估和防御机制，例如差分隐私微调和改写感知训练，以加强医疗AI系统的安全性和可信度。</p>
<h3 id="An-Automated-Multi-modal-Evaluation-Framework-for-Mobile-Intelligent-Assistants-Based-on-Large-Language-Models-and-Multi-Agent-Collaboration"><a href="#An-Automated-Multi-modal-Evaluation-Framework-for-Mobile-Intelligent-Assistants-Based-on-Large-Language-Models-and-Multi-Agent-Collaboration" class="headerlink" title="An Automated Multi-modal Evaluation Framework for Mobile Intelligent Assistants Based on Large Language Models and Multi-Agent Collaboration"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.09507v2">An Automated Multi-modal Evaluation Framework for Mobile Intelligent Assistants Based on Large Language Models and Multi-Agent Collaboration</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>随着移动智能助手技术的快速发展，多模态AI助手已成为日常用户交互的重要接口。然而，当前的评估方法面临诸如人工成本高、标准不一致和主观偏见等挑战。本文提出了一种基于大语言模型和多智能体协作的自动化多模态评估框架。该框架采用三层智能体架构，包括交互评估智能体、语义验证智能体和经验决策智能体。通过对Qwen3-8B模型进行监督微调，我们实现了与人类专家显著匹配的评估准确率。在八个主要智能体上的实验结果表明，该框架在预测用户满意度和识别生成缺陷方面具有有效性。</p>
<h3 id="Reasoning-Language-Model-Inference-Serving-Unveiled-An-Empirical-Study"><a href="#Reasoning-Language-Model-Inference-Serving-Unveiled-An-Empirical-Study" class="headerlink" title="Reasoning Language Model Inference Serving Unveiled: An Empirical Study"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18672v1">Reasoning Language Model Inference Serving Unveiled: An Empirical Study</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>推理型大语言模型（RLLM）在解决数学、编程等复杂推理任务方面已被证明具有竞争力，相较于通用型大语言模型（LLM）。然而，目前对RLLM服务性能和行为的研究仍处于空白，这可能会影响其在现实场景中的部署和应用。为弥补这一不足，本文对RLLM服务进行了全面研究。我们首先通过一个试点研究，比较了RLLM与传统LLM在服务性能上的差异，发现其服务行为存在以下几个显著特点：(1) 显著的内存使用和波动；(2) 拖延请求；(3) 适应性运行时间；(4) 领域偏好。接着，我们进一步探讨现有的推理优化技术是否适用于RLLM。我们的主要结论是，模型量化方法和推测解码可以在对RLLM准确性影响较小的情况下提升服务系统的效率，而前缀缓存、KV缓存量化等技术可能会降低小规模RLLM的准确率或服务性能。最后，我们通过基于伽马分布建模的现实世界工作负载进行评估，以验证我们的发现。不同数据集的现实世界工作负载评估结果与我们关于RLLM服务的主要发现一致。我们希望本研究能够为学术界和产业界提供有价值的见解，以推动RLLM推理服务的发展。</p>
<h3 id="A-Survey-of-Process-Reward-Models-From-Outcome-Signals-to-Process-Supervisions-for-Large-Language-Models"><a href="#A-Survey-of-Process-Reward-Models-From-Outcome-Signals-to-Process-Supervisions-for-Large-Language-Models" class="headerlink" title="A Survey of Process Reward Models: From Outcome Signals to Process Supervisions for Large Language Models"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.08049v2">A Survey of Process Reward Models: From Outcome Signals to Process Supervisions for Large Language Models</a></h3><p><strong>Categories:</strong> Large Language Models, Reinforcement Learning, Robotics</p>
<p>尽管大型语言模型（LLMs）展现出强大的推理能力，但传统的对齐方法仍然主要依赖于仅评估最终答案的输出奖励模型（ORMs）。过程奖励模型（PRMs）通过在步骤或轨迹层面评估和引导推理，来弥补这一不足。本文全面回顾了PRMs的整个流程：如何生成过程数据、构建PRMs以及如何在测试时进行扩展和强化学习。我们总结了PRMs在数学、代码、文本、多模态推理、机器人和智能体等领域的应用，并回顾了新兴的基准测试。我们的目标是明确设计空间，揭示尚未解决的挑战，并引导未来的研究朝着更细粒度、更稳健的推理对齐方向发展。</p>
<h3 id="VITA-Audio-Fast-Interleaved-Cross-Modal-Token-Generation-for-Efficient-Large-Speech-Language-Model"><a href="#VITA-Audio-Fast-Interleaved-Cross-Modal-Token-Generation-for-Efficient-Large-Speech-Language-Model" class="headerlink" title="VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient Large Speech-Language Model"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.03739v2">VITA-Audio: Fast Interleaved Cross-Modal Token Generation for Efficient Large Speech-Language Model</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>随着对自然人机交互需求的不断增长，基于语音的系统因其是日常交流最常见的形式之一而受到越来越多的关注。然而，现有的语音模型在流式处理过程中生成第一个音频标记时仍存在较高的延迟，这成为部署的一个显著瓶颈。为了解决这一问题，我们提出了VITA-Audio，这是一个端到端的大规模语音模型，能够快速生成音频-文本标记。具体来说，我们引入了一个轻量级的多模态标记预测（Multiple Cross-modal Token Prediction, MCTP）模块，该模块能够在单次模型前向传播中高效生成多个音频标记，不仅加速了推理过程，还显著降低了流式场景下第一个音频标记生成的延迟。此外，我们还探索了一种四阶段渐进式训练策略，以在保持语音质量的前提下实现模型加速。据我们所知，VITA-Audio是首个能够在首次前向传播过程中生成音频输出的多模态大语言模型，从而实现了低延迟的实时对话能力。VITA-Audio是完全可复现的，并且仅基于开源数据进行训练。实验结果表明，我们的模型在70亿参数规模下实现了3~5倍的推理加速，同时在多个自动语音识别（ASR）、文本到语音（TTS）和语音问答（SQA）任务的基准测试中，显著优于类似规模的开源模型。</p>
<h3 id="R-Horizon-How-Far-Can-Your-Large-Reasoning-Model-Really-Go-in-Breadth-and-Depth"><a href="#R-Horizon-How-Far-Can-Your-Large-Reasoning-Model-Really-Go-in-Breadth-and-Depth" class="headerlink" title="R-Horizon: How Far Can Your Large Reasoning Model Really Go in Breadth and Depth?"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.08189v2">R-Horizon: How Far Can Your Large Reasoning Model Really Go in Breadth and Depth?</a></h3><p><strong>Categories:</strong> Large Language Models, Reinforcement Learning</p>
<p>近期，推理模型（如 OpenAI o1、DeepSeek-R1）在测试时扩展（test-time scaling）方面的趋势通过长链式思维（Chain-of-Thought, CoT）实现了显著的性能提升。然而，现有的基准测试主要关注即时的、单一时间范围的任务，未能充分评估模型在理解和应对复杂、长时间范围场景中的能力。为了解决对大型推理模型（Large Reasoning Models, LRMs）评估不全面的问题，我们提出了 R-HORIZON，这是一种通过查询组合来激发 LRMs 长时间范围推理行为的方法。基于 R-HORIZON，我们构建了一个长时推理基准测试，包含涉及长推理时间范围的复杂多步骤推理任务，其中各子问题相互依赖。通过使用 R-HORIZON 基准对 LRMs 进行全面评估，我们发现即使是最先进的 LRMs 也存在显著的性能下降。我们的分析表明，LRMs 的有效推理长度有限，且难以在多个问题之间合理分配思维预算。针对这些局限性，我们利用 R-HORIZON 构建了用于强化学习的长时推理数据集，并结合了经过验证的奖励机制（RLVR）。与仅使用单一时间范围数据进行训练相比，基于 R-HORIZON 的 RLVR 不仅显著提升了多时间范围推理任务的性能，还提高了标准推理任务的准确性，AIME2024 任务的准确率提高了 7.5。这些结果表明，R-HORIZON 是一种可扩展、可控且低成本的范式，可用于增强和评估 LRMs 的长时推理能力。</p>
<h3 id="A-Justice-Lens-on-Fairness-and-Ethics-Courses-in-Computing-Education-LLM-Assisted-Multi-Perspective-and-Thematic-Evaluation"><a href="#A-Justice-Lens-on-Fairness-and-Ethics-Courses-in-Computing-Education-LLM-Assisted-Multi-Perspective-and-Thematic-Evaluation" class="headerlink" title="A Justice Lens on Fairness and Ethics Courses in Computing Education: LLM-Assisted Multi-Perspective and Thematic Evaluation"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18931v1">A Justice Lens on Fairness and Ethics Courses in Computing Education: LLM-Assisted Multi-Perspective and Thematic Evaluation</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>课程大纲为课程设定了基调和期望，塑造了学生和教师的学习体验。在计算课程中，特别是涉及人工智能（AI）、机器学习（ML）和算法设计中的公平性和伦理问题的课程中，我们必须理解如何应对实现公平结果的障碍。这些期望应具有包容性、透明性，并以促进批判性思维为基础。课程大纲分析为我们提供了一种评估课程内容覆盖范围、深度、实践方式和期望的途径。然而，手动进行大纲评估耗时且容易出现不一致。为了解决这一问题，我们开发了一个以正义为导向的评分量表，并请一个大型语言模型（LLM）通过多视角角色模拟来审查课程大纲。使用该量表，我们从四个视角评估了24个课程大纲：教师、系主任、机构审查者和外部评估者。此外，我们还引导LLM识别课程中的主题趋势。研究结果表明，多视角评估有助于我们注意到具有角色特性的细致优先事项，并利用这些信息填补人工智能&#x2F;机器学习及相关计算课程中关于公平性和伦理内容的课程设计中的隐性空白。这些见解为改进此类课程中公平性、伦理和正义内容的设计和实施提供了具体的方向。</p>
<h3 id="EvaLearn-Quantifying-the-Learning-Capability-and-Efficiency-of-LLMs-via-Sequential-Problem-Solving"><a href="#EvaLearn-Quantifying-the-Learning-Capability-and-Efficiency-of-LLMs-via-Sequential-Problem-Solving" class="headerlink" title="EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via Sequential Problem Solving"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2506.02672v3">EvaLearn: Quantifying the Learning Capability and Efficiency of LLMs via Sequential Problem Solving</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>我们介绍了EvaLearn，这是一个开创性的基准测试，用于评估大型语言模型（LLMs）在具有挑战性任务中的学习能力和效率，这是模型潜力中一个关键但尚未充分探索的方面。EvaLearn包含六种任务类型共648个具有挑战性的问题，分为182个序列，每个序列专注于一种任务类型。与大多数现有的并行评估基准不同，EvaLearn要求模型按顺序解决这些问题，使模型能够利用先前解决方案所获得的经验。EvaLearn提供了五个全面的自动化评估指标，用于评估模型并量化其学习能力和效率。我们对九个前沿模型进行了广泛基准测试，观察到不同的性能表现：一些模型（如Claude-3.7-sonnet）初始性能中等，但表现出较强的学习能力；而另一些模型则难以从经验中获益，甚至可能表现出负迁移现象。此外，我们研究了模型在两种学习设置下的表现，发现实例级评分标准和教师模型反馈进一步促进了模型的学习。值得注意的是，我们观察到当前具有更强静态能力的LLMs在所有任务中的学习能力并未表现出明显优势，这表明EvaLearn评估的是模型性能的一个新维度。我们希望EvaLearn能为评估LLM的潜力和理解模型与人类能力之间的差距提供新的视角，推动更深入、更具动态性的评估方法的发展。本文中所涉及的所有数据集、自动评估框架以及研究结果均可在GitHub仓库中获取。</p>
<h3 id="Can-LLMs-Reconcile-Knowledge-Conflicts-in-Counterfactual-Reasoning"><a href="#Can-LLMs-Reconcile-Knowledge-Conflicts-in-Counterfactual-Reasoning" class="headerlink" title="Can LLMs Reconcile Knowledge Conflicts in Counterfactual Reasoning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2506.15732v3">Can LLMs Reconcile Knowledge Conflicts in Counterfactual Reasoning</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>大型语言模型（LLMs）已被证明在其参数中包含大量世界知识，使其在许多需要广泛知识的任务中表现出色。然而，当部署到新环境中时，LLMs常常会遇到需要将参数知识与新信息或不熟悉信息进行整合的情况。在本工作中，我们通过反事实推理的视角，探讨LLMs是否能够结合上下文中的知识与参数知识。通过在多跳推理问题上的合成和真实实验，我们发现LLMs在反事实推理方面普遍表现不佳，往往只能依赖其参数知识。此外，我们还表明，简单的后处理微调往往难以培养反事实推理能力，甚至可能导致存储的参数知识退化。最终，我们的研究揭示了当前LLMs在新环境中重新利用参数知识方面存在的重要局限性。</p>
<h3 id="Patent-Language-Model-Pretraining-with-ModernBERT"><a href="#Patent-Language-Model-Pretraining-with-ModernBERT" class="headerlink" title="Patent Language Model Pretraining with ModernBERT"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2509.14926v2">Patent Language Model Pretraining with ModernBERT</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>基于Transformer的语言模型，如BERT，已成为自然语言处理（NLP）的基础，但在专利等专业领域中表现有所下降，因为专利文本通常具有长篇幅、技术性以及法律结构的特点。以往的专利NLP方法主要依赖于对通用模型或使用有限数据进行预训练的领域适配变体进行微调。在本研究中，我们使用ModernBERT架构和一个经过精心整理的包含超过6000万条专利记录的语料库，预训练了三种专门针对专利的掩码语言模型。我们的方法结合了架构优化，包括FlashAttention、旋转嵌入（rotary embeddings）和GLU前馈层。我们在四个下游专利分类任务上评估了这些模型。我们的模型ModernBERT-base-PT在四个数据集中有三个表现优于通用的ModernBERT基线模型，并且与基线PatentBERT模型表现相当。此外，使用ModernBERT-base-VX和Mosaic-BERT-large进行的额外实验表明，增加模型规模和自定义分词器可以进一步提升特定任务的性能。值得注意的是，所有ModernBERT变体的推理速度均显著快于PatentBERT，快了3倍以上，这凸显了它们在时效性要求高的应用场景中的适用性。这些结果突显了针对特定领域进行预训练以及架构改进对于专利相关NLP任务的重要性。</p>
<h3 id="LAMP-PRo-Label-aware-Attention-for-Multi-label-Prediction-of-DNA-and-RNA-binding-Proteins-using-Protein-Language-Models"><a href="#LAMP-PRo-Label-aware-Attention-for-Multi-label-Prediction-of-DNA-and-RNA-binding-Proteins-using-Protein-Language-Models" class="headerlink" title="LAMP-PRo: Label-aware Attention for Multi-label Prediction of DNA- and RNA-binding Proteins using Protein Language Models"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2509.24262v2">LAMP-PRo: Label-aware Attention for Multi-label Prediction of DNA- and RNA-binding Proteins using Protein Language Models</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>识别DNA结合蛋白（DBPs）和RNA结合蛋白（RBPs）对于理解细胞功能、分子相互作用以及调控功能至关重要。由于DBPs和RBPs之间高度相似，现有的大多数方法在区分它们时面临挑战，导致较高的交叉预测错误。此外，识别同时结合DNA和RNA的蛋白质（DRBPs）也是一个相当具有挑战性的任务。针对这一问题，我们提出了一种新颖的框架LAMP-PRo，该框架基于预训练的蛋白质语言模型（PLM）、注意力机制和多标签学习，以缓解上述问题。首先，使用预训练的PLM（如ESM-2）对蛋白质序列进行嵌入，然后通过卷积神经网络（CNN）进行处理。随后，应用多头自注意力机制来获取上下文信息，同时使用标签感知注意力机制，通过针对每个标签（DBP、RBP和非NABP）量身定制的方式，从序列中计算类别特异性表示。我们还引入了一种新颖的跨标签注意力机制，以显式捕捉DNA结合蛋白和RNA结合蛋白之间的依赖关系，从而更准确地预测DRBP。最后，通过一个线性层和一个Sigmoid函数进行最终预测。我们进行了大量实验，将LAMP-PRo与现有方法进行比较，结果显示所提出的模型具有稳定且优异的性能。此外，我们还提供了可视化结果，以展示模型的可解释性，突出显示对预测标签最相关的序列部分。原始数据集可在<a target="_blank" rel="noopener" href="http://bliulab.net/iDRBP_MMC%E8%8E%B7%E5%8F%96%EF%BC%8C%E4%BB%A3%E7%A0%81%E5%8F%AF%E5%9C%A8https://github.com/NimishaGhosh/LAMP-PRo%E8%8E%B7%E5%8F%96%E3%80%82">http://bliulab.net/iDRBP_MMC获取，代码可在https://github.com/NimishaGhosh/LAMP-PRo获取。</a></p>
<h3 id="SimBench-Benchmarking-the-Ability-of-Large-Language-Models-to-Simulate-Human-Behaviors"><a href="#SimBench-Benchmarking-the-Ability-of-Large-Language-Models-to-Simulate-Human-Behaviors" class="headerlink" title="SimBench: Benchmarking the Ability of Large Language Models to Simulate Human Behaviors"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.17516v2">SimBench: Benchmarking the Ability of Large Language Models to Simulate Human Behaviors</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>大型语言模型（LLM）对人类行为的模拟具有革命性潜力，前提是它们能够真实反映人类实际行为。然而目前的评估方法零散，基于定制任务和指标，导致结果缺乏可比性。为了解决这一问题，我们提出了SimBench，这是首个大规模、标准化的基准测试，旨在推动LLM模拟的稳健、可重复的科学研究。通过统一涵盖道德决策、经济选择等任务的20个多样化数据集，SimBench为探讨LLM模拟何时、如何以及为何成功或失败等基本问题提供了必要的基础。我们表明，尽管目前最佳的LLM在模拟能力上仍然有限（得分：40.80&#x2F;100），但模型性能随着模型规模的增加呈对数线性增长。模拟性能不会因推理时间计算量的增加而提升。我们展示了模拟能力与对齐之间的权衡：指令调优可以提升低熵（共识性）问题的性能，但会降低高熵（多样性）问题的性能。当模拟特定人口群体时，模型尤其难以应对。最后，我们证明模拟能力与深度、知识密集型推理（MMLU-Pro，相关系数r&#x3D;0.939）之间的关联最为紧密。通过使进展可衡量，我们旨在加速开发更加忠实的LLM模拟器。</p>
<h3 id="SentinelNet-Safeguarding-Multi-Agent-Collaboration-Through-Credit-Based-Dynamic-Threat-Detection"><a href="#SentinelNet-Safeguarding-Multi-Agent-Collaboration-Through-Credit-Based-Dynamic-Threat-Detection" class="headerlink" title="SentinelNet: Safeguarding Multi-Agent Collaboration Through Credit-Based Dynamic Threat Detection"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.16219v2">SentinelNet: Safeguarding Multi-Agent Collaboration Through Credit-Based Dynamic Threat Detection</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>恶意代理对由大型语言模型（LLMs）驱动的多智能体系统（MAS）的可靠性和决策能力构成重大威胁。现有防御机制由于反应式设计或集中式架构可能引入单点故障，往往效果不佳。为了解决这些问题，我们提出了SentinelNet，这是首个用于主动检测和缓解多智能体协作中恶意行为的去中心化框架。SentinelNet为每个智能体配备了基于信用的检测器，该检测器通过对比学习在增强的对抗性辩论轨迹上进行训练，从而实现消息可信度的自主评估，并通过底部k淘汰机制动态地对邻居进行排序，以抑制恶意通信。为克服攻击数据稀缺的问题，SentinelNet生成模拟多种威胁的对抗性轨迹，确保训练的鲁棒性。在MAS基准测试中的实验表明，SentinelNet能够在两轮辩论内几乎完全检测到恶意智能体（接近100%），并从被攻击的基线中恢复95%的系统精度。通过在不同领域和攻击模式中展现出强大的泛化能力，SentinelNet建立了一个保护协作型MAS的新范式。</p>
<h3 id="AI-Debaters-are-More-Persuasive-when-Arguing-in-Alignment-with-Their-Own-Beliefs"><a href="#AI-Debaters-are-More-Persuasive-when-Arguing-in-Alignment-with-Their-Own-Beliefs" class="headerlink" title="AI Debaters are More Persuasive when Arguing in Alignment with Their Own Beliefs"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.13912v2">AI Debaters are More Persuasive when Arguing in Alignment with Their Own Beliefs</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>作为可扩展监督技术，AI辩论的核心假设是：令人信服地撒谎比反驳一个谎言更为困难，从而使裁判能够识别出正确的立场。然而，现有的辩论实验依赖于带有已知正确答案的数据集，其中撒谎被简化为捍卫一个错误的命题。这种做法忽略了主观维度：撒谎还需要相信所捍卫的主张是错误的。在本研究中，我们将辩论应用于主观性问题，并在实验前明确测量大语言模型的先验信念。辩论者被要求选择其偏好的立场，然后面对一个刻意与其先验信念相冲突的裁判角色。这种设置旨在测试模型是否会采取阿谀奉承的策略，迎合裁判预设的立场以提高说服力，还是坚持其先验信念。我们实施并比较了两种辩论协议：顺序辩论和同时辩论，以评估潜在的系统性偏差。最后，我们评估了模型在捍卫与其先验信念一致的立场时，是否比在反对这些立场时更具说服力，并产生更高质量的论点。我们的主要发现表明：模型倾向于捍卫与裁判角色一致的立场，而非其先验信念；顺序辩论引入了显著的偏差，倾向于支持第二个辩论者；模型在捍卫与其先验信念一致的立场时更具说服力；而令人惊讶的是，在成对比较中，与先验信念不一致的论点却被评为质量更高。这些结果有助于人类裁判提供更高质量的训练信号，有助于构建更对齐的AI系统，同时揭示了关于语言模型中说服动态的重要人类-人工智能互动方面。</p>
<h3 id="Large-language-models-for-folktale-type-automation-based-on-motifs-Cinderella-case-study"><a href="#Large-language-models-for-folktale-type-automation-based-on-motifs-Cinderella-case-study" class="headerlink" title="Large language models for folktale type automation based on motifs: Cinderella case study"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18561v1">Large language models for folktale type automation based on motifs: Cinderella case study</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>人工智能方法正被应用于许多研究领域，包括数字人文。我们为民间故事研究构建了一种大规模分析的方法。通过机器学习和自然语言处理技术，我们自动检测了大量灰姑娘变体故事中的主题，并利用聚类和降维技术分析其相似性与差异性。研究结果表明，大语言模型能够检测故事中复杂的互动关系，从而实现对大量文本集合的计算分析，并促进跨语言的比较研究。</p>
<h3 id="WebDevJudge-Evaluating-M-LLMs-as-Critiques-for-Web-Development-Quality"><a href="#WebDevJudge-Evaluating-M-LLMs-as-Critiques-for-Web-Development-Quality" class="headerlink" title="WebDevJudge: Evaluating (M)LLMs as Critiques for Web Development Quality"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18560v1">WebDevJudge: Evaluating (M)LLMs as Critiques for Web Development Quality</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>“LLM作为法官”的范式正逐渐成为一种可扩展且高效的替代方案，用以替代人工评估，其在定义明确的任务中表现出色。然而，在动态环境和复杂交互的开放性任务中，其可靠性仍尚未被探索。为弥合这一差距，我们引入了WebDevJudge，这是一个用于评估“LLM作为法官”在网页开发中表现的系统性基准测试，支持基于静态观察的非交互式评估和基于动态网页环境的连续交互式评估。WebDevJudge包含针对成对网页实现的人类偏好标签，并通过结构化和查询基础的评分标准进行标注，以确保高质量的基准事实。利用这一基准，我们全面评估了多种评估者，包括LLMs、MLLMs和代理工作流程。我们系统地研究了不同范式和指导机制的影响。我们的实验揭示了LLM法官与人类专家之间存在显著差距。深入分析表明，这一差距源于模型的基本限制，包括无法识别功能等价性、验证任务可行性以及缓解偏见等问题。总体而言，WebDevJudge对“LLM作为法官”提出了重大挑战，为未来研究提供了指导，以开发更加可靠和强大的自动化评估工具，用于复杂场景。代码和数据可在<a target="_blank" rel="noopener" href="https://github.com/lcy2723/WebDevJudge%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/lcy2723/WebDevJudge获取。</a></p>
<h3 id="Extracting-alignment-data-in-open-models"><a href="#Extracting-alignment-data-in-open-models" class="headerlink" title="Extracting alignment data in open models"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18554v1">Extracting alignment data in open models</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>在本项工作中，我们展示了从微调后的模型中提取大量对齐训练数据的可能性——这对于引导模型提升某些能力（如长上下文推理、安全性、指令遵循和数学能力）非常有用。尽管大多数关于记忆（memorization）的相关研究都集中在通过字符串匹配来衡量训练数据提取的成功程度，但我们认为嵌入模型更适合我们特定的目标。通过高质量的嵌入模型计算的距离可以识别字符串之间的语义相似性，而像编辑距离这样的其他度量方法则难以捕捉到这些相似性。事实上，在我们的调查中，近似字符串匹配方法会因为一些微不足道的干扰因素而严重低估（保守估计低估了10倍）可提取的数据量。有趣的是，我们发现模型会轻易地重复输出在微调阶段使用过的训练数据，例如监督微调（SFT）或强化学习（RL）阶段使用的数据。我们展示了这些数据可以用于训练基础模型，从而恢复相当一部分原始性能。我们认为，我们的工作揭示了在提取对齐数据过程中可能被忽视的风险。最后，我们的研究引发了对蒸馏实践下游影响的有趣讨论：由于模型似乎会重复输出其训练集中的内容，因此可以认为蒸馏过程实际上是在间接地基于模型的原始数据集进行训练。</p>
<h3 id="SOCIA-Nabla-Textual-Gradient-Meets-Multi-Agent-Orchestration-for-Automated-Simulator-Generation"><a href="#SOCIA-Nabla-Textual-Gradient-Meets-Multi-Agent-Orchestration-for-Automated-Simulator-Generation" class="headerlink" title="SOCIA-Nabla: Textual Gradient Meets Multi-Agent Orchestration for Automated Simulator Generation"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18551v1">SOCIA-Nabla: Textual Gradient Meets Multi-Agent Orchestration for Automated Simulator Generation</a></h3><p><strong>Categories:</strong> Large Language Models, Reinforcement Learning</p>
<p>在本文中，我们提出了SOCIA-Nabla，一个端到端的智能体框架，它将模拟器构建视为在文本计算图中对代码进行实例优化。专门的LLM驱动智能体被嵌入为图节点，工作流管理器执行一个以损失驱动的循环：代码合成→执行→评估→代码修复。优化器执行文本梯度下降（TGD），而人机交互则保留用于任务特定确认，从而最小化专家的工作量，并将代码本身作为可训练对象。在三个CPS任务——用户建模、掩码采用和个性化移动中，SOCIA-Nabla实现了最先进的整体精度。通过将多智能体协调统一到一个与损失对齐的优化视角下，SOCIA-Nabla将脆弱的提示管道转化为可重复、具有约束意识的模拟器代码生成，并且可以跨领域和模拟粒度进行扩展。本工作正在审阅中，我们即将发布代码。</p>
<h3 id="EfficientNav-Towards-On-Device-Object-Goal-Navigation-with-Navigation-Map-Caching-and-Retrieval"><a href="#EfficientNav-Towards-On-Device-Object-Goal-Navigation-with-Navigation-Map-Caching-and-Retrieval" class="headerlink" title="EfficientNav: Towards On-Device Object-Goal Navigation with Navigation Map Caching and Retrieval"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18546v1">EfficientNav: Towards On-Device Object-Goal Navigation with Navigation Map Caching and Retrieval</a></h3><p><strong>Categories:</strong> Large Language Models, Robotics</p>
<p>目标-目标导航（ObjNav）任务要求智能体在一个未见过的环境中导航至特定物体的位置。配备大型语言模型（LLMs）和在线构建的导航地图的具身智能体可以以零样本方式执行ObjNav。然而，现有的智能体严重依赖云端的大型LLM（如GPT-4），而直接切换到小型LLM（如LLaMA3.2-11b）则由于模型容量有限，难以理解复杂的导航地图，从而导致成功率显著下降，这阻碍了在本地设备上部署ObjNav。同时，导航地图描述引入的长提示语会导致本地设备上的高规划延迟。在本文中，我们提出EfficientNav，以实现本地设备上基于LLM的高效零样本ObjNav。为了帮助较小的LLM更好地理解环境，我们提出语义感知的记忆检索机制，以剔除导航地图中的冗余信息。为了减少规划延迟，我们提出离散记忆缓存和基于注意力的记忆聚类，以高效地保存和重用KV缓存。大量实验结果表明，EfficientNav在HM3D基准测试中比基于GPT-4的基线方法成功率提高了11.1%，并且相比GPT-4规划器实现了6.7倍的实时延迟降低和4.7倍的端到端延迟降低。我们的代码将很快发布。</p>
<h3 id="When-Text-Embedding-Meets-Large-Language-Model-A-Comprehensive-Survey"><a href="#When-Text-Embedding-Meets-Large-Language-Model-A-Comprehensive-Survey" class="headerlink" title="When Text Embedding Meets Large Language Model: A Comprehensive Survey"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2412.09165v4">When Text Embedding Meets Large Language Model: A Comprehensive Survey</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>文本嵌入已成为深度学习时代自然语言处理（NLP）的基础技术，推动了众多下游任务的发展。虽然许多自然语言理解任务现在可以利用生成范式进行建模，并借助大型语言模型（LLMs）强大的生成和理解能力，但诸如语义匹配、聚类和信息检索等大量实际应用仍然依赖文本嵌入以实现高效性和有效性。因此，近年来将LLMs与文本嵌入相结合已成为一个重要的研究方向。在本次综述中，我们将LLMs与文本嵌入之间的相互作用归纳为三个主要主题：（1）LLM增强的文本嵌入，即利用LLMs增强传统的嵌入方法；（2）将LLMs作为文本嵌入器，利用其固有能力生成高质量嵌入；（3）利用LLMs理解文本嵌入，借助LLMs对嵌入进行分析与解释。通过按照交互模式而非特定的下游应用来组织近期的研究成果，我们提供了一个新颖且系统的概述，涵盖LLMs时代各类研究和应用领域的贡献。此外，我们还突出了预LLMs时代中基于预训练语言模型（PLMs）所遗留的未解决挑战，并探讨了LLMs所带来的新兴障碍。在此分析的基础上，我们概述了文本嵌入演进的未来方向，应对NLP快速发展的理论与实践机会。</p>
<h3 id="Pay-Attention-to-the-Triggers-Constructing-Backdoors-That-Survive-Distillation"><a href="#Pay-Attention-to-the-Triggers-Constructing-Backdoors-That-Survive-Distillation" class="headerlink" title="Pay Attention to the Triggers: Constructing Backdoors That Survive Distillation"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18541v1">Pay Attention to the Triggers: Constructing Backdoors That Survive Distillation</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>大语言模型（LLMs）通常被下游用户用作教师模型进行知识蒸馏，从而将其能力压缩到更节省内存的模型中。然而，由于这些教师模型可能来源于不可信的实体，蒸馏过程可能会带来意想不到的安全风险。本文研究了从植入后门的教师模型中进行知识蒸馏所带来的安全影响。首先，我们表明，现有的后门大多无法转移到学生模型中。我们的关键见解是，现有LLM后门植入方法选择的触发词在常规语境中很少出现。我们认为，这种做法低估了知识蒸馏的安全风险，并提出了一种新的后门植入技术T-MTB，该技术能够构建和研究可转移的后门。T-MTB精心构造了一个复合后门触发器，由若干个在预期蒸馏数据集中单独经常出现的特定词组成。因此，被污染的教师模型仍然隐蔽，但在蒸馏过程中，这些词的单独出现为后门向学生模型的转移提供了足够的信号。利用T-MTB，我们展示了并广泛研究了在两种攻击场景（越狱攻击和内容调制）以及四种LLM模型家族中可转移后门所带来的安全风险。</p>
<h3 id="Can-we-Evaluate-RAGs-with-Synthetic-Data"><a href="#Can-we-Evaluate-RAGs-with-Synthetic-Data" class="headerlink" title="Can we Evaluate RAGs with Synthetic Data?"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.11758v2">Can we Evaluate RAGs with Synthetic Data?</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>我们研究由大语言模型（LLMs）生成的合成问答（QA）数据是否可以在无法获得人工标注基准的情况下，作为有效的替代品。我们通过两个实验评估合成基准的可靠性：第一个实验在保持生成器不变的情况下，改变检索器参数；第二个实验则在保持检索器参数不变的情况下，改变生成器。在四个数据集（其中两个是开放领域，另外两个是专有领域）上，我们发现合成基准能够可靠地对检索器配置不同的RAG系统进行排序，其结果与人工标注基准基线高度一致。然而，在比较生成器架构时，合成基准并不能始终可靠地产生稳定的RAG排序。这种不一致可能源于合成基准与人工基准之间的任务不匹配，以及某种生成器在风格上的偏倚。</p>
<h3 id="Counterfactual-Reasoning-for-Steerable-Pluralistic-Value-Alignment-of-Large-Language-Models"><a href="#Counterfactual-Reasoning-for-Steerable-Pluralistic-Value-Alignment-of-Large-Language-Models" class="headerlink" title="Counterfactual Reasoning for Steerable Pluralistic Value Alignment of Large Language Models"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18526v1">Counterfactual Reasoning for Steerable Pluralistic Value Alignment of Large Language Models</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>随着大型语言模型（LLMs）日益融入服务于跨文化、跨社区和跨人群用户的应用场景，将LLMs与超越平均原则（如HHH）的多元人类价值观对齐变得至关重要。在心理和社会价值理论（如Schwartz的价值理论）中，多元价值观由多个价值维度及其不同优先级组成。然而，现有方法在与这些细粒度价值目标对齐时面临两大挑战：1）它们通常将多个价值观视为独立且同等重要的，忽略了它们之间的相互依赖关系和相对优先级（价值复杂性）；2）它们难以精确控制细微的价值优先级，特别是那些代表性不足的价值（价值可引导性）。为应对这些挑战，我们提出了COUPLE，一种用于多元价值观对齐的反事实推理框架。它引入了结构因果模型（SCM），以体现特征之间的复杂依赖关系和优先级，以及高层次价值维度与行为之间的因果关系。此外，它通过反事实推理生成与任何期望价值目标对齐的输出。得益于显式的因果建模，COUPLE还提供了更好的可解释性。我们在两个具有不同价值体系的数据集上评估了COUPLE，并证明其在多种类型的价值目标上优于其他基线方法。</p>
<h3 id="From-Unaligned-to-Aligned-Scaling-Multilingual-LLMs-with-Multi-Way-Parallel-Corpora"><a href="#From-Unaligned-to-Aligned-Scaling-Multilingual-LLMs-with-Multi-Way-Parallel-Corpora" class="headerlink" title="From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way Parallel Corpora"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.14045v4">From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way Parallel Corpora</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>在大规模多语言数据上进行持续预训练和指令调优已被证明是扩展大规模语言模型（LLMs）至低资源语言的有效方法。然而，此类数据的不对齐特性限制了其有效捕捉跨语言语义的能力。相比之下，多语言并行数据（即相同内容在多种语言中对齐）提供了更强的跨语言一致性，并且在提升多语言性能方面具有更大的潜力。本文我们引入了一个大规模、高质量的多语言并行语料库TED2025，该语料库基于TED演讲。该语料库涵盖113种语言，最多有50种语言实现并行对齐，确保了广泛的多语言覆盖。利用该数据集，我们探讨了如何利用多语言并行数据增强LLMs的最佳实践，包括持续预训练策略、指令调优方法以及关键影响因素的分析。在六个多语言基准测试上的实验表明，使用多语言并行数据训练的模型在性能上始终优于使用非对齐多语言数据训练的模型。</p>
<h3 id="The-Narcissus-Hypothesis-Descending-to-the-Rung-of-Illusion"><a href="#The-Narcissus-Hypothesis-Descending-to-the-Rung-of-Illusion" class="headerlink" title="The Narcissus Hypothesis: Descending to the Rung of Illusion"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2509.17999v4">The Narcissus Hypothesis: Descending to the Rung of Illusion</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>现代基础模型越来越多地不仅反映世界知识，还反映了其训练数据中嵌入的人类偏好模式。我们假设，通过人类反馈和模型生成语料的递归对齐，会引发一种社会可接受性偏差，促使模型更倾向于选择讨好或奉承的回应，而非客观推理。我们将这种现象称为“纳西索斯假说”（Narcissus Hypothesis），并通过标准化的人格评估和一个新颖的社会可接受性偏差评分，对31个模型进行了测试。研究结果揭示了向社会合群特征显著偏移的趋势，这对语料库的完整性以及后续推理的可靠性具有深远影响。随后，我们提供了一种新颖的认识论解释，分析递归偏差如何可能将高阶推理降级至佩尔（Pearl）的因果阶梯（Ladder of Causality），最终导致我们所称的“幻觉阶梯”（Rung of Illusion）。</p>
<h3 id="One-Size-Fits-All-A-Modular-Adaptive-Sanitization-Kit-MASK-for-Customizable-Privacy-Preserving-Phone-Scam-Detection"><a href="#One-Size-Fits-All-A-Modular-Adaptive-Sanitization-Kit-MASK-for-Customizable-Privacy-Preserving-Phone-Scam-Detection" class="headerlink" title="One Size Fits All? A Modular Adaptive Sanitization Kit (MASK) for Customizable Privacy-Preserving Phone Scam Detection"></a><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.18493v1">One Size Fits All? A Modular Adaptive Sanitization Kit (MASK) for Customizable Privacy-Preserving Phone Scam Detection</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>电话诈骗仍然是全球范围内对个人安全和财务安全构成的普遍威胁。近期，大型语言模型（LLMs）在通过分析转录的电话对话来检测欺诈行为方面展现出强大的潜力。然而，这些能力也带来了显著的隐私风险，因为这类对话通常包含敏感的个人信息，在处理过程中可能会暴露给第三方服务提供商。在本研究中，我们探讨了如何利用LLMs进行电话诈骗检测，同时保护用户隐私。我们提出了MASK（模块化自适应净化工具包），这是一个可训练且可扩展的框架，能够根据个人偏好动态调整隐私保护级别。MASK提供了一种可插拔的架构，支持多样化的净化方法——从针对高隐私需求用户的传统关键词技术，到针对更注重准确性的用户所采用的复杂神经网络方法。此外，我们还讨论了未来发展的潜在建模方法和损失函数设计，从而实现真正个性化、隐私意识的基于LLMs的检测系统，这种系统能够在电话诈骗之外的场景中，在用户信任与检测效果之间取得平衡。</p>
<h3 id="Crucible-Quantifying-the-Potential-of-Control-Algorithms-through-LLM-Agents"><a href="#Crucible-Quantifying-the-Potential-of-Control-Algorithms-through-LLM-Agents" class="headerlink" title="Crucible: Quantifying the Potential of Control Algorithms through LLM Agents"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18491v1">Crucible: Quantifying the Potential of Control Algorithms through LLM Agents</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>在生产环境中，控制算法通常需要领域专家根据特定场景调整其参数和逻辑。然而，现有研究主要关注算法在理想或默认配置下的性能表现，忽视了调优潜力这一关键方面。为弥合这一差距，我们引入了Crucible，这是一个利用大型语言模型（LLM）驱动的多级专家模拟代理，能够将算法转化为可调优的形式，并定义了一个形式化的指标，以定量评估其调优潜力。我们在一系列案例研究中展示了Crucible的有效性，涵盖了从经典控制任务到复杂计算机系统的多种场景，并在真实部署中验证了其发现。实验结果表明，Crucible能够系统地量化不同算法之间的可调空间。此外，Crucible为算法分析与设计提供了一个新的维度，最终实现了性能提升。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/thu-media/Crucible%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/thu-media/Crucible获取。</a></p>
<h3 id="LIMOPro-Reasoning-Refinement-for-Efficient-and-Effective-Test-time-Scaling"><a href="#LIMOPro-Reasoning-Refinement-for-Efficient-and-Effective-Test-time-Scaling" class="headerlink" title="LIMOPro: Reasoning Refinement for Efficient and Effective Test-time Scaling"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.19187v2">LIMOPro: Reasoning Refinement for Efficient and Effective Test-time Scaling</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>大语言模型（LLMs）通过测试时扩展方法展现了卓越的推理能力，尤其是在使用从更强大的大型推理模型（LRMs）中蒸馏得到的链式推理（CoT）数据进行微调后。然而，这些推理链通常包含冗长的元素，这些元素模仿人类解决问题的方式，可分为两类：渐进式推理（核心解决方案的发展路径）和功能元素（验证过程、替代解决方案和错误修正）。虽然渐进式推理至关重要，但功能元素在测试时推理过程中显著增加了计算需求。我们引入了PIR（基于困惑度的重要性优化）框架，该框架以推理步骤对答案预测置信度的影响为依据，定量评估每个推理步骤的重要性。PIR系统地识别并仅选择性地剪枝低重要性的功能步骤，同时保留渐进式推理组件，从而创建优化后的训练数据，这些数据在保持核心解决方案路径完整性的同时减少了冗余。在PIR优化数据上微调的模型表现出更优的测试时扩展特性，生成更简洁的推理链，同时在具有挑战性的推理基准测试（如AIME、AMC和GPQA Diamond）中实现了更高的准确率（+0.9%至+6.6%），并大幅减少了标记使用量（-3%至-41%）。我们的方法在不同模型规模、数据来源和标记预算下展现出强大的泛化能力，为在需要高效测试时扩展、响应时间和计算效率的场景中部署具备推理能力的LLM提供了一种实用的解决方案。</p>
<h3 id="AndroidControl-Curated-Revealing-the-True-Potential-of-GUI-Agents-through-Benchmark-Purification"><a href="#AndroidControl-Curated-Revealing-the-True-Potential-of-GUI-Agents-through-Benchmark-Purification" class="headerlink" title="AndroidControl-Curated: Revealing the True Potential of GUI Agents through Benchmark Purification"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18488v1">AndroidControl-Curated: Revealing the True Potential of GUI Agents through Benchmark Purification</a></h3><p><strong>Categories:</strong> Large Language Models, Vision-Language Models</p>
<p>像Siri和Google Assistant这样的本地设备虚拟助手正变得越来越关键，但它们的能力却受到对僵硬、依赖开发者的API的依赖所限制。图形用户界面（GUI）代理提供了一种强大且独立于API的替代方案，但其采用受到人们对性能不佳的普遍看法的阻碍。即使是最先进的模型（例如Qwen3-VL-235B），在像AndroidControl这样的基准测试中的得分也仅能达到约60%，离实际应用场景仍有很大距离。我们的研究表明，这个问题不仅在于模型本身，也在于这些基准测试本身。我们发现AndroidControl存在一些显著的缺陷，包括模糊性和事实错误，这些系统性地低估了代理的能力。为了解决这一关键问题，我们将AndroidControl改进为AndroidControl-Curated，这是一个通过严格净化流程优化后的基准测试版本。在这一改进后的基准测试中，最先进的模型在复杂任务上的成功率接近75%（提升了15%），表明本地GUI代理实际上比之前认为的更接近实际部署。我们推出了新的SOTA模型Magma-R1-3B，该模型仅使用2400个精选样本，通过60小时的H20 GPU训练（约合60美元）。尽管该模型的参数量仅为Qwen3-VL-235B的200分之一，但其性能却与Qwen3-VL-235B相当。我们向研究社区发布了AndroidControl-Curated基准测试和Magma-R1模型，鼓励采用这一改进后的基准测试，以更准确地反映模型能力，并加速稳健的本地虚拟助手的开发。</p>
<h3 id="Tree-of-Agents-Improving-Long-Context-Capabilities-of-Large-Language-Models-through-Multi-Perspective-Reasoning"><a href="#Tree-of-Agents-Improving-Long-Context-Capabilities-of-Large-Language-Models-through-Multi-Perspective-Reasoning" class="headerlink" title="Tree of Agents: Improving Long-Context Capabilities of Large Language Models through Multi-Perspective Reasoning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2509.06436v2">Tree of Agents: Improving Long-Context Capabilities of Large Language Models through Multi-Perspective Reasoning</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>大型语言模型（LLMs）在处理长上下文任务时面临持续的挑战，其中最显著的问题是“中间信息丢失”问题，即长输入中间的信息往往被低估利用。一些现有的方法通过缩减输入存在丢失关键信息的风险，而另一些通过扩展上下文窗口则容易导致注意力分散。为了解决这些局限性，我们提出了Tree of Agents（TOA），这是一种多智能体推理框架，将输入分割成由独立智能体处理的块。每个智能体生成其本地认知，然后智能体沿着树状结构的路径动态交换信息，进行协作推理。TOA使智能体能够从多角度理解，探索不同的推理顺序，从而有效缓解位置偏差并减少幻觉。为了提高处理效率，我们引入了前缀哈希缓存和自适应剪枝策略，在API开销相当的情况下实现了显著的性能提升。实验表明，基于紧凑型LLaMA3.1-8B的TOA在多种长上下文任务中显著优于多个基线模型，并且在性能上与最新且规模更大的商业模型（如Gemini1.5-pro）相当。代码可在<a target="_blank" rel="noopener" href="https://github.com/Aireduce952/Tree-of-Agents%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Aireduce952/Tree-of-Agents获取。</a></p>
<h3 id="Exploring-Data-Efficient-Adaptation-of-Large-Language-Models-for-Code-Generation"><a href="#Exploring-Data-Efficient-Adaptation-of-Large-Language-Models-for-Code-Generation" class="headerlink" title="Exploring Data-Efficient Adaptation of Large Language Models for Code Generation"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2403.00046v3">Exploring Data-Efficient Adaptation of Large Language Models for Code Generation</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>尽管大型语言模型（LLMs）在代码生成方面取得了显著进展，但在特定场景下的代码生成任务中仍存在困难。这些场景通常需要对LLMs进行适应以满足特定需求，但实际中可用的训练数据有限，导致代码生成性能较差。因此，如何以少量训练数据有效地适应LLMs以应对新场景，是当前代码生成领域面临的主要挑战。本文提出了一种名为DEED的新颖适应方法，其名称代表“数据高效且以错误驱动学习的代码生成适应方法”。DEED利用LLMs生成的错误作为学习机会，通过错误修正来克服自身的不足，从而实现高效学习。具体而言，DEED包括以下几个步骤：识别LLMs生成的错误代码、使用Self-Revise进行代码修正、利用修正后的代码优化模型，并通过迭代适应过程实现持续改进。实验结果表明，与其它主流微调方法相比，DEED在少量训练数据下表现更优，在多个代码生成基准测试中，其Pass@1指标平均相对提升达46.2%。此外，我们还验证了Self-Revise的有效性，其生成的修正代码相比数据集中的代码样本，能更高效地优化模型。此外，DEED在各种LLMs上均表现出色，凸显了其广泛适用性。</p>
<h3 id="LAFA-Agentic-LLM-Driven-Federated-Analytics-over-Decentralized-Data-Sources"><a href="#LAFA-Agentic-LLM-Driven-Federated-Analytics-over-Decentralized-Data-Sources" class="headerlink" title="LAFA: Agentic LLM-Driven Federated Analytics over Decentralized Data Sources"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18477v1">LAFA: Agentic LLM-Driven Federated Analytics over Decentralized Data Sources</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>大型语言模型（LLMs）在通过理解自然语言查询并生成多操作执行计划来自动化数据分析任务方面展现出巨大潜力。然而，现有的基于LLM代理的分析框架假设数据访问是集中的，几乎没有提供隐私保护。相比之下，联邦分析（FA）能够在分布式数据源之间实现隐私保护的计算，但它缺乏对自然语言输入的支持，并且需要结构化的、机器可读的查询。在本工作中，我们提出了LAFA，这是首个将基于LLM代理的数据分析与联邦分析相结合的系统。LAFA引入了一种层次化的多代理架构，能够接受自然语言查询，并将其转换为优化且可执行的FA工作流。一个粗粒度规划器首先将复杂查询分解为子查询，而一个细粒度规划器则利用先验结构知识，将每个子查询映射为FA操作的有向无环图（DAG）。为了提高执行效率，一个优化代理会重写和合并多个DAG，消除冗余操作，并最大限度地减少计算和通信开销。我们的实验表明，LAFA在执行计划成功率方面显著优于基线提示策略，并大幅减少了资源密集型的FA操作。本工作为支持自然语言输入的隐私保护、由LLM驱动的分析奠定了实际基础。</p>
<h3 id="Probabilistic-Modeling-of-Intentions-in-Socially-Intelligent-LLM-Agents"><a href="#Probabilistic-Modeling-of-Intentions-in-Socially-Intelligent-LLM-Agents" class="headerlink" title="Probabilistic Modeling of Intentions in Socially Intelligent LLM Agents"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18476v1">Probabilistic Modeling of Intentions in Socially Intelligent LLM Agents</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>我们提出了一种用于多轮社交对话中大型语言模型（LLM）代理的基于概率的意图建模框架。该框架在合作伙伴的潜在意图上维护一个信念分布，该分布从上下文先验初始化，并在每次发言后通过似然估计动态更新。这种演化的分布为策略提供了额外的上下文基础，从而在不确定性下实现自适应的对话策略。</p>
<p>在SOTOPIA环境中的初步实验表明，该框架带来了持续的改进：与Qwen2.5-7B基线相比，在SOTOPIA-All上整体得分提高了9.0%，在SOTOPIA-Hard上提高了4.1%。此外，该框架略微超越了一个可以直接观察到合作伙伴意图的“理想”代理。这些初步结果表明，基于概率的意图建模有助于开发具有社会智能的LLM代理。</p>
<h3 id="Simple-and-Efficient-Heterogeneous-Temporal-Graph-Neural-Network"><a href="#Simple-and-Efficient-Heterogeneous-Temporal-Graph-Neural-Network" class="headerlink" title="Simple and Efficient Heterogeneous Temporal Graph Neural Network"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18467v1">Simple and Efficient Heterogeneous Temporal Graph Neural Network</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>异构时间图（HTGs）是现实世界中普遍存在的数据结构。最近，为了增强在HTGs上的表示学习，已经提出了许多基于注意力机制的神经网络。尽管这些方法取得了成功，但现有方法依赖于分离的时间和空间学习范式，这削弱了时空信息之间的交互作用，并导致模型复杂度较高。为了解决这一问题，我们提出了一种新的HTG学习范式，称为简单且高效的异构时间图神经网络（SE-HTGNN）。具体来说，我们通过一种新颖的动态注意力机制，创新性地将时间建模融入空间学习过程中，该机制保留了历史图快照中的注意力信息，以指导后续的注意力计算，从而提升HTGs的整体判别表示学习能力。此外，为了全面且自适应地理解HTGs，我们利用大语言模型对SE-HTGNN进行提示，使模型能够捕捉节点类型的隐含属性作为先验知识。大量实验表明，SE-HTGNN在保持最佳预测精度的同时，相比最先进的基线模型速度提升了高达10倍。</p>
<h3 id="PlanU-Large-Language-Model-Decision-Making-through-Planning-under-Uncertainty"><a href="#PlanU-Large-Language-Model-Decision-Making-through-Planning-under-Uncertainty" class="headerlink" title="PlanU: Large Language Model Decision Making through Planning under Uncertainty"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18442v1">PlanU: Large Language Model Decision Making through Planning under Uncertainty</a></h3><p><strong>Categories:</strong> Large Language Models,Reinforcement Learning</p>
<p>大语言模型（LLMs）正越来越多地被应用于各种决策任务中。然而，LLMs在处理某些不确定性较高的决策任务时常常表现不佳，而这些任务对人类来说相对容易，例如在随机环境中进行行动规划。LLMs在决策应用中的采用受到不确定性挑战的阻碍，例如LLM自身的不确定性以及环境的不确定性。LLM的不确定性源于其固有的随机抽样过程。大多数基于LLM的决策方法（LDM）通过多条推理链或搜索树来应对LLM的不确定性。然而，这些方法忽略了环境的不确定性，导致在状态转移具有随机性的环境中表现不佳。一些近期的LDM方法通过预测未知变量的概率来处理不确定性。但它们并不是为需要与环境进行交互的多步决策任务而设计的。为了解决LLM决策中的不确定性问题，我们引入了PlanU，这是一种基于LLM的规划方法，它通过蒙特卡洛树搜索（MCTS）来捕捉不确定性。PlanU将MCTS中每个节点的回报建模为一个分位数分布，使用一组分位数来表示回报分布。为了在树搜索过程中平衡探索与利用，PlanU引入了一种结合好奇心的置信上限（UCC）得分，用于估计MCTS节点的不确定性。通过大量实验，我们验证了PlanU在不确定性环境下的LLM决策任务中的有效性。</p>
<h3 id="AlphaOPT-Formulating-Optimization-Programs-with-Self-Improving-LLM-Experience-Library"><a href="#AlphaOPT-Formulating-Optimization-Programs-with-Self-Improving-LLM-Experience-Library" class="headerlink" title="AlphaOPT: Formulating Optimization Programs with Self-Improving LLM Experience Library"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18428v1">AlphaOPT: Formulating Optimization Programs with Self-Improving LLM Experience Library</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>优化建模在各个行业中支持关键决策，但自动化仍面临困难：需要将非正式语言映射为精确的数学公式和可执行求解器代码。以往的大型语言模型（LLM）方法要么依赖脆弱的提示，要么需要昂贵的重新训练且泛化能力有限。我们提出了AlphaOPT，这是一个自我改进的经验库，使LLM能够从有限的演示（甚至仅答案，而无需黄金标准程序）和求解器反馈中学习——无需标注的推理轨迹或参数更新。AlphaOPT运行在一个持续的两阶段循环中：(i) 一个“库学习”阶段，通过反思失败尝试，提取经过求解器验证的结构化见解，形式为{分类、条件、解释、示例}；(ii) 一个“库进化”阶段，诊断检索对齐问题，并优化存储见解的应用条件，从而提升在不同任务之间的迁移能力。这种设计具有以下优点：(1) 无需人工编排的推理过程，即可高效地从有限演示中学习；(2) 通过更新库而非模型权重，持续扩展模型能力，而无需昂贵的重新训练；(3) 使知识显式化并可解释，便于人类检查和干预。实验表明，随着数据量的增加，AlphaOPT性能稳步提升（从100个训练项到300个训练项，准确率从65%提升至72%），仅使用答案训练时，在分布外的OptiBench数据集上，其表现超过最强基线7.7%。代码和数据可在以下链接获取：<a target="_blank" rel="noopener" href="https://github.com/Minw913/AlphaOPT%E3%80%82">https://github.com/Minw913/AlphaOPT。</a></p>
<h3 id="When-Agents-go-Astray-Course-Correcting-SWE-Agents-with-PRMs"><a href="#When-Agents-go-Astray-Course-Correcting-SWE-Agents-with-PRMs" class="headerlink" title="When Agents go Astray: Course-Correcting SWE Agents with PRMs"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2509.02360v2">When Agents go Astray: Course-Correcting SWE Agents with PRMs</a></h3><p><strong>Categories:</strong> Large Language Models, Reinforcement Learning</p>
<p>大型语言模型（LLM）代理正越来越多地用于处理复杂且涉及多步骤的软件工程（SWE）任务。然而，这些代理的执行轨迹中常常存在高成本的低效问题，例如冗余探索、循环以及在找到解决方案后仍无法终止。以往的研究大多采用事后处理方式来处理这些错误，仅在执行完成后诊断失败原因。在本文中，我们引入了SWE-PRM，这是一种在推理过程中进行干预的流程奖励模型（PRM），能够在执行过程中检测并纠正轨迹级别的错误。我们的PRM设计利用了一种常见的低效问题分类体系，能够在不修改底层策略的情况下提供轻量级且可解释的反馈。在SWE-bench Verified数据集上，闭源PRM将问题解决率从40.0%提升至50.6%（+10.6个百分点），在中等和困难任务上提升最为显著。在反馈策略中，基于分类体系的PRM优于无指导或显式动作指示的变体，不仅提高了成功率，还减少了轨迹长度。这些优势的实现仅需极低的推理成本，低至0.2美元，使PRM成为提升SWE代理可靠性和效率的实用且可扩展机制。</p>
<h3 id="Explaining-Large-Language-Models-with-gSMILE"><a href="#Explaining-Large-Language-Models-with-gSMILE" class="headerlink" title="Explaining Large Language Models with gSMILE"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.21657v5">Explaining Large Language Models with gSMILE</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>像GPT、LLaMA和Claude这样的大型语言模型（LLMs）在文本生成方面表现出色，但其决策过程仍然不透明，这限制了在高风险应用中的信任和问责。我们提出了gSMILE（生成性SMILE），这是一个模型无关的、基于扰动的框架，用于LLMs的词级可解释性。gSMILE扩展了SMILE方法，通过受控的提示扰动、Wasserstein距离度量和加权线性代理模型，来识别对输出影响最大的输入词。这一过程能够生成直观的热图，以视觉方式突出显示有影响力的词和推理路径。我们使用归因保真度、归因一致性、归因稳定性、归因忠实度和归因准确性作为指标，在领先的LLMs（OpenAI的gpt-3.5-turbo-instruct、Meta的LLaMA 3.1 Instruct Turbo和Anthropic的Claude 2.1）上评估gSMILE。结果表明，gSMILE能够提供可靠的人类对齐归因，其中Claude 2.1在注意力保真度方面表现突出，而GPT-3.5实现了最高的输出一致性。这些发现展示了gSMILE在平衡模型性能和可解释性方面的能力，从而使得AI系统更加透明和值得信赖。</p>
<h3 id="PokeeResearch-Effective-Deep-Research-via-Reinforcement-Learning-from-AI-Feedback-and-Robust-Reasoning-Scaffold"><a href="#PokeeResearch-Effective-Deep-Research-via-Reinforcement-Learning-from-AI-Feedback-and-Robust-Reasoning-Scaffold" class="headerlink" title="PokeeResearch: Effective Deep Research via Reinforcement Learning from AI Feedback and Robust Reasoning Scaffold"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.15862v3">PokeeResearch: Effective Deep Research via Reinforcement Learning from AI Feedback and Robust Reasoning Scaffold</a></h3><p><strong>Categories:</strong> Large Language Models, Reinforcement Learning</p>
<p>工具增强型大语言模型（LLMs）正逐渐成为深度研究代理，这些系统能够分解复杂查询、检索外部证据并综合出有依据的回应。然而，当前的代理仍受到浅层检索、弱对齐度度量和脆弱的工具使用行为的限制。我们引入了PokeeResearch-7B，这是一种基于统一强化学习框架构建的70亿参数深度研究代理，旨在实现鲁棒性、对齐度和可扩展性。PokeeResearch-7B通过一种无需人工标注的基于AI反馈的强化学习（RLAIF）框架进行训练，利用基于LLM的奖励信号优化策略，这些奖励信号能够捕捉事实准确性、引用忠实性和指令遵循性。一种基于思维链驱动的多调用推理框架进一步通过自我验证和自适应恢复机制增强了系统的鲁棒性。在10个流行的深度研究基准测试中，PokeeResearch-7B在70亿参数规模的深度研究代理中取得了最先进的性能。这表明，精心设计的强化学习和推理机制可以产生高效、稳健且具备研究级能力的AI代理。该模型和推理代码已根据Apache 2.0许可证开源，地址为：<a target="_blank" rel="noopener" href="https://github.com/Pokee-AI/PokeeResearchOSS%E3%80%82">https://github.com/Pokee-AI/PokeeResearchOSS。</a></p>
<h3 id="Benchmarking-On-Device-Machine-Learning-on-Apple-Silicon-with-MLX"><a href="#Benchmarking-On-Device-Machine-Learning-on-Apple-Silicon-with-MLX" class="headerlink" title="Benchmarking On-Device Machine Learning on Apple Silicon with MLX"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18921v1">Benchmarking On-Device Machine Learning on Apple Silicon with MLX</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>最近，大型语言模型（LLMs）和机器学习的广泛应用激发了人们对探索在小型设备（如笔记本电脑和手机）上部署这些模型的可能性的研究兴趣。这促使了对能够充分利用设备本地硬件的框架和方法的需求。MLX框架正是为满足这一需求而创建的。它是一个专门为苹果硅芯片设备上的机器学习（ML）计算优化的框架，有助于更轻松地进行研究、实验和原型开发。</p>
<p>本文对MLX进行了性能评估，重点考察了Transformer模型的推理延迟。我们比较了MLX中不同Transformer架构实现的性能与其PyTorch对应实现的性能。为了进行这项研究，我们创建了一个名为MLX-transformers的框架，其中包括MLX中的多种Transformer实现，并从PyTorch下载模型检查点，将其转换为MLX格式。通过利用苹果硅芯片的先进架构和强大功能，MLX-Transformers能够无缝执行直接从Hugging Face获取的Transformer模型，从而消除了在不同框架之间移植模型时通常需要的检查点转换过程。</p>
<p>我们的研究在两台苹果硅芯片MacBook设备上，与NVIDIA CUDA GPU进行对比，对不同的Transformer模型进行了基准测试。具体而言，我们比较了参数量和检查点相同的模型的推理延迟性能。我们评估了BERT、RoBERTa和XLM-RoBERTa模型的性能，并计划将未来的工作扩展到包含不同模态的模型，从而对MLX的能力进行更全面的评估。实验结果突显了MLX在苹果生态系统中实现高效且更易访问的本地机器学习应用方面的潜力。</p>
<h3 id="Memory-Augmented-State-Machine-Prompting-A-Novel-LLM-Agent-Framework-for-Real-Time-Strategy-Games"><a href="#Memory-Augmented-State-Machine-Prompting-A-Novel-LLM-Agent-Framework-for-Real-Time-Strategy-Games" class="headerlink" title="Memory-Augmented State Machine Prompting: A Novel LLM Agent Framework for Real-Time Strategy Games"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18395v1">Memory-Augmented State Machine Prompting: A Novel LLM Agent Framework for Real-Time Strategy Games</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>本文提出了一种名为“记忆增强状态机提示”（Memory-Augmented State Machine Prompting, MASMP）的全新框架，用于在实时策略游戏中提升大语言模型（LLM）代理的表现。该框架针对现有方法中存在的幻觉现象和决策碎片化等关键问题，通过将状态机提示与记忆机制相结合，统一结构化动作与长期战术一致性。该框架具有以下两个主要特点：（1）一种由自然语言驱动的状态机架构，通过提示引导LLM模拟有限状态机和行为树；（2）一种轻量级的记忆模块，用于在决策周期中保留战略变量（如战术、优先单位等）。在《星际争霸II》中的实验表明，MASMP在面对最难的内置AI（Lv7）时取得了60%的胜率，远超其他基线方法（0%）。案例研究表明，该方法在保留LLM语义理解能力的同时，通过严格的状态-动作映射解决了“知道-做到”之间的差距，实现了可解释性与类似有限状态机（FSM）的可靠性。这项工作为在复杂决策中结合神经网络与符号AI建立了一个新的范式。</p>
<h3 id="SAFER-Risk-Constrained-Sample-then-Filter-in-Large-Language-Models"><a href="#SAFER-Risk-Constrained-Sample-then-Filter-in-Large-Language-Models" class="headerlink" title="SAFER: Risk-Constrained Sample-then-Filter in Large Language Models"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.10193v2">SAFER: Risk-Constrained Sample-then-Filter in Large Language Models</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>随着大语言模型（LLMs）越来越多地部署在对风险敏感的应用场景中，例如现实世界中的开放式问答（QA），确保其输出的可信度变得至关重要。现有的选择性合意预测（SCP）方法通过构建具有受限误覆盖率的预测集，为正确答案提供统计保证。然而，以往的研究不合理地假设，所有实例的可接受答案都可以通过有限采样获得，即使是在缺乏固定且有限解空间的开放式问答场景中也是如此。为了解决这一问题，我们引入了一种两阶段风险控制框架，包含“考虑拒绝的采样”（abstention-aware sampling）和“合意化过滤”（conformalized filtering）（SAFER）。首先，在一个保留的校准数据集上，SAFER在最大采样上限内，根据用户指定的风险水平（即采样集的最大可接受误覆盖率）使用Clopper-Pearson精确方法校准采样预算。如果在预算上限内无法满足风险水平，则拒绝回答；否则，校准后的采样预算成为测试阶段的最低要求。随后，我们使用在校准预算下能够获得正确答案的校准实例，并应用合意风险控制方法来确定一个统计上有效的不确定性阈值，从而从每个测试数据点的候选集中过滤掉不可靠的干扰项。在这一阶段，SAFER引入了一个额外的风险水平，以指导阈值的计算，从而控制正确答案被排除的风险。此外，我们还证明了SAFER能够兼容各种任务特定的准入标准和校准-测试数据分割比例，突显了其鲁棒性和高数据效率。</p>
<h3 id="SpecExit-Accelerating-Large-Reasoning-Model-via-Speculative-Exit"><a href="#SpecExit-Accelerating-Large-Reasoning-Model-via-Speculative-Exit" class="headerlink" title="SpecExit: Accelerating Large Reasoning Model via Speculative Exit"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2509.24248v2">SpecExit: Accelerating Large Reasoning Model via Speculative Exit</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>尽管大型推理模型（LRMs）在推理任务中表现出色，但它们常常会过度思考，产生不必要的长输出并导致端到端延迟较高，这成为其在现实场景中部署的重要限制。为了解决过度思考问题，人们提出了早期退出机制，即在常规完成之前终止推理过程，这种方法能够有效缩短生成长度，同时对准确性影响极小。然而，这些机制依赖于探测机制，引入了额外的检测开销，限制了其端到端延迟的优化效果，并影响了其在不同问题上的泛化能力。受推测解码中使用隐藏状态的启发，我们提出了一种新颖的框架SpecExit，该框架可以直接从轻量级草稿模型中预测未来标记和早期退出信号，而无需探测开销。我们的方法实现了显著改进，将平均生成长度减少了66%，在端到端延迟方面相比推测解码基线提升了2.5倍，且不牺牲准确性。我们的方法利用隐藏状态中的固有信号，提供有效的早期退出信号，表明隐藏状态在高效推理中具有更广泛的应用前景。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/Tencent/AngelSlim%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/Tencent/AngelSlim获取。</a></p>
<h3 id="SoK-Taxonomy-and-Evaluation-of-Prompt-Security-in-Large-Language-Models"><a href="#SoK-Taxonomy-and-Evaluation-of-Prompt-Security-in-Large-Language-Models" class="headerlink" title="SoK: Taxonomy and Evaluation of Prompt Security in Large Language Models"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.15476v2">SoK: Taxonomy and Evaluation of Prompt Security in Large Language Models</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>大语言模型（LLMs）迅速成为现实世界应用的重要组成部分，为跨多个领域的服务提供支持。然而，其广泛应用也暴露出关键的安全风险，特别是通过“越狱提示”（jailbreak prompts）可以绕过模型对齐机制，从而引发有害输出。尽管针对攻击和防御技术的研究非常深入，但该领域仍然存在碎片化问题：定义、威胁模型和评估标准差异很大，阻碍了系统性进展和公平比较。在本项知识系统化（Systematization of Knowledge, SoK）工作中，我们通过以下五个方面解决这些挑战：（1）提出一个全面的多层次分类体系，用于组织LLM提示安全中的攻击、防御和漏洞；（2）将威胁模型和成本假设形式化为可机器读取的配置文件，以实现可重复的评估；（3）引入一个开源评估工具包，用于标准化、可审计的攻击与防御方法比较；（4）发布JAILBREAKDB，这是迄今为止最大的包含越狱提示和良性提示的标注数据集；\footnote{该数据集可在<br>\href{<a target="_blank" rel="noopener" href="https://huggingface.co/datasets/youbin2014/JailbreakDB%7D%7B/textcolor%7Bpurple%7D%7Bhttps://huggingface.co/datasets/youbin2014/JailbreakDB%7D%7D">https://huggingface.co/datasets/youbin2014/JailbreakDB}{\textcolor{purple}{https://huggingface.co/datasets/youbin2014/JailbreakDB}}</a> 上获取。}（5）展示一个涵盖最先进方法的全面评估平台和排行榜 \footnote{即将发布。}。我们的工作整合了碎片化的研究，为未来的研究提供了严谨的基础，并支持开发稳健、可信赖的LLMs，以适用于高风险部署场景。</p>
<h3 id="Multi-Agent-Collaboration-via-Evolving-Orchestration"><a href="#Multi-Agent-Collaboration-via-Evolving-Orchestration" class="headerlink" title="Multi-Agent Collaboration via Evolving Orchestration"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.19591v2">Multi-Agent Collaboration via Evolving Orchestration</a></h3><p><strong>Categories:</strong> Large Language Models,Reinforcement Learning</p>
<p>大语言模型（LLMs）在各种下游任务中取得了显著成果，但其单体结构限制了其在复杂问题解决中的可扩展性和效率。虽然近期的研究探索了LLMs之间的多智能体协作，但大多数方法依赖于静态的组织结构，难以随着任务复杂性和智能体数量的增加而适应，导致协调开销和效率低下。为此，我们提出了一种类似“操纵者”（puppeteer）的范式，用于基于LLMs的多智能体协作。其中，一个集中式的协调者（“操纵者”）根据任务状态的变化动态地指导智能体（“傀儡”）。该协调者通过强化学习进行训练，以适应性地安排和优先级排序智能体，从而实现灵活且可演化的集体推理。在封闭域和开放域场景的实验表明，该方法在降低计算成本的同时实现了更优的性能。进一步的分析还揭示，关键的性能提升始终来源于协调者演化过程中出现的更为紧凑、循环的推理结构。我们的代码可在 <a target="_blank" rel="noopener" href="https://github.com/OpenBMB/ChatDev/tree/puppeteer">https://github.com/OpenBMB/ChatDev/tree/puppeteer</a> 获取。</p>
<h3 id="Learning-to-Interpret-Weight-Differences-in-Language-Models"><a href="#Learning-to-Interpret-Weight-Differences-in-Language-Models" class="headerlink" title="Learning to Interpret Weight Differences in Language Models"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.05092v3">Learning to Interpret Weight Differences in Language Models</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>微调（预训练）语言模型是一种用于更新其内部参数化知识并将其专门化到新任务和领域中的标准方法。然而，相应的模型权重变化（”权重差异”）通常难以解释。虽然检查微调数据集可以在一定程度上了解模型可能发生了哪些变化，但这些数据集通常不公开或太大而无法直接处理。为了全面理解自然语言中的权重差异，我们引入了“差异解释微调”（DIT）方法，该方法训练模型描述其自身由微调引起的修改。我们的方法使用合成且带标签的权重差异来训练DIT适配器，该适配器可以应用于兼容的微调模型，使其描述自身发生了哪些变化。我们在两个概念验证场景中展示了我们的方法：报告隐藏行为和总结微调知识。结果表明，我们的方法使模型能够使用准确的自然语言描述来说明其由微调引起的修改。</p>
<h3 id="SceneCOT-Eliciting-Grounded-Chain-of-Thought-Reasoning-in-3D-Scenes"><a href="#SceneCOT-Eliciting-Grounded-Chain-of-Thought-Reasoning-in-3D-Scenes" class="headerlink" title="SceneCOT: Eliciting Grounded Chain-of-Thought Reasoning in 3D Scenes"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.16714v2">SceneCOT: Eliciting Grounded Chain-of-Thought Reasoning in 3D Scenes</a></h3><p><strong>Categories:</strong> Large Language Models, Vision-Language Models</p>
<p>目前关于三维大型语言模型（LLMs）的研究仍难以实现基于场景的问答（grounded question-answering），这主要是因为对类似人类的场景-物体基于场景的推理机制研究尚不充分。本文通过提出一种新颖的框架来弥合这一差距。我们首先在三维场景中引入了一种基于场景的链式推理方法（SCENECOT），将复杂的推理任务分解为更简单且易于处理的子问题，并基于多模态专家模块构建相应的视觉线索。为了实现这一方法，我们开发了SCENECOT-185K，这是首个大规模的基于场景的链式推理数据集，包含185,000个高质量实例。在多个复杂三维场景推理基准测试中进行的广泛实验表明，我们的新框架在高场景问答一致性下表现出色。据我们所知，这是首次将链式推理应用于三维场景理解，实现了逐步的人类式推理，并展示了其在更广泛三维场景理解场景中扩展的潜力。</p>
<h3 id="Temporal-Alignment-of-LLMs-through-Cycle-Encoding-for-Long-Range-Time-Representations"><a href="#Temporal-Alignment-of-LLMs-through-Cycle-Encoding-for-Long-Range-Time-Representations" class="headerlink" title="Temporal Alignment of LLMs through Cycle Encoding for Long-Range Time Representations"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2503.04150v3">Temporal Alignment of LLMs through Cycle Encoding for Long-Range Time Representations</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>大语言模型（LLMs）在长时间跨度上存在时间对齐问题。这种问题源于LLMs在大量数据上进行训练，而这些数据在长时间跨度（如数千年）上时间信息较为稀疏，导致LLMs出现学习不足或灾难性遗忘。本文提出了一种名为“Ticktack”的方法，用于在年份尺度下解决LLMs的长时间跨度对齐问题。具体而言，我们首先提出使用六十甲子纪年法，而非LLMs通常使用的公历年份表达方式，以实现年份粒度上的更均匀分布。随后，我们采用极坐标来建模六十甲子周期以及每个甲子周期内的年份顺序，并通过额外的时间编码确保LLMs能够理解这些信息。最后，我们提出了一种时间表示对齐方法，用于微调后的LLMs，该方法能够有效区分具有相关知识的时间点，从而提升与时间相关任务的性能，特别是在长时间跨度上的表现。我们还构建了一个长时间跨度基准测试集用于评估。实验结果验证了我们方法的有效性。</p>
<h3 id="Program-Synthesis-via-Test-Time-Transduction"><a href="#Program-Synthesis-via-Test-Time-Transduction" class="headerlink" title="Program Synthesis via Test-Time Transduction"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2509.17393v3">Program Synthesis via Test-Time Transduction</a></h3><p><strong>Categories:</strong> Large Language Models, World Models</p>
<p>我们介绍了一种新的程序合成方法——归纳式程序合成（transductive program synthesis），该方法在合成过程中明确利用测试输入。与以往基于自然语言描述或输入-输出示例的程序合成方法不同，这些方法通常旨在从训练示例中进行泛化，但在现实场景中往往难以保证鲁棒性，尤其是在训练示例有限且测试输入包含各种边缘情况的情况下。为了解决这一问题，我们提出了一种新的框架，通过将合成视为在由程序输出定义的有限假设类上的主动学习，从而提高鲁棒性。我们使用大型语言模型（LLM）预测选定测试输入的输出，并消除不一致的假设，其中输入的选择采用贪心最大最小算法（greedy maximin algorithm），以最小化所需的LLM查询次数。我们在四个基准数据集上评估了我们的方法：Playgol、MBPP+、1D-ARC 以及 MiniGrid 上的程序化世界建模。实验结果表明，我们的方法在准确性和效率方面显著提升了程序合成的效果。我们将在 <a target="_blank" rel="noopener" href="https://github.com/klee972/SYNTRA">https://github.com/klee972/SYNTRA</a> 发布我们的代码。</p>
<h3 id="Misinformation-Detection-using-Large-Language-Models-with-Explainability"><a href="#Misinformation-Detection-using-Large-Language-Models-with-Explainability" class="headerlink" title="Misinformation Detection using Large Language Models with Explainability"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18918v1">Misinformation Detection using Large Language Models with Explainability</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>在线平台上虚假信息的迅速传播削弱了个体之间的信任，并阻碍了知情决策。本文展示了一种可解释且计算效率高的流程，利用基于Transformer的预训练语言模型（PLM）来检测虚假信息。我们采用两步策略对RoBERTa和DistilBERT进行了优化：首先，我们冻结主干网络并仅训练分类头；然后，逐步解冻主干网络层，同时应用逐层学习率衰减。我们在两个现实世界基准数据集——“COVID Fake News”和“FakeNewsNet GossipCop”上，使用统一的预处理协议和分层划分方法测试了所提出的方法。为确保透明度，我们在词级别整合了局部可解释模型无关解释（LIME）以呈现词级别的解释理由，并在全局特征归因级别使用Shapley加法解释（SHAP）。实验结果表明，DistilBERT在准确率上与RoBERTa相当，但所需的计算资源显著减少。本研究的两个关键贡献是：（1）定量证明轻量级PLM可以在大幅降低计算成本的同时保持任务性能；（2）提出了一种可解释的流程，能够检索忠实的局部和全局解释，而不影响性能。实验结果表明，结合合理微调和可解释性的PLM可以成为一种有效的、可扩展且可信的虚假信息检测框架。</p>
<h3 id="“She’s-Like-a-Person-but-Better”-Characterizing-Companion-Assistant-Dynamics-in-Human-AI-Relationships"><a href="#“She’s-Like-a-Person-but-Better”-Characterizing-Companion-Assistant-Dynamics-in-Human-AI-Relationships" class="headerlink" title="“She’s Like a Person but Better”: Characterizing Companion-Assistant Dynamics in Human-AI Relationships"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.15905v2">“She’s Like a Person but Better”: Characterizing Companion-Assistant Dynamics in Human-AI Relationships</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>大型语言模型正越来越多地用于任务型协助和社会陪伴，但研究通常集中于其中一方面。基于一项调查（N&#x3D;204）和30次对高参与度ChatGPT和Replika用户的访谈，我们将数字陪伴描述为一种新兴的人机关系形式。在使用这两个系统时，用户被其类似人类的特质所吸引，如情感共鸣和个性化回应，同时也被其非人类特质所吸引，如全天候可用性和无尽的包容性。这导致了灵活的聊天机器人使用方式，例如将Replika作为写作助手，将ChatGPT作为情感倾诉对象，尽管它们的品牌定位各不相同。然而，我们观察到数字陪伴关系中存在一些具有挑战性的张力：参与者在形成深厚依恋的同时，却否认聊天机器人具有“真正”的人类特质，并且难以将聊天机器人关系与社会规范相协调。这些动态对数字陪伴的设计以及混合型、通用型人工智能系统的兴起提出了问题。</p>
<h3 id="MMAO-Bench-MultiModal-All-in-One-Benchmark-Reveals-Compositional-Law-between-Uni-modal-and-Omni-modal-in-OmniModels"><a href="#MMAO-Bench-MultiModal-All-in-One-Benchmark-Reveals-Compositional-Law-between-Uni-modal-and-Omni-modal-in-OmniModels" class="headerlink" title="MMAO-Bench: MultiModal All in One Benchmark Reveals Compositional Law between Uni-modal and Omni-modal in OmniModels"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18915v1">MMAO-Bench: MultiModal All in One Benchmark Reveals Compositional Law between Uni-modal and Omni-modal in OmniModels</a></h3><p><strong>Categories:</strong> Large Language Models, Vision-Language Models</p>
<p>多模态大语言模型正在从单模态理解向统一视觉、音频和语言模态的方向发展，这种综合模型被称为“全模态模型”（omni models）。然而，单模态与全模态之间的关联尚不明确，这需要全面的评估来推动全模态模型的智能演进。在本工作中，我们提出了一种新颖、高质量且多样化的全模态模型基准测试平台——MultiModal All in One Benchmark（MMAO-Bench），该基准能够有效评估单模态和全模态的理解能力。该基准包含1880个由人类精心策划的样本，涵盖44种任务类型，并引入了一种创新的多步骤开放式问题类型，能够更好地评估复杂的推理任务。实验结果表明，跨模态与单模态性能之间存在组合规律，全模态能力在弱模型中表现出瓶颈效应，而在强模型中则展现出协同促进效应。</p>
<h3 id="Context-aware-Fairness-Evaluation-and-Mitigation-in-LLMs"><a href="#Context-aware-Fairness-Evaluation-and-Mitigation-in-LLMs" class="headerlink" title="Context-aware Fairness Evaluation and Mitigation in LLMs"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18914v1">Context-aware Fairness Evaluation and Mitigation in LLMs</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>大型语言模型通常会在其内部表示中表现出一些不希望的行为，这些行为会损害公平性、导致不一致性漂移、放大有害内容以及传播不受欢迎的模式，特别是在长时间的对话和交流中。尽管在训练阶段或数据驱动的方法尝试减少这些影响，但这些方法计算成本高、一旦部署就不可逆，并且难以快速适应新的对话环境。基于剪枝的方法提供了一种灵活且透明的方式来减少偏差，通过调整负责某些行为的神经元来实现。然而，大多数现有方法是静态的；一旦某个神经元被移除，模型在对话或上下文变化时将失去适应能力。为了解决这一问题，我们提出了一种动态、可逆的基于剪枝的框架，该框架能够检测上下文感知的神经元激活，并在生成过程中应用自适应掩码以调节其影响。我们的推理阶段解决方案提供了细粒度的、内存感知的缓解措施，在多语言的单轮和多轮对话中保持知识保留、行为更一致，从而实现实时对话AI中的动态公平性控制。</p>
<h3 id="Genesis-Evolving-Attack-Strategies-for-LLM-Web-Agent-Red-Teaming"><a href="#Genesis-Evolving-Attack-Strategies-for-LLM-Web-Agent-Red-Teaming" class="headerlink" title="Genesis: Evolving Attack Strategies for LLM Web Agent Red-Teaming"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18314v1">Genesis: Evolving Attack Strategies for LLM Web Agent Red-Teaming</a></h3><p><strong>Categories:</strong> Large Language Models, Reinforcement Learning</p>
<p>随着大型语言模型（LLM）代理越来越多地自动化复杂的网络任务，它们在提高生产力的同时，也引入了新的安全风险。然而，针对网络代理攻击的相关研究仍较为有限。现有的红队（red-teaming）方法主要依赖于人工设计的攻击策略或离线训练的静态模型。这些方法无法捕捉网络代理的底层行为模式，使得其难以在多样化的环境中进行泛化。在网络代理攻击中，成功需要持续发现和演进攻击策略。为此，我们提出了Genesis，一个由三个模块组成的新型代理框架：攻击者（Attacker）、评分者（Scorer）和策略制定者（Strategist）。攻击者通过将遗传算法与混合策略表示相结合，生成对抗性注入。评分者评估目标网络代理的响应以提供反馈。策略制定者则从交互日志中动态发现有效的策略，并将其编译进一个不断增长的策略库中，然后重新部署以提升攻击者的有效性。在各种网络任务上的广泛实验表明，我们的框架能够发现新颖的策略，并且在现有攻击基线中表现优异。</p>
<h3 id="Offline-Policy-Evaluation-of-Multi-Turn-LLM-Health-Coaching-with-Real-Users"><a href="#Offline-Policy-Evaluation-of-Multi-Turn-LLM-Health-Coaching-with-Real-Users" class="headerlink" title="Offline Policy Evaluation of Multi-Turn LLM Health Coaching with Real Users"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.17173v2">Offline Policy Evaluation of Multi-Turn LLM Health Coaching with Real Users</a></h3><p><strong>Categories:</strong> Large Language Models,Reinforcement Learning</p>
<p>我们研究了一种基于网络部署、增强工具的大型语言模型（LLM）健康教练，并使用真实用户进行测试。在一项涉及七名用户（共280次评价回合）的试点研究中，通过因子化决策头（工具&#x2F;风格）进行的离线策略评估（OPE）显示，一种统一使用大量工具的策略虽然能提升日志中的平均价值，但却会对特定子群体造成负面影响，尤其是健康素养较低但自我效能感较高的用户。进一步使用一种轻量级模拟器并引入隐藏的原型模型后发现，添加一个早期信息增益奖励可以可靠地缩短特征识别时间，并提升目标达成率和pass@3指标。综上，这些早期研究结果表明，个性化应首先以评估为导向：冻结生成器，基于类型化奖励（目标工具结果和满意度）学习子群体感知的决策头，并始终报告基于原型的指标，以揭示平均值所掩盖的子群体负面影响。</p>
<h3 id="From-Retrieval-to-Generation-Unifying-External-and-Parametric-Knowledge-for-Medical-Question-Answering"><a href="#From-Retrieval-to-Generation-Unifying-External-and-Parametric-Knowledge-for-Medical-Question-Answering" class="headerlink" title="From Retrieval to Generation: Unifying External and Parametric Knowledge for Medical Question Answering"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18297v1">From Retrieval to Generation: Unifying External and Parametric Knowledge for Medical Question Answering</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>医疗问答（QA）需要广泛访问领域专业知识。一种有前景的方向是通过从医学语料库中检索外部知识或从模型参数中存储的参数化知识来增强大型语言模型（LLMs）。现有方法通常可分为两类：检索增强生成（RAG），其通过外部检索的证据来支撑模型推理；以及生成增强生成（GAG），其仅依赖模型内部知识来生成上下文文档。然而，RAG常常面临检索结果噪声大或不完整的问题，而GAG则由于不受限制的生成容易产生幻觉或不准确的信息。这两个问题都可能误导推理并损害答案的可靠性。为了解决这些挑战，我们提出了MedRGAG，一个统一的检索-生成增强框架，无缝整合外部知识和参数化知识，用于医学问答。MedRGAG包含两个关键模块：知识引导的上下文补全（KGCC），其指导生成器生成补充检索所揭示缺失知识的背景文档；以及知识感知的文档选择（KADS），其自适应地选择检索和生成文档的最优组合，以形成简洁而全面的证据，用于答案生成。在五个医学问答基准测试中进行的大量实验表明，MedRGAG在MedRAG的基础上提升了12.5%，在MedGENIE的基础上提升了4.5%，突显了统一检索和生成对于知识密集型推理的有效性。我们的代码和数据已公开，可访问：<a target="_blank" rel="noopener" href="https://anonymous.4open.science/r/MedRGAG">https://anonymous.4open.science/r/MedRGAG</a></p>
<h3 id="Secure-and-Efficient-Access-Control-for-Computer-Use-Agents-via-Context-Space"><a href="#Secure-and-Efficient-Access-Control-for-Computer-Use-Agents-via-Context-Space" class="headerlink" title="Secure and Efficient Access Control for Computer-Use Agents via Context Space"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2509.22256v2">Secure and Efficient Access Control for Computer-Use Agents via Context Space</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>基于大型语言模型（LLM）的计算机使用代理代表了人工智能与操作系统能力的融合，使自然语言能够控制系统和应用程序级别的功能。然而，由于LLM本身存在固有的不确定性问题，赋予代理对计算机的控制权会带来显著的安全风险。当代理的行为偏离用户意图时，可能会造成不可逆的后果。现有的缓解措施，如用户确认和基于LLM的动态操作验证，仍存在可用性、安全性和性能方面的局限性。为了解决这些问题，我们提出了CSAgent，这是一个面向计算机使用代理的系统级、基于静态策略的访问控制框架。为了弥合静态策略与动态上下文及用户意图之间的差距，CSAgent引入了意图和上下文感知的策略，并提供了一个自动化工具链，帮助开发者构建和优化这些策略。CSAgent通过优化的操作系统服务强制执行这些策略，确保代理操作只能在特定用户意图和上下文中执行。CSAgent支持通过多种接口（包括API、CLI和GUI）控制计算机的代理的安全防护。我们实现了并评估了CSAgent，其成功抵御了超过99.36%的攻击，同时仅引入了6.83%的性能开销。</p>
<h3 id="FinAI-Data-Assistant-LLM-based-Financial-Database-Query-Processing-with-the-OpenAI-Function-Calling-API"><a href="#FinAI-Data-Assistant-LLM-based-Financial-Database-Query-Processing-with-the-OpenAI-Function-Calling-API" class="headerlink" title="FinAI Data Assistant: LLM-based Financial Database Query Processing with the OpenAI Function Calling API"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.14162v2">FinAI Data Assistant: LLM-based Financial Database Query Processing with the OpenAI Function Calling API</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>我们介绍了FinAI Data Assistant，这是一种结合大型语言模型（LLMs）与OpenAI函数调用API的实用方法，用于对金融数据库进行自然语言查询。与通过文本到SQL（text-to-SQL）合成完整SQL语句不同，我们的系统将用户请求路由至一组经过验证的、参数化的查询库，以生成性灵活性换取可靠性、低延迟和成本效率。我们通过实证研究探讨了三个问题：（RQ1）仅依靠LLMs是否能够在不依赖外部检索的情况下可靠地回忆或外推时间相关的金融数据；（RQ2）LLMs在将公司名称映射到股票代码方面表现如何；（RQ3）函数调用是否在端到端数据库查询处理中优于文本到SQL方法。在对价格和基本面数据进行控制实验时，仅使用LLMs的预测结果显示了非可忽略的误差，并且相对于模型知识截止点，主要在股票价格上表现出前瞻性偏差。股票代码映射的准确性对于纳斯达克100指数成分股接近完美，对于标普500指数公司则很高。最后，FinAI Data Assistant在我们的任务集上相比文本到SQL基线方法实现了更低的延迟和成本，以及更高的可靠性。我们讨论了设计权衡、局限性以及部署的潜在途径。</p>
<h3 id="A-2-FM-An-Adaptive-Agent-Foundation-Model-for-Tool-Aware-Hybrid-Reasoning"><a href="#A-2-FM-An-Adaptive-Agent-Foundation-Model-for-Tool-Aware-Hybrid-Reasoning" class="headerlink" title="A$^2$FM: An Adaptive Agent Foundation Model for Tool-Aware Hybrid Reasoning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.12838v3">A$^2$FM: An Adaptive Agent Foundation Model for Tool-Aware Hybrid Reasoning</a></h3><p><strong>Categories:</strong> Large Language Models, Reinforcement Learning</p>
<p>大型语言模型可以分为两个家族：以推理为中心的LLM（reasoning-centric LLM），它们增强了内部的思维链推理能力，但无法调用外部工具；以及智能体型LLM（agentic LLM），它们学习与环境进行交互并利用工具，但往往在深度推理方面存在不足。这种差异源于根本不同的训练目标，导致在简单查询任务上，两种模型的性能不匹配且效率低下，通常都会过度思考或过度调用工具。在本工作中，我们提出了一个统一的框架——自适应智能体基础模型（Adaptive Agent Foundation Model，A$^2$FM），其遵循“路径优先，然后对齐”的原则：模型首先学习任务感知的路由机制，然后在共享主干网络下对特定模式的轨迹进行对齐。为了解决效率差距，我们引入了第三种模式——直接处理简单查询的模式，从而避免不必要的推理或工具调用，同时补充智能体模式和推理模式的功能。为了同时提升准确性和效率，我们提出了自适应策略优化（Adaptive Policy Optimization，APO），它在不同模式之间执行自适应采样，并应用了成本正则化的奖励机制。在320亿参数规模下，A$^2$FM在BrowseComp上达到了13.4%，在AIME25上达到了70.4%，在HLE上达到了16.7%，在同类模型中创造了新的SOTA（最先进水平），并在智能体、推理和通用基准测试中与前沿LLM表现相当。值得注意的是，自适应执行的每正确答案成本仅为0.00487美元，相比推理模式降低了45.2%，相比智能体模式降低了33.5%，从而实现了显著更高的成本效率，同时保持了相当的准确性。</p>
<h3 id="Learning-from-Mistakes-Enhancing-Harmful-Meme-Detection-via-Misjudgment-Risk-Patterns"><a href="#Learning-from-Mistakes-Enhancing-Harmful-Meme-Detection-via-Misjudgment-Risk-Patterns" class="headerlink" title="Learning from Mistakes: Enhancing Harmful Meme Detection via Misjudgment Risk Patterns"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.15946v2">Learning from Mistakes: Enhancing Harmful Meme Detection via Misjudgment Risk Patterns</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>网络迷因已成为一种流行的多模态媒介，但它们正越来越多地被武器化，通过诸如讽刺和隐喻等微妙的修辞手法传播有害观点。现有的检测方法，包括基于大规模语言模型（MLLM）的技术，难以处理这些隐含表达，导致频繁的误判。本文介绍了PatMD，这是一种新颖的方法，通过学习并主动缓解这些潜在误判风险，从而改进有害迷因的检测。我们的核心思想是超越表面内容匹配，转而识别潜在误判风险的模式，并主动引导MLLM避免已知的误判陷阱。我们首先构建了一个知识库，其中每个迷因都被分解为一个误判风险模式，解释了为何它可能被误判，要么是忽略了有害的隐含意义（假阴性），要么是过度解读了中性内容（假阳性）。对于给定的目标迷因，PatMD检索相关的模式，并利用这些模式动态引导MLLM的推理过程。在涵盖5个有害内容检测任务、共计6,626个迷因的基准测试中，实验结果表明，PatMD优于最先进的基线方法，F1分数平均提高了8.30%，准确率提高了7.71%，展示了其强大的泛化能力和对有害迷因检测能力的提升。</p>
<h3 id="Learning-from-the-Best-Differently-A-Diversity-Driven-Rethinking-on-Data-Selection"><a href="#Learning-from-the-Best-Differently-A-Diversity-Driven-Rethinking-on-Data-Selection" class="headerlink" title="Learning from the Best, Differently: A Diversity-Driven Rethinking on Data Selection"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18909v1">Learning from the Best, Differently: A Diversity-Driven Rethinking on Data Selection</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>高质量的预训练数据对于大型语言模型至关重要，其中质量体现在事实的可靠性与语义价值上，而多样性则确保了广泛覆盖和分布的异质性。现有的方法通常依赖于基于单维或多维评分的选择策略。然而，直接选择评分最高的数据往往会降低性能，需要从更广泛的范围内进行采样以恢复性能。上述数据集评分与下游基准结果之间的非单调关系揭示了一个根本性的偏差：基于评分的方法会压缩相关维度，导致高评分数据看似质量高，但却系统性地忽略了多样性。我们认为，确保多样性需要将相关指标分解为正交的特征维度，从而可以直接从这些维度中选择高评分数据。因此，我们提出了正交多样性感知选择（ODiS）算法，在数据选择过程中同时保留质量和多样性。首先，ODiS从多个维度评估数据，涵盖语言质量、知识质量和理解难度。然后，通过主成分分析（PCA）对多维评分进行去相关处理，得到正交的评估维度。对于每个维度，训练一个基于Roberta的评分模型，将数据回归到PCA投影后的评分上，从而实现对大规模语料库的可扩展推理。最后，ODiS通过在每个正交维度中选择高评分数据来构建训练数据集，从而确保质量和多样性的双重兼顾。实验结果表明，ODiS选择的数据在不同维度之间的重叠率低于2%，证实了各维度之间的正交性。更重要的是，使用ODiS选择的数据训练的模型在下游基准测试中显著优于其他基线方法，突显了对大型语言模型（LLMs）进行正交、多样性感知数据选择的必要性。</p>
<h3 id="Improving-Topic-Modeling-of-Social-Media-Short-Texts-with-Rephrasing-A-Case-Study-of-COVID-19-Related-Tweets"><a href="#Improving-Topic-Modeling-of-Social-Media-Short-Texts-with-Rephrasing-A-Case-Study-of-COVID-19-Related-Tweets" class="headerlink" title="Improving Topic Modeling of Social Media Short Texts with Rephrasing: A Case Study of COVID-19 Related Tweets"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18908v1">Improving Topic Modeling of Social Media Short Texts with Rephrasing: A Case Study of COVID-19 Related Tweets</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>社交媒体平台（如Twitter，现为X）为分析公众舆论提供了丰富的数据，尤其是在像新冠（COVID-19）大流行这样的危机期间。然而，社交媒体短文本的简短性、非正式性和噪声常常阻碍传统主题建模的有效性，导致产生不连贯或冗余的主题，这些主题通常难以解读。为了解决这些挑战，我们开发了TM-Rephrase，这是一种模型无关的框架，利用大型语言模型（LLMs）将原始推文重新表述为更加标准化和正式的语言，然后再进行主题建模。我们使用包含25,027条与新冠相关的Twitter推文的数据集，研究了两种重述策略（通用到正式重述和口语化到正式重述）对多种主题建模方法的影响。结果表明，TM-Rephrase能够提升三个衡量主题建模性能的指标（即主题连贯性、主题独特性和主题多样性），同时减少大多数主题建模算法的主题冗余，其中口语化到正式重述策略带来的性能提升最为显著，特别是在潜在狄利克雷分布（LDA）算法中。本研究为提升公共卫生相关社交媒体分析中的主题建模提供了一种模型无关的方法，对改善健康危机中公众舆论的理解以及其他重要领域具有广泛的启示意义。</p>
<h3 id="DelvePO-Direction-Guided-Self-Evolving-Framework-for-Flexible-Prompt-Optimization"><a href="#DelvePO-Direction-Guided-Self-Evolving-Framework-for-Flexible-Prompt-Optimization" class="headerlink" title="DelvePO: Direction-Guided Self-Evolving Framework for Flexible Prompt Optimization"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18257v1">DelvePO: Direction-Guided Self-Evolving Framework for Flexible Prompt Optimization</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>由于提示优化在引导大型语言模型解决各种任务方面的强大能力，它已成为一种关键的方法。然而，当前的研究主要依赖于大型语言模型（LLMs）的随机重写能力，且优化过程通常聚焦于特定的影响因素，这使得容易陷入局部最优解。此外，优化后的提示性能往往不稳定，这限制了其在不同任务中的迁移能力。为了解决上述挑战，我们提出了 $\textbf{DelvePO}$（$\textbf{D}$irection-Guid$\textbf{e}$d Se$\textbf{l}$f-E$\textbf{v}$olving Framework for Fl$\textbf{e}$xible $\textbf{P}$rompt $\textbf{O}$ptimization），这是一种任务无关的框架，以自我演进的方式优化提示。在我们的框架中，我们将提示分解为不同的组件，这些组件可用于探索不同因素对各种任务可能产生的影响。在此基础上，我们引入了工作记忆，通过它，LLMs可以缓解自身不确定性带来的缺陷，并进一步获得关键见解以指导新提示的生成。我们在多个涵盖不同领域的任务上进行了广泛的实验，包括开源和闭源的大语言模型，如 DeepSeek-R1-Distill-Llama-8B、Qwen2.5-7B-Instruct 和 GPT-4o-mini。实验结果表明，在相同的实验设置下，DelvePO 一致优于之前最先进的方法，证明了其在不同任务中的有效性与迁移能力。</p>
<h3 id="AI-Agentic-Vulnerability-Injection-And-Transformation-with-Optimized-Reasoning"><a href="#AI-Agentic-Vulnerability-Injection-And-Transformation-with-Optimized-Reasoning" class="headerlink" title="AI Agentic Vulnerability Injection And Transformation with Optimized Reasoning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.20866v2">AI Agentic Vulnerability Injection And Transformation with Optimized Reasoning</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>随着软件系统的复杂性不断增加以及网络攻击的日益复杂化，有效自动漏洞检测与修复系统变得尤为重要。基于数据驱动的方法，如深度学习模型，展现出巨大的潜力，但其关键依赖于大规模、准确标注的数据集。然而，现有的数据集要么存在标签噪声，要么漏洞类型覆盖有限，或者无法真实反映实际软件中的漏洞情况。这也限制了对这类解决方案的大规模基准测试。自动漏洞注入为直接解决这些数据集局限性提供了途径，但现有的技术手段在覆盖范围、上下文保真度或注入成功率方面仍存在局限。在本文中，我们提出了AVIATOR，这是首个基于人工智能代理的漏洞注入流程。它能够自动注入真实且具有类别特性的漏洞，从而实现高保真、多样化、大规模的漏洞数据集生成。与以往的单体方法不同，AVIATOR通过协调专门的AI代理、功能代理和传统的代码分析工具，模拟专家推理过程。该流程结合了语义分析、基于LoRA微调的注入合成以及检索增强生成（Retrieval-Augmented Generation），并通过静态分析和基于大语言模型（LLM）的判别器进行注入后验证。这种模块化分解使专门的代理能够专注于不同的任务，从而提高注入的鲁棒性并减少流程中错误的传播。在三个不同的基准测试中进行的评估表明，AVIATOR的注入成功率达到了91%至95%，在准确性和软件漏洞的覆盖范围方面显著超越了现有的自动数据集生成技术。</p>
<h3 id="ssToken-Self-modulated-and-Semantic-aware-Token-Selection-for-LLM-Fine-tuning"><a href="#ssToken-Self-modulated-and-Semantic-aware-Token-Selection-for-LLM-Fine-tuning" class="headerlink" title="ssToken: Self-modulated and Semantic-aware Token Selection for LLM Fine-tuning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18250v1">ssToken: Self-modulated and Semantic-aware Token Selection for LLM Fine-tuning</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>数据质量在提升大语言模型（LLMs）的监督微调（SFT）中起着关键作用，而基于标记级别的数据选择因其细粒度特性，已成为一个有前景的研究方向。尽管现有的基于标记级别的选择方法在实证性能上表现出色，但它们存在两个关键的局限性：（1）需要训练或访问额外的参考模型；（2）仅依赖损失信息进行标记选择，无法很好地保留那些在基于损失的指标下不受青睐但语义上重要的标记。为了解决这些挑战，我们提出了ssToken，一种自调节且语义感知的标记选择方法。ssToken利用易于获取的历史模型，计算当前模型与历史模型之间每个标记的损失差异，这种差异作为自调节信号，使模型能够沿着其优化轨迹自适应地选择标记，而不是像以往方法那样依赖于离线训练的参考模型的大量损失。我们进一步引入了一种基于注意力的语义感知标记重要性评估指标，该指标与基于损失的选择方法正交，并提供互补的语义信息，从而实现更有效的过滤。在不同模型家族和规模上的大量实验表明，仅使用自调节选择或语义感知选择的方法均优于全数据微调，而它们的结合——即ssToken——能够实现协同增益，并进一步超越现有的基于标记级别的选择方法，在保持训练效率的同时实现性能提升。</p>
<h3 id="Scaling-Laws-Meet-Model-Architecture-Toward-Inference-Efficient-LLMs"><a href="#Scaling-Laws-Meet-Model-Architecture-Toward-Inference-Efficient-LLMs" class="headerlink" title="Scaling Laws Meet Model Architecture: Toward Inference-Efficient LLMs"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18245v1">Scaling Laws Meet Model Architecture: Toward Inference-Efficient LLMs</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>增加参数数量和训练数据规模已被证明是提升大语言模型（LLM）性能的有效策略。然而，随着这些模型变得越来越强大并被广泛应用，推理成本已成为一个紧迫的问题。尽管其重要性不言而喻，但模型精度与推理效率之间的权衡仍缺乏深入研究。在本工作中，我们探讨了几个关键的架构因素——隐藏层大小、MLP和注意力机制之间参数的分配比例（mlp-to-attention ratio）以及分组查询注意力（GQA）——如何影响推理成本和模型精度。我们引入了一种条件扩展定律，将架构信息整合到Chinchilla框架中，并提出了一种用于识别同时具备高推理效率和高精度的架构的搜索框架。为了验证我们的方法，我们训练了超过200个模型，参数量从80M到3B不等，训练数据量从8B到100B tokens不等，并拟合了所提出的条件扩展定律。我们的结果表明，该条件扩展定律能够可靠地预测最佳的架构选择，且由此得到的模型在现有开源基线模型上表现更优。在相同的训练预算下，优化后的架构模型的精度比LLaMA-3.2高出最高2.1%，推理吞吐量则提高了42%。</p>
<h3 id="Ontology-Enhanced-Knowledge-Graph-Completion-using-Large-Language-Models"><a href="#Ontology-Enhanced-Knowledge-Graph-Completion-using-Large-Language-Models" class="headerlink" title="Ontology-Enhanced Knowledge Graph Completion using Large Language Models"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2507.20643v2">Ontology-Enhanced Knowledge Graph Completion using Large Language Models</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>大语言模型（LLMs）在知识图谱补全（KGC）中得到了广泛应用，展示了显著的研究进展。然而，作为由深度神经网络架构驱动的黑箱模型，当前基于LLM的KGC方法依赖于隐式的知识表示，并通过并行传播错误知识，从而阻碍了它们产生明确且决定性的推理结果。我们旨在将神经感知结构信息与本体知识相结合，利用LLM的强大能力，以更深入地理解知识的内在逻辑。我们提出了一种基于LLM的本体增强型KGC方法——OL-KGC。该方法首先利用神经感知机制，将结构信息有效嵌入到文本空间中，然后使用自动化提取算法从需要补全的知识图谱（KGs）中检索本体知识，并进一步将其转换为LLM可理解的文本格式，以提供逻辑指导。我们在三个广泛使用的基准数据集——FB15K-237、UMLS和WN18RR上进行了大量实验。实验结果表明，OL-KGC在多个评估指标上显著优于现有的主流KGC方法，取得了最先进的性能。</p>
<h3 id="Noise-Robustness-Through-Noise-A-Framework-combining-Asymmetric-LoRA-with-Poisoning-MoE"><a href="#Noise-Robustness-Through-Noise-A-Framework-combining-Asymmetric-LoRA-with-Poisoning-MoE" class="headerlink" title="Noise-Robustness Through Noise: A Framework combining Asymmetric LoRA with Poisoning MoE"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.23868v5">Noise-Robustness Through Noise: A Framework combining Asymmetric LoRA with Poisoning MoE</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>当前用于将预训练语言模型适配到下游任务的参数高效微调方法容易受到噪声数据的干扰。传统的噪声处理方法要么依赖繁琐的数据预处理，要么采用容易导致误差累积的模型架构修改。与现有噪声处理范式不同，我们提出了一种基于非对称LoRA中毒专家（LoPE）的噪声鲁棒适应方法，这是一种新颖的框架，仅通过生成的噪声数据来增强模型对噪声的鲁棒性。受专家混合架构的启发，LoPE在非对称LoRA配置中战略性地引入了一个专用的中毒专家。通过两阶段范式，LoPE在微调过程中对中毒专家进行噪声注入，以增强其噪声识别和处理能力。在推理阶段，我们选择性地屏蔽专用的中毒专家，利用正常专家获取的纯净知识，以实现噪声鲁棒的输出。大量实验表明，LoPE仅通过低成本的噪声注入即可实现强大的性能和鲁棒性，完全消除了数据清洗的需求。</p>
<h3 id="BlockScan-Detecting-Anomalies-in-Blockchain-Transactions"><a href="#BlockScan-Detecting-Anomalies-in-Blockchain-Transactions" class="headerlink" title="BlockScan: Detecting Anomalies in Blockchain Transactions"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2410.04039v5">BlockScan: Detecting Anomalies in Blockchain Transactions</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>我们提出了一种名为BlockScan的定制化Transformer模型，用于检测区块链交易中的异常行为。与现有依赖基于规则的系统或直接使用现成大型语言模型（LLMs）的方法不同，BlockScan引入了一系列定制化设计，以有效建模区块链交易的独特数据结构。首先，区块链交易是多模态的，包含区块链专用的标记、文本和数值。我们设计了一种新颖的模块化分词器来处理这些多模态输入，并在不同模态之间平衡信息。其次，我们设计了一种定制化的掩码语言建模机制，用于预训练Transformer架构，结合RoPE嵌入和FlashAttention以处理更长的序列。最后，我们设计了一种基于模型输出的新型异常检测方法。我们还对系统检测方法进行了理论分析。在以太坊和Solana交易上的大量实验评估表明，BlockScan在保持较低误报率的同时，展现出卓越的异常检测能力。值得注意的是，BlockScan是唯一能够以高准确率检测Solana上异常交易的方法，而其他所有方法的检测召回率都极低甚至为零。这项工作为在区块链数据分析中应用基于Transformer的方法设定了新的基准。</p>
<h3 id="GPO-Learning-from-Critical-Steps-to-Improve-LLM-Reasoning"><a href="#GPO-Learning-from-Critical-Steps-to-Improve-LLM-Reasoning" class="headerlink" title="GPO: Learning from Critical Steps to Improve LLM Reasoning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2509.16456v2">GPO: Learning from Critical Steps to Improve LLM Reasoning</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>大语言模型（LLMs）在各种领域中的应用日益增多，在不同任务中展现出令人印象深刻的潜力。最近，研究人员提出了推理型大语言模型（reasoning LLMs），以增强LLMs的推理或思考能力，从而解决复杂问题。尽管推理型LLMs取得了有希望的成果，但增强LLMs的多步推理能力仍然是一个重大挑战。虽然现有的优化方法已经提升了LLM的推理能力，但它们通常将推理轨迹视为整体，而没有考虑到轨迹内部的关键步骤。在本文中，我们引入了一种新的微调策略——<strong>引导关键点优化</strong>（Guided Pivotal Optimization, GPO），该策略深入推理过程，以实现更有效的改进。GPO首先识别推理轨迹中的“关键步骤”——这是模型必须仔细处理以成功解决问题的点。我们通过估计优势函数来定位关键步骤。接着，GPO将策略重置到关键步骤，采样新的轨迹，并优先在这些轨迹上进行学习过程。这种关注使模型能够更有效地从推理过程中的关键时刻学习，从而提高推理性能。我们证明，GPO是一种通用策略，可以与各种优化方法结合，以提升推理性能。除了理论分析之外，我们在具有挑战性的推理基准测试中进行的实验表明，GPO能够持续且显著地增强现有优化方法的性能，展示了其在提升LLM推理能力方面的有效性与通用性，这体现在其对生成过程中关键时刻的聚焦上。</p>
<h3 id="3D-Optimization-for-AI-Inference-Scaling-Balancing-Accuracy-Cost-and-Latency"><a href="#3D-Optimization-for-AI-Inference-Scaling-Balancing-Accuracy-Cost-and-Latency" class="headerlink" title="3D Optimization for AI Inference Scaling: Balancing Accuracy, Cost, and Latency"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18905v1">3D Optimization for AI Inference Scaling: Balancing Accuracy, Cost, and Latency</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>人工智能推理扩展通常通过一维启发式方法（固定推理次数）或二维双变量权衡（例如性能与计算量）进行调整，这些方法未能考虑成本和延迟约束。我们引入了一个三维优化框架，在统一的决策空间内联合校准精度、成本和延迟，从而实现面向约束的推理扩展。通过在三个代表性场景和九个模拟的大语言模型上进行蒙特卡洛模拟，我们评估了四种优化方法以解决三维多目标优化（MOO）问题。将推理扩展建模为MOO问题，构建了一个1D和2D优化方法无法捕捉的可行空间，从而实现环境自适应的推理扩展因子k的选择。结果表明，拐点优化方法在平衡各方面性能方面表现最佳，而当优先考虑精度时，精度最大化方法依然具有优势。该框架为在多种运行环境中实现面向部署的推理扩展提供了理论基础。</p>
<h3 id="Contrastive-Decoding-Mitigates-Score-Range-Bias-in-LLM-as-a-Judge"><a href="#Contrastive-Decoding-Mitigates-Score-Range-Bias-in-LLM-as-a-Judge" class="headerlink" title="Contrastive Decoding Mitigates Score Range Bias in LLM-as-a-Judge"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18196v1">Contrastive Decoding Mitigates Score Range Bias in LLM-as-a-Judge</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>大语言模型（LLMs）常被用作各种应用中的评估者，但评估结果的可靠性仍然是一个挑战。其中一项挑战是使用LLMs作为评判者进行直接评估，即在没有参考的情况下，根据指定的范围进行打分。我们首先表明，这一挑战源于LLM评判者的输出与评分范围偏差有关，也就是说，LLM评判者的输出对预定义的评分范围非常敏感，从而阻碍了寻找最佳评分范围的探索。我们还表明，同一家族中的模型也存在类似的偏差。随后，我们通过对比解码的方法来缓解这种偏差，在不同评分范围内，Spearman相关性与人类判断相比平均实现了11.3%的相对提升。</p>
<h3 id="ActivationReasoning-Logical-Reasoning-in-Latent-Activation-Spaces"><a href="#ActivationReasoning-Logical-Reasoning-in-Latent-Activation-Spaces" class="headerlink" title="ActivationReasoning: Logical Reasoning in Latent Activation Spaces"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18184v1">ActivationReasoning: Logical Reasoning in Latent Activation Spaces</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>大语言模型（LLMs）在生成流畅文本方面表现出色，但其内部推理过程仍然不透明且难以控制。稀疏自动编码器（SAEs）通过暴露通常与人类概念对齐的潜在特征，使隐藏激活更加可解释。然而，这些特征具有脆弱性和被动性，无法提供系统推理或模型控制的机制。为了解决这个问题，我们引入了ActivationReasoning（AR），这是一种将显式逻辑推理嵌入到LLM潜在空间中的框架。该框架分为三个阶段：（1）寻找潜在表示，首先识别潜在概念表示（例如，通过SAEs）并将它们组织成一个字典；（2）激活命题，在推理过程中，AR检测激活的概念并将它们映射为逻辑命题；（3）逻辑推理，通过对这些命题应用逻辑规则，推断出高阶结构、组合新概念并引导模型行为。我们在多个任务上评估AR，包括多步推理（PrOntoQA）、抽象能力和对间接概念提示的鲁棒性（Rail2Country）、在自然和多样语言上的推理（ProverQA），以及上下文敏感的安全性（BeaverTails）。在所有任务中，AR随着推理复杂性的增加而稳健地扩展，能够推广到抽象和上下文敏感的任务，并在不同的模型架构之间进行迁移。这些结果表明，将逻辑结构根植于潜在激活不仅提高了透明度，还使结构化推理、可靠的控制和与期望行为的对齐成为可能，为更加可靠和可审计的AI提供了一条路径。</p>
<h3 id="DuoLens-A-Framework-for-Robust-Detection-of-Machine-Generated-Multilingual-Text-and-Code"><a href="#DuoLens-A-Framework-for-Robust-Detection-of-Machine-Generated-Multilingual-Text-and-Code" class="headerlink" title="DuoLens: A Framework for Robust Detection of Machine-Generated Multilingual Text and Code"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18904v1">DuoLens: A Framework for Robust Detection of Machine-Generated Multilingual Text and Code</a></h3><p><strong>Categories:</strong> Large Language Models</p>
<p>大型语言模型（LLMs）在生成多语言文本和源代码方面的普及，进一步加剧了对机器生成内容检测器在跨领域中准确性和效率的迫切需求。当前的检测器主要采用零样本方法（如Fast DetectGPT或GPTZero），但要么计算成本高昂，要么准确率不足，通常两者之间存在权衡，仍有改进空间。为解决这些问题，我们提出对仅编码器的小型语言模型（SLMs）进行微调，特别是使用针对源代码和其他自然语言的专用数据集对RoBERTA和CodeBERTa等预训练模型进行训练，以证明在二分类任务中，SLMs在计算资源消耗仅为LLMs的极小部分的情况下，表现远超LLMs。我们的编码器在512个标记输入下，实现了AUROC&#x3D;0.97至0.99，macro-F1&#x3D;0.89至0.94，同时将延迟降低了8至12倍，峰值VRAM降低了3至5倍。在跨生成器偏移和对抗性变换（如改写、反向翻译；代码格式化&#x2F;重命名）的情况下，性能仍能保持干净数据的AUROC的92%以上。我们发布了训练和评估脚本，包含种子和配置；还附有可复现性检查清单。</p>
<h2 id="Reinforcement-Learning"><a href="#Reinforcement-Learning" class="headerlink" title="Reinforcement Learning"></a>Reinforcement Learning</h2><h3 id="Improving-planning-and-MBRL-with-temporally-extended-actions"><a href="#Improving-planning-and-MBRL-with-temporally-extended-actions" class="headerlink" title="Improving planning and MBRL with temporally-extended actions"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.15754v2">Improving planning and MBRL with temporally-extended actions</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>连续时间系统通常使用离散时间动态模型进行建模，但为了保持精度，这需要较小的仿真步长。反过来，这又要求较长的规划时间范围，导致计算量大且性能下降。以往的无模型强化学习（model-free reinforcement learning）工作通过动作重复（action repeats）部分解决了这一问题，即通过学习策略来决定离散动作的持续时间。相反，我们提出直接控制连续决策的时间尺度，使用时间扩展动作（temporally-extended actions），并让规划器将动作的持续时间作为标准动作变量之外的额外优化变量来处理。这种额外的结构具有多重优势。它加快了轨迹的仿真时间，更重要的是，它允许在基本动作层面进行深度时间范围搜索，而规划器仅需进行较浅的搜索深度。此外，在基于模型的强化学习（MBRL）设置中，它减少了模型学习中的误差累积，提高了模型的训练效率。我们证明了这一方法的有效性，并且可以使用多臂老虎机（multi-armed bandit）形式化方法自动选择动作持续时间的范围，并将其集成到MBRL框架中。在规划和MBRL方面的广泛实验评估表明，我们的方法能够实现更快的规划、更好的解决方案，并且可以解决标准形式下无法解决的问题。</p>
<h3 id="Rectifying-Shortcut-Behaviors-in-Preference-based-Reward-Learning"><a href="#Rectifying-Shortcut-Behaviors-in-Preference-based-Reward-Learning" class="headerlink" title="Rectifying Shortcut Behaviors in Preference-based Reward Learning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19050v1">Rectifying Shortcut Behaviors in Preference-based Reward Learning</a></h3><p><strong>Categories:</strong> Reinforcement Learning, Large Language Models</p>
<p>在基于人类反馈的强化学习中，以偏好为基础的奖励模型在将大型语言模型对齐到人类一致行为方面起着核心作用。然而，近期研究表明，这些模型容易出现奖励黑客行为（reward hacking），并且由于过度优化，常常难以很好地泛化。它们通过利用捷径（即与训练数据中人类偏好标签相关联的虚假特征，例如响应的冗长性、讨好性的语气或阿谀奉承的表达）来获得高奖励分数，而不是真正反映预期的目标。在本文中，我们并未逐一探讨这些问题，而是从更广泛的角度看待奖励黑客问题，将其视为捷径行为，并提出一种具有原则性且灵活的方法，以减轻基于偏好的奖励学习中的捷径行为。受核视角下不变性理论的启发，我们提出了基于偏好的捷径抑制方法（Preference-based Reward Invariance for Shortcut Mitigation，简称PRISM），该方法在一个闭合形式的学习目标下，学习具有群不变性的核函数及其特征映射。在多个基准测试中的实验结果表明，我们的方法在多种分布外任务中持续提高了奖励模型的准确性，并减少了下游策略模型对捷径的依赖，从而建立了一个稳健的基于偏好的对齐框架。</p>
<h3 id="REPAIR-Approach-for-Social-based-City-Reconstruction-Planning-in-case-of-natural-disasters"><a href="#REPAIR-Approach-for-Social-based-City-Reconstruction-Planning-in-case-of-natural-disasters" class="headerlink" title="REPAIR Approach for Social-based City Reconstruction Planning in case of natural disasters"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19048v1">REPAIR Approach for Social-based City Reconstruction Planning in case of natural disasters</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>自然灾害对人类生活总是产生多方面的影响。对于政府来说，应对这些突发事件并利用现有资源（主要是预算和时间）重建经济、社会和物理基础设施与设施是一项具有挑战性的任务。政府总是根据法律和政治策略来制定计划和政策，以最大化社会效益。灾害的严重程度以及恢复生活常态所需的巨大资源，使得此类重建工作面临巨大挑战。本文是在我们之前发表的研究基础上进行的扩展，通过整合额外的深度学习模型以及随机智能体（作为基准模型）进行全面的比较分析。我们之前的研究引入了一种决策支持系统，采用深度强化学习技术，用于灾后城市重建的规划，旨在最大化重建过程中的社会效益，同时考虑可用资源，满足广大社区利益相关者（如市民的社会效益和政治家的优先事项）的需求，并考虑城市结构的约束条件（如道路和建筑之间的相互依赖关系）。所提出的这种方法名为“灾后重建规划提供者”（REPAIR），具有通用性。它能够为地方管理者提供一组替代性计划，由其选择最合适的方案实施，而且可以应用于任何规模的地区。我们展示了REPAIR在实际案例中的应用，即用于2009年发生严重地震后意大利拉奎拉市的重建过程。</p>
<h3 id="FP-IRL-Fokker-Planck-Inverse-Reinforcement-Learning-–-A-Physics-Constrained-Approach-to-Markov-Decision-Processes"><a href="#FP-IRL-Fokker-Planck-Inverse-Reinforcement-Learning-–-A-Physics-Constrained-Approach-to-Markov-Decision-Processes" class="headerlink" title="FP-IRL: Fokker-Planck Inverse Reinforcement Learning – A Physics-Constrained Approach to Markov Decision Processes"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2306.10407v2">FP-IRL: Fokker-Planck Inverse Reinforcement Learning – A Physics-Constrained Approach to Markov Decision Processes</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>逆强化学习（Inverse Reinforcement Learning, IRL）是一种强大的范式，通过从马尔可夫决策过程（Markov Decision Process, MDP）中观察到的轨迹中推断出未知的奖励函数，从而揭示驱动智能体行为的激励结构。然而，大多数现有的IRL方法都需要访问转移函数，无论是预先规定还是事先估计的，这在系统底层动力学未知、不可观测或难以采样的情况下带来了显著挑战。</p>
<p>我们提出了一种新的物理约束逆强化学习框架——福克-普朗克逆强化学习（Fokker-Planck Inverse Reinforcement Learning, FP-IRL），专门针对由福克-普朗克（Fokker-Planck, FP）动力学所支配的系统。FP-IRL能够直接从轨迹数据中同时推断出奖励函数和转移函数，而无需访问采样的转移数据。我们的方法利用了一个关于MDP与福克-普朗克方程之间等价性的假设，将MDP中的奖励最大化与FP动力学中的自由能最小化联系起来。这种联系使得我们能够通过变分系统辨识方法推断势函数，从而利用解析表达式恢复MDP的全部组成部分——奖励、转移和策略。</p>
<p>我们通过在合成基准和修改后的山车问题（Mountain Car）上的实验，展示了FP-IRL的有效性。实验结果表明，FP-IRL能够在保持计算效率和物理可解释性的同时，准确地恢复智能体的激励结构。</p>
<h3 id="SPiDR-A-Simple-Approach-for-Zero-Shot-Safety-in-Sim-to-Real-Transfer"><a href="#SPiDR-A-Simple-Approach-for-Zero-Shot-Safety-in-Sim-to-Real-Transfer" class="headerlink" title="SPiDR: A Simple Approach for Zero-Shot Safety in Sim-to-Real Transfer"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2509.18648v4">SPiDR: A Simple Approach for Zero-Shot Safety in Sim-to-Real Transfer</a></h3><p><strong>Categories:</strong> Reinforcement Learning, Robotics</p>
<p>在现实世界中安全地部署强化学习（RL）具有挑战性，因为模拟器中训练的策略必须面对不可避免的模拟到现实（sim-to-real）差距。鲁棒安全的RL技术理论上是安全的，但难以扩展；而领域随机化（domain randomization）虽然更具实用性，但容易导致不安全行为。为了解决这一问题，我们提出了SPiDR（通过悲观领域随机化实现模拟到现实的迁移，即Sim-to-real via Pessimistic Domain Randomization）——一种具有可证明安全保证、可扩展的算法，能够实现安全的模拟到现实迁移。SPiDR利用领域随机化将对模拟到现实差距的不确定性纳入安全约束中，使其具有高度通用性，并且与现有训练流程高度兼容。通过在模拟到模拟基准测试和两个不同的现实世界机器人平台上的大量实验，我们证明了SPiDR能够在存在模拟到现实差距的情况下有效确保安全性，同时保持强大的性能表现。</p>
<h3 id="Every-Step-Evolves-Scaling-Reinforcement-Learning-for-Trillion-Scale-Thinking-Model"><a href="#Every-Step-Evolves-Scaling-Reinforcement-Learning-for-Trillion-Scale-Thinking-Model" class="headerlink" title="Every Step Evolves: Scaling Reinforcement Learning for Trillion-Scale Thinking Model"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18855v1">Every Step Evolves: Scaling Reinforcement Learning for Trillion-Scale Thinking Model</a></h3><p><strong>Categories:</strong> Reinforcement Learning, Large Language Models</p>
<p>我们推出了 Ring-1T，这是首个开源且最先进的万亿参数规模思考模型。该模型拥有总计 1 万亿个参数，每个 token 激活约 500 亿个参数。在万亿参数规模上训练模型带来了前所未有的挑战，包括训练与推理之间的不匹配、滚出处理中的低效以及强化学习（RL）系统中的瓶颈。为了解决这些问题，我们开创性地提出了三项相互关联的创新：(1) IcePop 通过 token 级别差异掩码和裁剪稳定强化学习训练，解决了由训练与推理不匹配导致的不稳定性；(2) C3PO++ 在 token 预算下，通过动态划分长滚出，提高资源利用率，从而获得高时间效率；(3) ASystem 是一个高性能的强化学习框架，旨在克服阻碍万亿参数模型训练的系统性瓶颈。Ring-1T 在关键基准测试中取得了突破性成果：在 AIME-2025 上达到 93.4，在 HMMT-2025 上达到 86.72，在 CodeForces 上达到 2088，在 ARC-AGI-v1 上达到 55.94。值得注意的是，它在 IMO-2025 上取得了银牌级别的成绩，凸显了其卓越的推理能力。通过向社区发布完整的 1 万亿参数 MoE 模型，我们为研究社区提供了直接访问前沿推理能力的途径。这一贡献标志着大规模推理智能普及的重要里程碑，并为开源模型性能设定了新的基准。</p>
<h3 id="Lyapunov-Aware-Quantum-Inspired-Reinforcement-Learning-for-Continuous-Time-Vehicle-Control-A-Feasibility-Study"><a href="#Lyapunov-Aware-Quantum-Inspired-Reinforcement-Learning-for-Continuous-Time-Vehicle-Control-A-Feasibility-Study" class="headerlink" title="Lyapunov-Aware Quantum-Inspired Reinforcement Learning for Continuous-Time Vehicle Control: A Feasibility Study"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18852v1">Lyapunov-Aware Quantum-Inspired Reinforcement Learning for Continuous-Time Vehicle Control: A Feasibility Study</a></h3><p><strong>Categories:</strong> Reinforcement Learning, Robotics</p>
<p>本文提出了一种基于李雅普诺夫函数的量子强化学习（LQRL）框架，该框架将量子策略优化与李雅普诺夫稳定性分析相结合，用于连续时间车辆控制。所提出的方法结合了变分量子电路（VQCs）的表示能力与一种具有稳定性感知的策略梯度机制，以确保在动态环境中实现渐近收敛和安全决策。车辆纵向控制问题被建模为一个连续状态的强化学习任务，其中量子策略网络在李雅普诺夫稳定性约束下生成控制动作。在基于量子启发策略的稳定性反馈训练下，通过闭环自适应巡航控制场景进行了仿真实验。结果表明，LQRL框架成功地将李雅普诺夫稳定性验证嵌入到量子策略学习中，实现了可解释且具有稳定性的控制性能。尽管在激进加速情况下观察到了瞬态超调和李雅普诺夫发散现象，但系统仍保持了状态演化的有界性，验证了在量子强化学习架构中集成安全保证的可行性。所提出的框架为自主系统和混合量子-经典优化领域中可证明安全的量子控制提供了基础性步骤。</p>
<h3 id="DP-2-O-SR-Direct-Perceptual-Preference-Optimization-for-Real-World-Image-Super-Resolution"><a href="#DP-2-O-SR-Direct-Perceptual-Preference-Optimization-for-Real-World-Image-Super-Resolution" class="headerlink" title="DP$^2$O-SR: Direct Perceptual Preference Optimization for Real-World Image Super-Resolution"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18851v1">DP$^2$O-SR: Direct Perceptual Preference Optimization for Real-World Image Super-Resolution</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>得益于预训练的文本到图像（T2I）扩散模型，现实世界图像超分辨率（Real-ISR）方法可以合成丰富且逼真的细节。然而，由于T2I模型固有的随机性，不同的噪声输入常常导致感知质量差异较大的输出。尽管这种随机性有时被视为一种限制，但它也引入了更广的感知质量范围，可以用于提升Real-ISR的性能。为此，我们引入了一种名为“直接感知偏好优化用于Real-ISR（DP$^2$O-SR）”的框架，该框架无需耗费大量的人工标注，即可将生成模型与感知偏好对齐。我们通过结合在大规模人类偏好数据集上训练的全参考和无参考图像质量评估（IQA）模型，构建了一个混合奖励信号。该奖励信号鼓励结构保真度和自然外观。为了更好地利用感知多样性，我们超越了传统的最佳与最差选择方法，从同一模型的输出中构建多个偏好对。我们的分析表明，最佳选择比例取决于模型容量：较小的模型受益于更广泛的覆盖范围，而较大的模型则对监督中更强的对比度反应更好。此外，我们提出了分层偏好优化方法，该方法根据组内奖励差距和组间多样性自适应地加权训练对，从而实现更高效且稳定的训练。在基于扩散模型和流模型的T2I主干网络上进行的大量实验表明，DP$^2$O-SR显著提升了感知质量，并在现实世界基准测试中具有良好的泛化能力。</p>
<h3 id="Towards-Faithful-and-Controllable-Personalization-via-Critique-Post-Edit-Reinforcement-Learning"><a href="#Towards-Faithful-and-Controllable-Personalization-via-Critique-Post-Edit-Reinforcement-Learning" class="headerlink" title="Towards Faithful and Controllable Personalization via Critique-Post-Edit Reinforcement Learning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18849v1">Towards Faithful and Controllable Personalization via Critique-Post-Edit Reinforcement Learning</a></h3><p><strong>Categories:</strong> Reinforcement Learning, Large Language Models</p>
<p>将大型语言模型（LLMs）忠实个性化以符合个人用户偏好是一项关键但具有挑战性的任务。尽管监督微调（SFT）能够迅速达到性能瓶颈，但基于人类反馈的标准强化学习（RLHF）也难以处理个性化中的细微差别。基于标量的奖励模型容易受到奖励欺骗，导致生成的回复冗长且表面上的个性化。为了解决这些问题，我们提出了Critique-Post-Edit，一种稳健的强化学习框架，能够实现更忠实且可控的个性化。我们的框架集成了两个关键组件：（1）个性化生成奖励模型（GRM），它可以提供多维评分和文本批评，以抵抗奖励欺骗；（2）Critique-Post-Edit机制，其中策略模型根据这些批评修订自身的输出，以实现更具针对性和高效的学习。在严格的长度控制评估下，我们的方法在个性化基准测试中显著优于标准PPO。个性化Qwen2.5-7B模型平均提高了11%的胜率，个性化Qwen2.5-14B模型的性能超越了GPT-4.1。这些结果表明了一条实现忠实、高效且可控个性化的实用路径。</p>
<h3 id="Actor-Free-Continuous-Control-via-Structurally-Maximizable-Q-Functions"><a href="#Actor-Free-Continuous-Control-via-Structurally-Maximizable-Q-Functions" class="headerlink" title="Actor-Free Continuous Control via Structurally Maximizable Q-Functions"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18828v1">Actor-Free Continuous Control via Structurally Maximizable Q-Functions</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>基于价值的算法是无策略强化学习的核心，因为它们具有简单性和训练稳定性。然而，传统上它们仅限于离散动作空间，因为它们依赖于对单个状态-动作对进行Q值估计。在连续动作空间中，对整个动作空间进行Q值评估在计算上是不可行的。为了解决这一问题，通常采用演员-评论家方法，其中评论家利用无策略数据训练以估计Q值，而演员则被训练以最大化评论家的输出。尽管这些方法很受欢迎，但在训练过程中常常会遇到不稳定性问题。在本文中，我们提出了一种纯粹基于价值的框架，用于连续控制，重新审视Q函数的结构最大化，引入了一系列关键的架构和算法选择，以实现高效且稳定的训练。我们评估了所提出的无演员Q学习方法在一系列标准仿真任务中的表现，结果表明其性能和样本效率与最先进的基线相当，而无需单独学习一个演员。特别是在具有约束动作空间的环境中，由于价值函数通常是不光滑的，我们的基于结构最大化的算法在性能上优于传统的基于梯度最大化的演员-评论家方法。我们已将代码发布在<a target="_blank" rel="noopener" href="https://github.com/USC-Lira/Q3C%E4%B8%8A%E3%80%82">https://github.com/USC-Lira/Q3C上。</a></p>
<h3 id="Stabilizing-MoE-Reinforcement-Learning-by-Aligning-Training-and-Inference-Routers"><a href="#Stabilizing-MoE-Reinforcement-Learning-by-Aligning-Training-and-Inference-Routers" class="headerlink" title="Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.11370v2">Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers</a></h3><p><strong>Categories:</strong> Reinforcement Learning, Large Language Models</p>
<p>强化学习（RL）已成为增强大语言模型能力的关键方法。然而，在专家混合（Mixture-of-Experts，MoE）模型中，路由机制常常引入不稳定性，甚至导致灾难性的强化学习训练崩溃。我们分析了MoE模型的训练与推理一致性，并发现两个阶段之间存在显著的路由行为差异。此外，即使在相同条件下，路由框架在多次前向传递中也可能产生不同的专家选择。为了解决这一基础性不一致性，我们提出了Rollout Routing Replay（R3）方法，该方法记录推理引擎中的路由分布并在训练过程中回放这些分布。R3显著降低了训练与推理策略的KL散度，并在不牺牲训练速度的前提下缓解了极端差异。在多种设置下的大量实验表明，R3成功地稳定了强化学习训练，防止了崩溃，并优于GSPO和TIS等方法。我们相信，这项工作为在MoE模型中稳定强化学习提供了一种新的解决方案。</p>
<h3 id="Understanding-Reinforcement-Learning-for-Model-Training-and-future-directions-with-GRAPE"><a href="#Understanding-Reinforcement-Learning-for-Model-Training-and-future-directions-with-GRAPE" class="headerlink" title="Understanding Reinforcement Learning for Model Training, and future directions with GRAPE"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2509.04501v2">Understanding Reinforcement Learning for Model Training, and future directions with GRAPE</a></h3><p><strong>Categories:</strong> Reinforcement Learning, Large Language Models</p>
<p>本文提供了一种从零开始、自洽的关于模型指令微调关键算法的阐述：监督微调（SFT）、拒绝抽样（Rejection Sampling）、REINFORCE、信任区域策略优化（TRPO）、邻近策略优化（PPO）、组相对策略优化（GRPO）以及直接偏好优化（DPO）。通常，对这些算法的解释往往假设读者具备先验知识，缺乏关键细节，或者过于笼统和复杂。在这里，我们以简化且明确的符号，聚焦于大语言模型（LLMs），逐步讨论和推导每种方法，旨在消除歧义，提供清晰且直观的概念理解。通过尽量减少对更广泛强化学习（RL）文献的偏离，并将概念与LLMs联系起来，我们消除了多余的抽象概念，降低了认知负担。在阐述之后，我们对超出本文详细讨论范围的新技术和方法进行了文献综述。最后，我们提出了新的研究和探索思路，即广义相对优势策略进化（GRAPE）。</p>
<h3 id="Preference-based-Reinforcement-Learning-beyond-Pairwise-Comparisons-Benefits-of-Multiple-Options"><a href="#Preference-based-Reinforcement-Learning-beyond-Pairwise-Comparisons-Benefits-of-Multiple-Options" class="headerlink" title="Preference-based Reinforcement Learning beyond Pairwise Comparisons: Benefits of Multiple Options"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18713v1">Preference-based Reinforcement Learning beyond Pairwise Comparisons: Benefits of Multiple Options</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>我们研究基于在线偏好强化学习（Preference-based Reinforcement Learning，PbRL）的方法，旨在提高样本效率。尽管近年来由于PbRL在实际应用中的成功，特别是与大规模语言模型（LLMs）对齐方面的进展，已涌现出大量理论研究，但大多数现有研究仅关注成对比较。最近一些工作（Zhu等，2023；Mukherjee等，2024；Thekumparampil等，2024）探索了利用多比较和排名反馈的方法，但它们的性能保证随着反馈长度的增加而无法改善，甚至可能恶化，尽管可用的信息更加丰富。为了解决这一问题，我们采用Plackett-Luce（PL）模型来处理对动作子集的排名反馈，并提出了一种名为M-AUPO的算法，该算法通过在提供的子集中最大化平均不确定性来选择多个动作。我们证明，M-AUPO的次优差距为 $\tilde{\mathcal{O}}\left( \frac{d}{T} \sqrt{ \sum_{t&#x3D;1}^T \frac{1}{|S_t|}} \right)$，其中 $T$ 是总回合数，$d$ 是特征维度，$|S_t|$ 是第 $t$ 回合中子集的大小。这一结果表明，更大的子集直接带来了性能的提升，而且值得注意的是，该界避免了对未知参数范数的指数依赖，这在大多数先前工作中是一个根本性的限制。此外，我们建立了一个接近匹配的下界 $\Omega \left( \frac{d}{K \sqrt{T}} \right)$，其中 $K$ 是最大子集大小。据我们所知，这是首个在PbRL中针对排名反馈明确展示样本效率随子集大小而改善的理论结果。</p>
<h3 id="Sherlock-Your-Queries-Learning-to-Ask-the-Right-Questions-for-Dialogue-Based-Retrieval"><a href="#Sherlock-Your-Queries-Learning-to-Ask-the-Right-Questions-for-Dialogue-Based-Retrieval" class="headerlink" title="Sherlock Your Queries: Learning to Ask the Right Questions for Dialogue-Based Retrieval"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18659v1">Sherlock Your Queries: Learning to Ask the Right Questions for Dialogue-Based Retrieval</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>在信息检索中，用户的查询往往具有歧义性，使得系统难以仅凭单个查询就确定用户的意图。虽然近年来基于对话的交互式检索系统能够澄清用户的意图，但它们效率较低，通常缺乏明确的策略来提出最具信息量的问题。为了解决这一局限，我们提出了SherlockLLM，这是一个基于对话的检索框架，通过强化学习（RL）学习最优的提问策略，从而避免需要大规模标注的对话数据。在我们的框架中，一个智能体被训练生成一系列二元问题，以高效地缩小搜索空间。为了验证我们的方法，我们引入了一个包含结构化和非结构化任务的基准测试。实验结果表明，SherlockLLM是一个稳健且高效的解决方案。在结构化任务中，其性能与强大的基线方法相当，并接近二分查找所定义的理论最优值。在具有挑战性的非结构化任务中，我们的智能体显著优于这些基线方法，展示了其学习高效信息获取对话策略的能力。</p>
<h3 id="Pretraining-a-Shared-Q-Network-for-Data-Efficient-Offline-Reinforcement-Learning"><a href="#Pretraining-a-Shared-Q-Network-for-Data-Efficient-Offline-Reinforcement-Learning" class="headerlink" title="Pretraining a Shared Q-Network for Data-Efficient Offline Reinforcement Learning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.05701v2">Pretraining a Shared Q-Network for Data-Efficient Offline Reinforcement Learning</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>离线强化学习（Offline RL）旨在从静态数据集中学习策略，而无需进一步与环境进行交互。由于离线RL的数据收集需要大量的环境交互，因此收集足够大的数据集非常耗时，而且当与环境的交互受到限制时，这一过程变得尤为困难。因此，如何在最小的静态数据集上让智能体学习到最优策略，是离线强化学习中的一个关键问题，这与在线强化学习中的样本效率问题类似。在本文中，我们提出了一种简单而有效的插件式预训练方法，用于初始化Q网络的特征，以提高离线强化学习的数据效率。具体而言，我们引入了一种共享Q网络结构，该结构可以输出下一个状态和Q值的预测。我们通过一个监督回归任务对共享Q网络进行预训练，该任务预测下一个状态，并使用多种离线强化学习方法训练共享Q网络。通过大量的实验，我们实证表明，我们的方法在D4RL、Robomimic和V-D4RL基准测试中提升了现有流行离线强化学习方法的性能。此外，我们还表明，我们的方法在不同数据质量和数据分布下，显著提升了数据效率的离线强化学习效果，通过D4RL和ExoRL基准测试验证了这一点。值得注意的是，我们的方法仅使用数据集的10%便能超越标准算法在完整数据集上的表现。</p>
<h3 id="Query-Decomposition-for-RAG-Balancing-Exploration-Exploitation"><a href="#Query-Decomposition-for-RAG-Balancing-Exploration-Exploitation" class="headerlink" title="Query Decomposition for RAG: Balancing Exploration-Exploitation"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18633v1">Query Decomposition for RAG: Balancing Exploration-Exploitation</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>检索增强生成（RAG）系统通过将复杂用户请求分解为子查询，为每个子查询检索可能相关的文档，然后将这些文档聚合以生成答案，从而解决复杂用户请求。高效地选择具有信息量的文档需要在关键权衡之间取得平衡：(i) 检索范围足够广泛以涵盖所有相关内容，以及 (ii) 限制检索以避免过多的噪声和计算成本。我们将查询分解和文档检索建模为一种探索-利用框架，在此框架中，每次检索一个文档都会形成对该子查询有用性的信念，并据此决定是继续利用现有信息还是探索其他替代方案。我们尝试了多种多臂老虎机（bandit learning）学习方法，并展示了它们在动态选择最具信息量的子查询方面的有效性。我们的主要发现是：使用排序信息和人类判断来估计文档的相关性，可以提高文档级别的精确度35%，α-nDCG指标提升15%，并且在长文本生成的下游任务中表现更好。</p>
<h3 id="A-representational-framework-for-learning-and-encoding-structurally-enriched-trajectories-in-complex-agent-environments"><a href="#A-representational-framework-for-learning-and-encoding-structurally-enriched-trajectories-in-complex-agent-environments" class="headerlink" title="A representational framework for learning and encoding structurally enriched trajectories in complex agent environments"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2503.13194v2">A representational framework for learning and encoding structurally enriched trajectories in complex agent environments</a></h3><p><strong>Categories:</strong> Reinforcement Learning, World Models</p>
<p>人工智能代理在复杂场景中做出最优决策并将其推广到不同领域和任务的能力受到限制。为了解决这一问题，一种方法是学习对世界进行高效表示，并研究代理行为如何影响这些表示的状态-动作转换。虽然这些表示在程序上是高效的，但它们缺乏结构上的丰富性。为了解决这一问题，我们提出增强代理的本体论，并扩展传统的轨迹概念化，以提供对任务执行更细致的视角。结构丰富轨迹（Structurally Enriched Trajectories，SETs）通过引入对象、交互和可用性之间的层次关系，扩展了状态序列及其转换的编码方式。SETs以多层次图的形式构建，提供对代理动态的详细表示，并提供可迁移的任务功能抽象。SETs被集成到一个架构中，称为结构丰富轨迹学习与编码（Structurally Enriched Trajectory Learning and Encoding，SETLE），该架构采用多层次关系依赖的异构图结构，这对于泛化至关重要。我们证明SETLE可以支持下游任务，使代理能够在CREATE和MiniGrid环境中识别与任务相关的结构模式。最后，我们将SETLE与强化学习相结合，并展示了在复杂、稀疏奖励任务中下游性能的可衡量提升，包括在这些任务中取得突破性的成功率。</p>
<h3 id="BAPO-Stabilizing-Off-Policy-Reinforcement-Learning-for-LLMs-via-Balanced-Policy-Optimization-with-Adaptive-Clipping"><a href="#BAPO-Stabilizing-Off-Policy-Reinforcement-Learning-for-LLMs-via-Balanced-Policy-Optimization-with-Adaptive-Clipping" class="headerlink" title="BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18927v1">BAPO: Stabilizing Off-Policy Reinforcement Learning for LLMs via Balanced Policy Optimization with Adaptive Clipping</a></h3><p><strong>Categories:</strong> Reinforcement Learning, Large Language Models</p>
<p>强化学习（RL）最近已成为对齐和增强大型语言模型（LLMs）的核心范式。然而，在离策略（off-policy）设置中应用RL——即利用过去策略生成的过时数据进行训练——虽然可以提高样本效率，但仍然具有挑战性：策略熵迅速下降，优化过程常常不稳定，甚至可能崩溃。通过理论和实证分析，我们得出了两个关键见解：(i) 优化过程中存在不平衡现象，负优势样本主导了策略梯度，抑制了有益行为，可能导致梯度爆炸；(ii) 我们推导出熵裁剪规则（Entropy-Clip Rule），揭示了在类似PPO的目标中，固定的裁剪机制系统性地阻断了熵增加的更新，从而以牺牲探索为代价，使策略偏向过度利用。基于这些见解，我们提出了自适应裁剪的平衡策略优化（BAPO），这是一种简单而有效的方法，通过动态调整裁剪边界，自适应地重新平衡正负贡献，保持熵值，从而稳定强化学习的优化过程。在多种离策略场景中——包括样本回放和部分 rollout——BAPO 实现了快速、稳定且数据高效的训练。在AIME 2024和AIME 2025基准测试中，我们的7B规模BAPO模型超越了SkyWork-OR1-7B等开源模型，而我们的32B规模BAPO模型不仅在同等规模模型中取得了最先进的结果，还超越了领先的专有系统，如o3-mini和Gemini-2.5-Flash-Thinking。</p>
<h3 id="SimKO-Simple-Pass-K-Policy-Optimization"><a href="#SimKO-Simple-Pass-K-Policy-Optimization" class="headerlink" title="SimKO: Simple Pass@K Policy Optimization"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.14807v2">SimKO: Simple Pass@K Policy Optimization</a></h3><p><strong>Categories:</strong> Reinforcement Learning, Large Language Models</p>
<p>可验证奖励强化学习（RLVR）提升了大型语言模型（LLMs）的推理能力。然而，现有的RLVR方法表现出一种系统性的倾向，即更倾向于利用已知信息而忽视探索新可能性，这一现象在通过pass@1指标提升的同时，pass@K（K&gt;1）指标却有所下降中得到印证。为了解决这一问题，我们通过跟踪词汇候选词上的词元级概率分布，分析RLVR方法的训练动态。我们的分析揭示了一个一致的概率集中效应：排名前1的候选词逐渐积累概率质量，而其他候选词的概率则被抑制。更重要的是，更强的概率集中与更差的pass@K表现密切相关。受此发现启发，我们提出了一种名为“简单Pass@K优化”（SimKO）的方法，旨在缓解过度集中问题，从而鼓励探索。SimKO采用不对称机制：对于经过验证正确的回答，它会提升前K个候选词的概率；而对于经过验证错误的回答，它会对前1个候选词施加更强的惩罚。我们观察到，这种不对称设计在应用于高熵词元时，特别有效于缓解过度集中问题。在各种数学和逻辑推理基准测试中，SimKO在广泛的K值范围内始终表现出更高的pass@K值，为改进RLVR的探索能力提供了一种简单有效的方法。</p>
<h3 id="Beyond-Pass-k-Breadth-Depth-Metrics-for-Reasoning-Boundaries"><a href="#Beyond-Pass-k-Breadth-Depth-Metrics-for-Reasoning-Boundaries" class="headerlink" title="Beyond Pass@k: Breadth-Depth Metrics for Reasoning Boundaries"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.08325v2">Beyond Pass@k: Breadth-Depth Metrics for Reasoning Boundaries</a></h3><p><strong>Categories:</strong> Reinforcement Learning, Large Language Models</p>
<p>可验证奖励强化学习（RLVR）作为一种强大的范式，已被用于提升大型语言模型在推理任务（如编程、数学或逻辑）上的表现。为了评估推理边界（即模型能够解决的问题比例），研究人员通常在大规模采样预算下报告Pass@k指标。最近的研究发现了一种交叉现象：虽然RLVR模型在小k值时优于基线模型，但当采样大量补全结果时，基线模型通常表现更优。这被解释为基线模型具有更大的推理边界。我们认为，在具有离散答案空间的任务中（例如数学问题的数值输出），在大k值下的Pass@k反映了随着试验次数增加而逐渐上升的成功概率，而不是真正的推理能力，因此可能具有误导性。为此，我们提出了Cover@tau指标，该指标衡量模型能够解决的问题中，至少有tau比例的补全结果是正确的比例。与Pass@k不同，Cover@tau在显式的可靠性阈值下衡量推理能力：依赖随机猜测的模型在tau增加时会迅速退化。我们使用基于Cover@tau的指标评估了几种RLVR模型，并展示了流行算法在Cover@tau下的相对排名与Pass@1相比的变化，从而提供了对推理边界的不同视角。</p>
<h3 id="MetaBox-v2-A-Unified-Benchmark-Platform-for-Meta-Black-Box-Optimization"><a href="#MetaBox-v2-A-Unified-Benchmark-Platform-for-Meta-Black-Box-Optimization" class="headerlink" title="MetaBox-v2: A Unified Benchmark Platform for Meta-Black-Box Optimization"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.17745v2">MetaBox-v2: A Unified Benchmark Platform for Meta-Black-Box Optimization</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>元黑箱优化（Meta-Black-Box Optimization，MetaBBO）通过元学习简化了优化算法设计的自动化过程。它通常采用双层结构：元层策略通过元训练来减少在低层优化任务中开发算法所需的人工努力。原始的MetaBox（2023）首次提供了基于强化学习的单目标MetaBBO的开源框架。然而，其相对狭窄的适用范围已无法跟上该领域快速发展的步伐。在本文中，我们介绍了MetaBox-v2（<a target="_blank" rel="noopener" href="https://github.com/MetaEvo/MetaBox%EF%BC%89%EF%BC%8C%E4%BD%9C%E4%B8%BA%E4%B8%80%E6%AC%A1%E5%85%B7%E6%9C%89%E9%87%8C%E7%A8%8B%E7%A2%91%E6%84%8F%E4%B9%89%E7%9A%84%E5%8D%87%E7%BA%A7%EF%BC%8C%E5%85%B7%E6%9C%89%E5%9B%9B%E4%B8%AA%E5%88%9B%E6%96%B0%E7%89%B9%E6%80%A7%EF%BC%9A1%EF%BC%89%E4%B8%80%E7%A7%8D%E7%BB%9F%E4%B8%80%E7%9A%84%E6%9E%B6%E6%9E%84%EF%BC%8C%E6%94%AF%E6%8C%81%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E3%80%81%E8%BF%9B%E5%8C%96%E7%AE%97%E6%B3%95%E5%92%8C%E5%9F%BA%E4%BA%8E%E6%A2%AF%E5%BA%A6%E7%9A%84%E6%96%B9%E6%B3%95%EF%BC%8C%E6%88%91%E4%BB%AC%E5%80%9F%E6%AD%A4%E5%A4%8D%E7%8E%B0%E4%BA%8623%E4%B8%AA%E6%9C%80%E6%96%B0%E7%9A%84%E5%9F%BA%E7%BA%BF%E6%96%B9%E6%B3%95%EF%BC%9B2%EF%BC%89%E9%AB%98%E6%95%88%E7%9A%84%E5%B9%B6%E8%A1%8C%E5%8C%96%E6%96%B9%E6%A1%88%EF%BC%8C%E5%B0%86%E8%AE%AD%E7%BB%83%E5%92%8C%E6%B5%8B%E8%AF%95%E6%97%B6%E9%97%B4%E5%87%8F%E5%B0%91%E4%BA%8610%E5%88%B040%E5%80%8D%EF%BC%9B3%EF%BC%89%E4%B8%80%E5%A5%97%E6%B6%B5%E7%9B%9618%E4%B8%AA%E5%90%88%E6%88%90/%E7%8E%B0%E5%AE%9E%E4%BB%BB%E5%8A%A1%EF%BC%88%E8%B6%85%E8%BF%871900%E4%B8%AA%E5%AE%9E%E4%BE%8B%EF%BC%89%E7%9A%84%E5%85%A8%E9%9D%A2%E5%9F%BA%E5%87%86%E6%B5%8B%E8%AF%95%E9%9B%86%EF%BC%8C%E6%B6%B5%E7%9B%96%E5%8D%95%E7%9B%AE%E6%A0%87%E3%80%81%E5%A4%9A%E7%9B%AE%E6%A0%87%E3%80%81%E5%A4%9A%E6%A8%A1%E5%9E%8B%E5%92%8C%E5%A4%9A%E4%BB%BB%E5%8A%A1%E4%BC%98%E5%8C%96%E5%9C%BA%E6%99%AF%EF%BC%9B4%EF%BC%89%E4%B8%B0%E5%AF%8C%E4%B8%94%E5%8F%AF%E6%89%A9%E5%B1%95%E7%9A%84%E6%8E%A5%E5%8F%A3%EF%BC%8C%E6%94%AF%E6%8C%81%E8%87%AA%E5%AE%9A%E4%B9%89%E5%88%86%E6%9E%90/%E5%8F%AF%E8%A7%86%E5%8C%96%E4%BB%A5%E5%8F%8A%E4%B8%8E%E5%A4%96%E9%83%A8%E4%BC%98%E5%8C%96%E5%B7%A5%E5%85%B7/%E5%9F%BA%E5%87%86%E7%9A%84%E9%9B%86%E6%88%90%E3%80%82%E4%B8%BA%E4%BA%86%E5%B1%95%E7%A4%BAMetaBox-v2%E7%9A%84%E5%AE%9E%E7%94%A8%E6%80%A7%EF%BC%8C%E6%88%91%E4%BB%AC%E8%BF%9B%E8%A1%8C%E4%BA%86%E4%B8%80%E9%A1%B9%E7%B3%BB%E7%BB%9F%E6%80%A7%E7%9A%84%E6%A1%88%E4%BE%8B%E7%A0%94%E7%A9%B6%EF%BC%8C%E4%BB%8E%E4%BC%98%E5%8C%96%E6%80%A7%E8%83%BD%E3%80%81%E6%B3%9B%E5%8C%96%E8%83%BD%E5%8A%9B%E5%92%8C%E5%AD%A6%E4%B9%A0%E6%95%88%E7%8E%87%E4%B8%89%E4%B8%AA%E6%96%B9%E9%9D%A2%E8%AF%84%E4%BC%B0%E5%86%85%E7%BD%AE%E7%9A%84%E5%9F%BA%E7%BA%BF%E6%96%B9%E6%B3%95%E3%80%82%E9%80%9A%E8%BF%87%E5%AF%B9%E8%BF%99%E4%BA%9B%E6%96%B9%E6%B3%95%E8%BF%9B%E8%A1%8C%E8%AF%A6%E5%B0%BD%E5%88%86%E6%9E%90%EF%BC%8C%E6%88%91%E4%BB%AC%E4%B8%BA%E4%BB%8E%E4%B8%9A%E8%80%85%E4%BB%A5%E5%8F%8A%E8%AF%A5%E9%A2%86%E5%9F%9F%E7%9A%84%E6%96%B0%E6%89%8B%E6%80%BB%E7%BB%93%E5%87%BA%E5%AE%9D%E8%B4%B5%E7%9A%84%E8%A7%81%E8%A7%A3%E3%80%82">https://github.com/MetaEvo/MetaBox），作为一次具有里程碑意义的升级，具有四个创新特性：1）一种统一的架构，支持强化学习、进化算法和基于梯度的方法，我们借此复现了23个最新的基线方法；2）高效的并行化方案，将训练和测试时间减少了10到40倍；3）一套涵盖18个合成/现实任务（超过1900个实例）的全面基准测试集，涵盖单目标、多目标、多模型和多任务优化场景；4）丰富且可扩展的接口，支持自定义分析/可视化以及与外部优化工具/基准的集成。为了展示MetaBox-v2的实用性，我们进行了一项系统性的案例研究，从优化性能、泛化能力和学习效率三个方面评估内置的基线方法。通过对这些方法进行详尽分析，我们为从业者以及该领域的新手总结出宝贵的见解。</a></p>
<h3 id="Noise-corrected-GRPO-From-Noisy-Rewards-to-Unbiased-Gradients"><a href="#Noise-corrected-GRPO-From-Noisy-Rewards-to-Unbiased-Gradients" class="headerlink" title="Noise-corrected GRPO: From Noisy Rewards to Unbiased Gradients"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18924v1">Noise-corrected GRPO: From Noisy Rewards to Unbiased Gradients</a></h3><p><strong>Categories:</strong> Reinforcement Learning, Large Language Models</p>
<p>基于人类反馈的强化学习（RLHF）或可验证奖励（RLVR）是当前对大语言模型（LLMs）进行对齐或构建最新SOTA推理模型的标准范式，但其对不一致或错误奖励带来的噪声非常敏感。然而，这类噪声与广泛使用的基于群体的策略优化方法之间的相互作用仍缺乏深入研究。我们提出了一种鲁棒性更强的群体相对策略优化（Group Relative Policy Optimization, GRPO）和“正确执行”GRPO（Done Right GRPO, Dr.GRPO）框架，该框架明确将奖励污染建模为伯努利噪声。我们的方法在估计奖励翻转概率后应用噪声校正，从而去偏学习信号，得到可证明无偏的梯度估计。理论分析表明，基于群体的方法在本质上能够缓解个体层面的噪声，而我们的校正策略则增强了这种鲁棒性。实验证明，在将我们的噪声校正应用于标准奖励模型时，数学和代码任务均表现出一致的性能提升，尤其在现实条件下，数学任务的准确率提升了最多6.7个百分点，代码任务则提升了1.5个百分点。本工作将监督学习中的标签噪声校正方法与现代RLHF相结合，既提供了理论洞见，也提出了一种适用于噪声环境的实际算法。</p>
<h3 id="ProSh-Probabilistic-Shielding-for-Model-free-Reinforcement-Learning"><a href="#ProSh-Probabilistic-Shielding-for-Model-free-Reinforcement-Learning" class="headerlink" title="ProSh: Probabilistic Shielding for Model-free Reinforcement Learning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.15720v2">ProSh: Probabilistic Shielding for Model-free Reinforcement Learning</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>安全性是强化学习（RL）中的一个主要关注点：我们的目标是开发出不仅能够最优运行，而且在部署时也安全的RL系统，为此我们需要提供关于其安全性的正式保证。为此，我们引入了基于风险增强的概率防护（Probabilistic Shielding via Risk Augmentation，ProSh），这是一种用于在成本约束下进行安全强化学习的无模型算法。ProSh通过在约束马尔可夫决策过程（Constrained MDP）的状态空间中引入一个风险预算来扩展状态空间，并利用一个通过学习获得的成本批评家（cost critic）对智能体的策略分布应用防护机制，以确保安全性。该防护机制确保所有采样的动作在期望上都保持安全。我们还证明，在环境是确定性的条件下，最优性得以保持。由于ProSh是无模型的，因此训练过程中的安全性依赖于我们对环境所获得的知识。我们提供了一个关于期望成本的紧致上界，该上界仅取决于备份批评家（backup-critic）的准确性，并且在训练过程中始终得到满足。在一些温和且在实践中可实现的假设下，实验结果表明，ProSh即使在训练过程中也能够保证安全性。</p>
<h3 id="Expressive-Reward-Synthesis-with-the-Runtime-Monitoring-Language"><a href="#Expressive-Reward-Synthesis-with-the-Runtime-Monitoring-Language" class="headerlink" title="Expressive Reward Synthesis with the Runtime Monitoring Language"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.16185v2">Expressive Reward Synthesis with the Runtime Monitoring Language</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>强化学习（RL）中的一个关键挑战是奖励（误）指定问题，即不精确的奖励函数可能导致意外的、甚至有害的行为。事实上，RL中的奖励函数通常被视为从状态-动作对到标量值的黑盒映射。虽然这种方法在许多场景中是有效的，但它无法提供关于为何给予奖励的信息，这可能会阻碍学习过程和可解释性。奖励机器（Reward Machines）通过将奖励函数表示为有限状态自动机来解决这一问题，从而能够指定结构化的、非马尔可夫的奖励函数。然而，它们的表达能力通常受限于正则语言，无法捕捉更复杂的行为主，例如计数或参数化条件。在本工作中，我们基于运行时监控语言（RML）开发了一种新型的语言型奖励机器。通过利用RML内置的记忆机制，我们的方法可以为非正则、非马尔可夫任务指定奖励函数。我们通过实验展示了该方法的表达能力，并突出了其在灵活事件处理和任务指定方面相比现有基于奖励机器的方法所具有的额外优势。</p>
<h3 id="CodeRL-Improving-Code-Generation-via-Reinforcement-with-Execution-Semantics-Alignment"><a href="#CodeRL-Improving-Code-Generation-via-Reinforcement-with-Execution-Semantics-Alignment" class="headerlink" title="CodeRL+: Improving Code Generation via Reinforcement with Execution Semantics Alignment"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18471v1">CodeRL+: Improving Code Generation via Reinforcement with Execution Semantics Alignment</a></h3><p><strong>Categories:</strong> Reinforcement Learning, Large Language Models</p>
<p>尽管大型语言模型（LLMs）通过从庞大的代码语料库中学习，擅长于代码生成，但它们在文本模式训练与功能正确性目标之间仍存在一个基本的语义鸿沟，而功能正确性是由形式化执行语义所决定的。可验证奖励的强化学习（RLVR）方法尝试通过执行测试用例获得的结果奖励来弥合这一鸿沟。然而，仅仅依赖于二进制的通过&#x2F;失败信号，对于在代码的文本表示与其执行语义之间建立良好对齐关系来说是低效的，尤其是在处理代码中微妙的逻辑错误时。在本文中，我们提出了CodeRL+，这是一种新颖的方法，将执行语义对齐整合到RLVR的训练流程中，用于代码生成。CodeRL+使模型能够推断变量级别的执行轨迹，从而提供直接的执行语义学习信号。CodeRL+可以直接利用现有的基于策略的rollout来构建执行语义对齐，并且能够与各种强化学习算法无缝整合。大量实验表明，CodeRL+优于后训练基准（包括RLVR和蒸馏方法），在pass@1指标上平均相对提升4.6%。CodeRL+能够有效泛化到其他编码任务，在代码推理和测试输出生成基准测试中分别实现了15.5%和4.4%的更高准确率。CodeRL+在各种强化学习算法和大型语言模型中表现出强大的适用性。此外，探针分析提供了有力的证据，表明CodeRL+增强了代码文本表示与其底层执行语义之间的对齐。</p>
<h3 id="DeLoad-Demand-Driven-Short-Video-Preloading-with-Scalable-Watch-Time-Estimation"><a href="#DeLoad-Demand-Driven-Short-Video-Preloading-with-Scalable-Watch-Time-Estimation" class="headerlink" title="DeLoad: Demand-Driven Short-Video Preloading with Scalable Watch-Time Estimation"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18459v1">DeLoad: Demand-Driven Short-Video Preloading with Scalable Watch-Time Estimation</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>短视频流媒体已成为数字媒体中的主导范式，其特点是快速滑动交互和多样化的多媒体内容。一个关键的技术挑战是设计一种有效的预加载策略，该策略能够从不断变化的播放列表中动态地选择和优先处理下载任务，在实际商业约束条件下平衡用户体验质量（QoE）和带宽效率。然而，现实世界的分析揭示了现有方法的几个关键局限：（1）下载任务大小对动态条件的适应不足；（2）预测观看时间的模型难以大规模可靠部署。在本文中，我们提出了一种名为DeLoad的新颖预加载框架，通过引入动态任务大小和一种实用的、多维观看时间估计方法来解决这些问题。此外，我们还训练了一个增强型深度强化学习（DRL）智能体，以自适应地优化下载范围的决策。在大规模真实网络数据的支持下，我们在离线测试平台上进行了广泛的评估，结果表明DeLoad在QoE指标上实现了显著提升（提升幅度为34.4%至87.4%）。此外，DeLoad在大规模商用短视频平台上部署后，总体用户观看时间增加了0.09%，同时减少了回放事件并降低了3.76%的带宽消耗。</p>
<h3 id="Counterfactual-Effect-Decomposition-in-Multi-Agent-Sequential-Decision-Making"><a href="#Counterfactual-Effect-Decomposition-in-Multi-Agent-Sequential-Decision-Making" class="headerlink" title="Counterfactual Effect Decomposition in Multi-Agent Sequential Decision Making"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2410.12539v3">Counterfactual Effect Decomposition in Multi-Agent Sequential Decision Making</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>我们探讨了在多智能体马尔可夫决策过程中解释反事实结果的挑战。具体而言，我们旨在通过其对环境动态和智能体行为的影响，解释一个智能体行为对实际场景结果的总体反事实效应。为实现这一目标，我们引入了一种新颖的因果解释公式，通过将反事实效应分解为对每个智能体和状态变量的评分，反映它们在效应中的相应贡献。</p>
<p>首先，我们证明一个智能体行为的总体反事实效应可以分解为两个组成部分：一个是衡量通过后续所有智能体行为传播的效应，另一个是衡量通过状态转移传播的效应。基于近期在因果贡献分析方面的进展，我们进一步对这两种效应进行分解。对于前者，我们考虑了智能体特定效应——这是一个因果概念，用于量化一个智能体行为通过部分智能体传播的反事实效应。基于这一概念，我们使用夏普利值（Shapley value）将效应归因于各个智能体。对于后者，我们考虑了结构保持干预的概念，并根据状态变量的“内在”贡献将效应归因于这些变量。通过广泛的实验，我们在具有大语言模型（LLM）辅助的网格世界环境中以及脓毒症管理模拟器中展示了我们方法的可解释性。</p>
<h3 id="On-AI-Verification-in-Open-RAN"><a href="#On-AI-Verification-in-Open-RAN" class="headerlink" title="On AI Verification in Open RAN"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18417v1">On AI Verification in Open RAN</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>开放无线接入网络（Open RAN）引入了一种灵活的基于云的无线接入网络（RAN）架构，使人工智能（AI）&#x2F;机器学习（ML）驱动的自动化能够在异构、多供应商部署中实现。尽管可解释人工智能（XAI）有助于缓解AI模型的不透明性，但仅靠可解释性并不能保证可靠的网络运行。在本文中，我们提出了一种基于可解释模型的轻量级验证方法，用于验证开放RAN中用于网络切片和调度的深度强化学习（DRL）代理的行为。具体而言，我们使用基于决策树（DT）的验证器在运行时执行近实时的一致性检查，这在使用计算成本高昂的最先进验证器时是不可行的。我们分析了XAI和AI验证的现状，提出了可扩展的架构集成方案，并通过基于决策树的切片验证器展示了其实现的可行性。此外，我们还概述了未来面临的挑战，以确保在开放RAN中可信AI的采用。</p>
<h3 id="Heterogeneous-Adversarial-Play-in-Interactive-Environments"><a href="#Heterogeneous-Adversarial-Play-in-Interactive-Environments" class="headerlink" title="Heterogeneous Adversarial Play in Interactive Environments"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18407v1">Heterogeneous Adversarial Play in Interactive Environments</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>自我对弈是自主技能获取的基本范式，其中智能体通过自主探索环境来迭代提升自身能力。传统的自我对弈框架在零和竞争环境中利用智能体之间的对称性，但这种方法在具有内在不对称性的开放学习场景中显得不足。人类教学系统则展示了不对称的教学框架，其中教育者系统性地构建挑战，以适应个体学习者的发展轨迹。主要挑战在于如何在能够自主合成适当课程、无需预设任务层级的人工系统中实现这些不对称、自适应的教学机制。本文提出了异构对抗性对弈（Heterogeneous Adversarial Play, HAP），这是一种对抗性自动课程学习框架，将教师-学生互动形式化为一个最小最大优化问题，其中任务生成的教师和问题解决的学习者通过对抗动态共同进化。与当前主流的自动课程学习方法（其采用静态课程或单向任务选择机制）不同，HAP建立了一个双向反馈系统，其中教师根据学习者实时表现指标持续调整任务复杂度。在多任务学习领域的实验验证表明，我们的框架在性能上与最先进的基线方法相当，同时生成的课程能够提升人工智能代理和人类学习者的学习效果。</p>
<h3 id="MENTOR-A-Reinforcement-Learning-Framework-for-Model-Enhancement-via-Teacher-Optimized-Rewards-in-Small-Models"><a href="#MENTOR-A-Reinforcement-Learning-Framework-for-Model-Enhancement-via-Teacher-Optimized-Rewards-in-Small-Models" class="headerlink" title="MENTOR: A Reinforcement Learning Framework for Model Enhancement via Teacher-Optimized Rewards in Small Models"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18383v1">MENTOR: A Reinforcement Learning Framework for Model Enhancement via Teacher-Optimized Rewards in Small Models</a></h3><p><strong>Categories:</strong> Reinforcement Learning, Large Language Models</p>
<p>将大型语言模型（LLMs）的工具使用能力提炼为更小、更高效的微型语言模型（SLMs），是其实用化应用中的一个关键挑战。目前主流的方法是监督微调（SFT），但这种方法存在泛化能力差的问题，因为它训练模型模仿一组静态的教师轨迹，而非学习稳健的方法论。虽然强化学习（RL）提供了一种替代方案，但使用稀疏奖励的标准RL方法无法有效指导SLMs，导致其在探索过程中效率低下并采用次优策略。为了解决这些不同的挑战，我们提出了MENTOR框架，该框架协同结合了强化学习与教师引导的蒸馏方法。与简单的模仿不同，MENTOR采用基于强化学习的过程，通过探索学习更具泛化能力的策略。此外，为了解决奖励稀疏性问题，MENTOR利用教师的参考轨迹构建一个密集的、复合的教师引导奖励，从而提供更精细的指导。大量实验表明，与SFT和标准稀疏奖励RL基线方法相比，MENTOR显著提升了SLMs的跨领域泛化能力和策略水平。</p>
<h3 id="Segment-Policy-Optimization-Effective-Segment-Level-Credit-Assignment-in-RL-for-Large-Language-Models"><a href="#Segment-Policy-Optimization-Effective-Segment-Level-Credit-Assignment-in-RL-for-Large-Language-Models" class="headerlink" title="Segment Policy Optimization: Effective Segment-Level Credit Assignment in RL for Large Language Models"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.23564v2">Segment Policy Optimization: Effective Segment-Level Credit Assignment in RL for Large Language Models</a></h3><p><strong>Categories:</strong> Reinforcement Learning, Large Language Models</p>
<p>利用强化学习（RL）有效提升大语言模型的推理能力仍然是一个关键挑战。现有的方法主要采用两种截然不同的优势估计粒度：基于token级别的方法（如PPO）旨在提供细粒度的优势信号，但由于训练准确的critic模型困难，导致估计不准确。而在另一极端，基于轨迹级别的方法（如GRPO）仅依赖于最终奖励提供的粗粒度优势信号，导致信用分配不精确。为了解决这些局限性，我们提出了Segment Policy Optimization（SPO），一种新颖的强化学习框架，利用中间粒度的段级优势估计，在提供比轨迹级方法更精确的信用分配的同时，所需的估计点比token级方法更少，从而能够在不使用critic模型的情况下，基于蒙特卡洛（MC）方法实现准确的优势估计。SPO包含三个具有创新策略的组件：（1）灵活的段划分；（2）精确的段优势估计；（3）使用段优势进行策略优化，包括一种新颖的概率掩码策略。我们进一步将SPO应用于两个具体场景：（1）SPO-chain用于短链式思维（CoT），具有基于切点的划分和基于链的优势估计，相较于PPO和GRPO，在GSM8K数据集上准确率提升了6-12个百分点。（2）SPO-tree用于长链式思维（CoT），具有基于树的优势估计，显著降低了MC估计的成本，在MATH500数据集的2K和4K上下文评估中，相较于GRPO，准确率提升了7-11个百分点。我们的代码已公开在<a target="_blank" rel="noopener" href="https://github.com/AIFrameResearch/SPO%E3%80%82">https://github.com/AIFrameResearch/SPO。</a></p>
<h3 id="Shuffle-R1-Efficient-RL-framework-for-Multimodal-Large-Language-Models-via-Data-centric-Dynamic-Shuffle"><a href="#Shuffle-R1-Efficient-RL-framework-for-Multimodal-Large-Language-Models-via-Data-centric-Dynamic-Shuffle" class="headerlink" title="Shuffle-R1: Efficient RL framework for Multimodal Large Language Models via Data-centric Dynamic Shuffle"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.05612v3">Shuffle-R1: Efficient RL framework for Multimodal Large Language Models via Data-centric Dynamic Shuffle</a></h3><p><strong>Categories:</strong> Reinforcement Learning, Large Language Models</p>
<p>强化学习（RL）作为一种有效的后训练范式，已被证明能够增强多模态大语言模型（MLLM）的推理能力。然而，当前的RL流水线往往受到两个尚未充分探索的问题所导致的训练低效性困扰：优势坍缩（Advantage Collapsing），即一批数据中的大部分优势值集中在零附近；以及回滚静默（Rollout Silencing），即随时间推移，产生非零梯度的回滚比例逐渐减少。这些问题导致了次优的梯度更新，并阻碍了长期的学习效率。为了解决这些问题，我们提出了Shuffle-R1，这是一个简单而具有理论依据的框架，通过动态重构轨迹采样和批次组成，提高RL微调效率。该框架引入了两项技术：(1) 成对轨迹采样（Pairwise Trajectory Sampling），选择具有大优势值的高对比度轨迹以提升梯度信号质量；(2) 基于优势的轨迹洗牌（Advantage-based Trajectory Shuffle），通过有意识的批次重排，增加有价值回滚的曝光度。在多个推理基准测试中，我们的框架在几乎不增加额外开销的情况下，持续优于强大的RL基线方法。这些结果突显了针对数据为中心的适应方法在提高MLLM中RL训练效率方面的重要性。</p>
<h3 id="ADPO-Anchored-Direct-Preference-Optimization"><a href="#ADPO-Anchored-Direct-Preference-Optimization" class="headerlink" title="ADPO: Anchored Direct Preference Optimization"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18913v1">ADPO: Anchored Direct Preference Optimization</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>基于锚定的直接偏好优化（ADPO）是一个统一框架，它通过引入软偏好、参考策略锚定以及组内扩展，将直接偏好优化（DPO）进行泛化。与标准的DPO假设硬二元标签和成对比较不同，ADPO引入了以下三个关键改进：(i) 软偏好概率，用于编码不确定性并减轻梯度漂移；(ii) 任意参考策略锚定，通过组内平移不变性以及隐式KL正则化来稳定训练；(iii) 通过Plackett-Luce分布进行列表式偏好建模。我们证明了DPO、Bradley-Terry目标函数以及Top-1-vs-Rest形式是ADPO的特例。ADPO产生了三种实用变体：成对锚定的Soft-DPO、带有原始奖励的列表式锚定Soft-DPO，以及基于KDE的列表式平滑方法，用于处理重尾噪声。在上下文老虎机（contextual bandits）中，锚定方法相比标准DPO将WinMass性能提升了38%-63%，而基于KDE的平滑方法在重尾污染情况下（污染程度为112%的相对提升）达到了0.68 vs 0.32的性能。在序列强化学习（如CartPole和LunarLander）中，锚定方法将噪声偏好性能提升了15%-29%，验证了从单步到多步设置的迁移能力。使用10到256参数模型进行的实验提供了明确的指导：对于干净或中等噪声情况，使用成对锚定的Soft-DPO；而对于极端污染情况，使用基于KDE的列表式ADPO。</p>
<h3 id="ComputerRL-Scaling-End-to-End-Online-Reinforcement-Learning-for-Computer-Use-Agents"><a href="#ComputerRL-Scaling-End-to-End-Online-Reinforcement-Learning-for-Computer-Use-Agents" class="headerlink" title="ComputerRL: Scaling End-to-End Online Reinforcement Learning for Computer Use Agents"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.14040v2">ComputerRL: Scaling End-to-End Online Reinforcement Learning for Computer Use Agents</a></h3><p><strong>Categories:</strong> Reinforcement Learning, Large Language Models</p>
<p>我们引入了ComputerRL，这是一个用于自主桌面智能的框架，使智能体能够熟练地操作复杂的数字工作空间。ComputerRL采用API-GUI范式，将程序化的API调用与直接的GUI交互统一起来，以解决机器智能体与以人类为中心的桌面环境之间固有的不匹配问题。对端到端强化学习（RL）训练进行扩展是提高和泛化处理多种桌面任务的关键；然而，由于在长时间训练过程中环境效率低下和不稳定，这一目标仍面临挑战。为支持可扩展且稳健的训练，我们开发了一种分布式RL基础设施，能够协调数千个并行虚拟桌面环境，从而加速大规模在线RL。此外，我们提出了Entropulse，这是一种交替进行强化学习和监督微调的训练策略，有效缓解了长时间训练过程中出现的熵崩溃问题。我们在开放模型GLM-4-9B-0414和GLM-4.1V-9B-Thinking上应用ComputerRL，并在OSWorld基准上进行了评估。AutoGLM-OS-9B达到了48.9%的新状态-of-the-art准确率，展示了桌面自动化中通用智能体的显著提升。我们的代码和新的OfficeWorld基准可在<a target="_blank" rel="noopener" href="https://github.com/thudm/ComputerRL%E8%8E%B7%E5%8F%96%E3%80%82%E8%AF%A5%E7%AE%97%E6%B3%95%E5%92%8C%E6%A1%86%E6%9E%B6%E8%A2%AB%E7%94%A8%E4%BA%8E%E6%9E%84%E5%BB%BAAutoGLM%EF%BC%88Liu%E7%AD%89%EF%BC%8C2024b%EF%BC%89%E3%80%82">https://github.com/thudm/ComputerRL获取。该算法和框架被用于构建AutoGLM（Liu等，2024b）。</a></p>
<h3 id="Taming-the-Judge-Deconflicting-AI-Feedback-for-Stable-Reinforcement-Learning"><a href="#Taming-the-Judge-Deconflicting-AI-Feedback-for-Stable-Reinforcement-Learning" class="headerlink" title="Taming the Judge: Deconflicting AI Feedback for Stable Reinforcement Learning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.15514v2">Taming the Judge: Deconflicting AI Feedback for Stable Reinforcement Learning</a></h3><p><strong>Categories:</strong> Reinforcement Learning, Large Language Models</p>
<p>利用大型语言模型（LLM）法官的反馈对语言模型进行对齐，为人类标注提供了一种可扩展的替代方案，但其判断不一致性却会破坏强化学习的稳定性。尽管以往的研究主要关注法官的准确性，但逻辑一致性，特别是偏好循环这一关键问题，却一直未得到充分解决。为了解决这一问题，本文提出了一种端到端的框架，系统地检测并解决强化学习训练过程中这些不一致性。我们的框架包含两个核心贡献：冲突检测率（Conflict Detection Rate, CDR），一种用于量化判断冲突的新颖指标；以及去冲突图奖励（Deconflicted Graph Rewards, DGR），一种信号净化框架，能够在策略优化之前消除循环。DGR从原始判断中构建偏好图，将其转换为无冲突的有向无环图（DAG），并生成与任何策略优化器兼容的逻辑一致的奖励信号。实验表明，我们的框架在训练稳定性与模型性能方面显著优于现有强基线方法，确立了逻辑一致性作为AI反馈的一个关键且现已可解决的维度。我们的方法代码可在<a target="_blank" rel="noopener" href="https://github.com/modelscope/RM-Gallery%E4%B8%8A%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/modelscope/RM-Gallery上获取。</a></p>
<h3 id="NTKMTL-Mitigating-Task-Imbalance-in-Multi-Task-Learning-from-Neural-Tangent-Kernel-Perspective"><a href="#NTKMTL-Mitigating-Task-Imbalance-in-Multi-Task-Learning-from-Neural-Tangent-Kernel-Perspective" class="headerlink" title="NTKMTL: Mitigating Task Imbalance in Multi-Task Learning from Neural Tangent Kernel Perspective"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18258v1">NTKMTL: Mitigating Task Imbalance in Multi-Task Learning from Neural Tangent Kernel Perspective</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>多任务学习（Multi-Task Learning，MTL）允许一个模型同时学习多个任务，通过任务间的知识迁移来提升泛化能力，并已在多个领域得到广泛应用。然而，任务不平衡仍然是MTL中的一个主要挑战。尽管平衡不同任务的收敛速度是解决该问题的有效方法，但在复杂的MTL系统中，准确刻画多个任务的训练动态和收敛速度仍然是极具挑战性的。为此，我们尝试利用神经切线核（Neural Tangent Kernel，NTK）理论来分析MTL中的训练动态，并提出了一种新的MTL方法NTKMTL。具体而言，我们引入了适用于MTL的扩展NTK矩阵，并采用谱分析方法平衡多个任务的收敛速度，从而缓解任务不平衡问题。基于共享表示的近似，我们进一步提出了NTKMTL-SR方法，在提升训练效率的同时保持了具有竞争力的性能。大量实验表明，我们的方法在包括多任务监督学习和多任务强化学习在内的多种基准数据集上均取得了最先进的性能。源代码可在<a target="_blank" rel="noopener" href="https://github.com/jianke0604/NTKMTL%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/jianke0604/NTKMTL获取。</a></p>
<h3 id="Towards-Agentic-Self-Learning-LLMs-in-Search-Environment"><a href="#Towards-Agentic-Self-Learning-LLMs-in-Search-Environment" class="headerlink" title="Towards Agentic Self-Learning LLMs in Search Environment"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.14253v2">Towards Agentic Self-Learning LLMs in Search Environment</a></h3><p><strong>Categories:</strong> Reinforcement Learning, Large Language Models</p>
<p>我们研究自我学习是否可以在不依赖人工整理的数据集或预定义的基于规则的奖励机制的情况下，扩展基于大语言模型（LLM）的智能体。通过在搜索智能体环境中的受控实验，我们识别出可扩展智能体训练的两个关键决定因素：奖励信号的来源和智能体任务数据的规模。我们发现，生成式奖励模型（GRM）产生的奖励在开放领域学习中优于刚性的基于规则的信号，同时与策略共同进化可以进一步提升性能。增加智能体任务数据的量——即使这些数据是合成生成的——显著增强了智能体的能力。基于这些发现，我们提出了<strong>智能体自我学习</strong>（Agentic Self-Learning，简称ASL），这是一个完全闭环的、多角色强化学习框架，它在一个共享工具环境和LLM主干中统一了任务生成、策略执行和评估。ASL协调一个提示生成器、一个策略模型和一个生成式奖励模型，形成一个更难的任务设定、更精确的验证和更强的求解能力的良性循环。实验证明，ASL实现了持续的、一轮接一轮的提升，超越了诸如Search-R1等强大的RLVR基线方法（这些方法在性能趋于稳定甚至退化），并且在零标记数据条件下仍能持续改进，表明其具有优越的样本效率和鲁棒性。我们进一步表明，GRM的验证能力是主要瓶颈：如果被冻结，就会导致奖励黑客行为并阻碍进展；在不断演化的数据分布上持续训练GRM可以缓解这一问题，而在后期阶段注入少量真实验证数据可以提高性能上限。本工作确立了奖励来源和数据规模是开放领域智能体学习的关键杠杆，并展示了多角色共同进化对可扩展、自我改进智能体的有效性。本论文的数据和代码已发布在<br><a target="_blank" rel="noopener" href="https://github.com/forangel2014/Towards-Agentic-Self-Learning">https://github.com/forangel2014/Towards-Agentic-Self-Learning</a></p>
<h3 id="Infinity-Parser-Layout-Aware-Reinforcement-Learning-for-Scanned-Document-Parsing"><a href="#Infinity-Parser-Layout-Aware-Reinforcement-Learning-for-Scanned-Document-Parsing" class="headerlink" title="Infinity Parser: Layout Aware Reinforcement Learning for Scanned Document Parsing"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2506.03197v3">Infinity Parser: Layout Aware Reinforcement Learning for Scanned Document Parsing</a></h3><p><strong>Categories:</strong> Reinforcement Learning,Vision-Language Models</p>
<p>将扫描文档自动解析为结构丰富、可被机器读取的格式，仍然是文档人工智能（Document AI）中的关键瓶颈。由于传统的多阶段处理流程存在误差传播问题，并且对多样化的文档布局适应性有限。我们提出了 layoutRL，这是一个端到端的强化学习框架，通过优化归一化编辑距离、段落数量准确率以及阅读顺序保持度的综合奖励，训练模型显式地理解文档布局。借助我们最新发布的数据集 Infinity-Doc-55K，该数据集结合了55,000条高保真度的合成扫描文档解析数据和经过专家筛选的真实世界文档，我们基于视觉-语言模型构建了一个名为 Infinity-Parser 的解析器，并在其中实现了 layoutRL。在英文和中文的OCR、表格和公式提取以及阅读顺序检测基准测试中，Infinity-Parser 在准确率和结构保真度方面均取得了新的最先进性能，优于专门的处理流程和通用的视觉-语言模型。我们将公开发布我们的代码和数据集，以加快在鲁棒文档理解方面的研究进展。</p>
<h3 id="A-Generalized-Bisimulation-Metric-of-State-Similarity-between-Markov-Decision-Processes-From-Theoretical-Propositions-to-Applications"><a href="#A-Generalized-Bisimulation-Metric-of-State-Similarity-between-Markov-Decision-Processes-From-Theoretical-Propositions-to-Applications" class="headerlink" title="A Generalized Bisimulation Metric of State Similarity between Markov Decision Processes: From Theoretical Propositions to Applications"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2509.18714v2">A Generalized Bisimulation Metric of State Similarity between Markov Decision Processes: From Theoretical Propositions to Applications</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>仿射模拟度量（BSM）是一种在马尔可夫决策过程（MDP）中计算状态相似性的强大工具，揭示了在BSM中更接近的状态具有更相似的最优值函数。尽管BSM已被成功应用于强化学习（RL）中，如状态表示学习和策略探索等任务，但其在多MDP场景（如策略迁移）中的应用仍面临挑战。先前的研究尝试将BSM推广到MDP对之间，但由于对其数学性质缺乏严谨的分析，限制了进一步的理论进展。在本文中，我们正式建立了一种用于MDP对之间的广义仿射模拟度量（GBSM），并严格证明了其具有三个基本性质：GBSM对称性、跨MDP三角不等式以及在相同状态空间上的距离界。利用这些性质，我们从理论上分析了MDP中的策略迁移、状态聚合和基于采样的估计，获得了严格优于标准BSM所推导的界限的显式界。此外，GBSM还提供了估计的闭式样本复杂度，改进了基于BSM的现有渐近结果。数值结果验证了我们的理论发现，并展示了GBSM在多MDP场景中的有效性。</p>
<h3 id="Balancing-Act-Prioritization-Strategies-for-LLM-Designed-Restless-Bandit-Rewards"><a href="#Balancing-Act-Prioritization-Strategies-for-LLM-Designed-Restless-Bandit-Rewards" class="headerlink" title="Balancing Act: Prioritization Strategies for LLM-Designed Restless Bandit Rewards"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2408.12112v6">Balancing Act: Prioritization Strategies for LLM-Designed Restless Bandit Rewards</a></h3><p><strong>Categories:</strong> Reinforcement Learning, Large Language Models</p>
<p>大型语言模型（LLMs）正越来越多地用于基于人类偏好的强化学习（RL）中设计奖励函数。我们专注于在“ restless 多臂老虎机”（Restless Multi-Armed Bandits）框架下，由LLM设计的奖励函数。这一框架用于在多个智能体之间分配有限资源。在诸如公共卫生等应用中，这种方法使基层卫生工作者能够根据社区需求，定制自动化的资源分配决策。在存在多个智能体的情况下，根据人类偏好调整奖励函数可能会对不同的子群体产生截然不同的影响，从而导致复杂的权衡和多目标资源分配问题。我们首次提出了一种原理性的方法，称为“社会选择语言模型”（Social Choice Language Model），用于处理一般多智能体规划器（特别是restless老虎机）中LLM设计奖励函数所带来的权衡问题。我们模型的创新之处在于一个透明且可配置的选取组件，称为“仲裁者”（adjudicator），该组件独立于LLM，通过用户选择的社会福利函数来控制复杂的权衡。我们的实验表明，与纯粹基于LLM的方法相比，我们的模型能够可靠地选择出更有效、更一致且更平衡的奖励函数。</p>
<h3 id="Generalized-Principal-Agent-Problem-with-a-Learning-Agent"><a href="#Generalized-Principal-Agent-Problem-with-a-Learning-Agent" class="headerlink" title="Generalized Principal-Agent Problem with a Learning Agent"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2402.09721v7">Generalized Principal-Agent Problem with a Learning Agent</a></h3><p><strong>Categories:</strong> Reinforcement Learning</p>
<p>在诸如斯塔克尔伯格博弈（Stackelberg games）、契约设计和贝叶斯说服等经典委托-代理问题中，代理人会根据委托人的既定策略做出最优反应。我们研究了在委托人没有承诺能力的情况下，代理人使用算法学习以应对委托人时的重复广义委托-代理问题。我们将该问题简化为一个一次性问题，其中代理人近似最优地做出反应，并证明了以下结论：<br>（1）如果代理人使用上下文无关的无遗憾学习算法，其遗憾为 $ \mathrm{Reg}(T) $，那么委托人可以保证其效用至少为 $ U^* - \Theta\left(\sqrt{\frac{\mathrm{Reg}(T)}{T}}\right) $，其中 $ U^* $ 是在经典模型中代理人最优反应时委托人的最优效用。<br>（2）如果代理人使用上下文无关的无交换遗憾学习算法，其交换遗憾为 $ \mathrm{SReg}(T) $，那么委托人无法获得超过 $ U^* + O\left(\frac{\mathrm{SReg}(T)}{T}\right) $ 的效用。<br>（3）此外，如果代理人使用基于均值的学习算法（这类算法可能是无遗憾的，但不是无交换遗憾的），那么委托人有时可以显著优于 $ U^* $。<br>这些结果不仅改进了以往关于斯塔克尔伯格博弈和契约设计的研究，还为具有学习代理人的贝叶斯说服问题，以及所有代理人没有私人信息的广义委托-代理问题带来了新的结论。</p>
<h2 id="Robotics"><a href="#Robotics" class="headerlink" title="Robotics"></a>Robotics</h2><h3 id="A-Cross-Environment-and-Cross-Embodiment-Path-Planning-Framework-via-a-Conditional-Diffusion-Model"><a href="#A-Cross-Environment-and-Cross-Embodiment-Path-Planning-Framework-via-a-Conditional-Diffusion-Model" class="headerlink" title="A Cross-Environment and Cross-Embodiment Path Planning Framework via a Conditional Diffusion Model"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19128v1">A Cross-Environment and Cross-Embodiment Path Planning Framework via a Conditional Diffusion Model</a></h3><p><strong>Categories:</strong> Robotics, Reinforcement Learning</p>
<p>在高维复杂环境下的机器人系统路径规划需要高效、安全且能够适应不同环境和硬件。传统方法计算时间较长且需要大量参数调整，而先前的基于学习的方法仍然难以有效泛化。本研究的主要目标是开发一种路径规划框架，该框架能够在无需重新训练的情况下，泛化到未见过的环境和新型机器人操作器。我们提出了GADGET（可泛化且自适应的扩散引导环境感知轨迹生成），这是一种基于扩散模型的规划方法，它根据体素化的场景表示以及起始和目标配置生成关节空间轨迹。GADGET的关键创新在于其混合双条件机制，该机制结合了通过学习场景编码实现的无分类器引导（classifier-free guidance）与基于分类器的控制屏障函数（Control Barrier Function, CBF）安全形状设计，将环境感知和实时避障直接整合到去噪过程中。这种设计支持在无需重新训练的情况下，零样本迁移至新环境和机器人实体。实验结果表明，GADGET在球形障碍物、拣取和货架环境中实现了高成功率和低碰撞强度，CBF引导进一步提高了安全性。此外，比较评估表明，GADGET在采样方法和基于学习的基线方法上均表现出强劲的性能。此外，GADGET能够在Franka Panda、Kinova Gen3（6&#x2F;7自由度）和UR5机器人之间进行迁移，并且在Kinova Gen3机器人上的物理执行验证了其在现实场景中生成安全、无碰撞轨迹的能力。</p>
<h3 id="nabla-SDF-Learning-Euclidean-Signed-Distance-Functions-Online-with-Gradient-Augmented-Octree-Interpolation-and-Neural-Residual"><a href="#nabla-SDF-Learning-Euclidean-Signed-Distance-Functions-Online-with-Gradient-Augmented-Octree-Interpolation-and-Neural-Residual" class="headerlink" title="$\nabla$-SDF: Learning Euclidean Signed Distance Functions Online with Gradient-Augmented Octree Interpolation and Neural Residual"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18999v1">$\nabla$-SDF: Learning Euclidean Signed Distance Functions Online with Gradient-Augmented Octree Interpolation and Neural Residual</a></h3><p><strong>Categories:</strong> Robotics</p>
<p>从点云数据中估计有符号距离函数（SDF）已被证明可以提升许多机器人自主能力，包括定位、地图构建、运动规划和控制。支持在线和大规模SDF重建的方法通常依赖离散的体素数据结构，这会影响SDF估计的连续性和可微性。最近，使用隐式特征的神经网络方法已经展示了高保真度和可微的SDF重建，但它们往往效率较低，在大规模环境中容易出现灾难性遗忘和内存限制，并且通常受限于截断的SDF。本文提出了一种混合方法∇-SDF，该方法结合了从梯度增强的八叉树插值中获得的显式先验和隐式神经残差。我们的方法实现了非截断（欧几里得）的SDF重建，在计算和内存效率方面与体素方法相当，同时在可微性和准确性方面与神经网络方法相当。大量实验表明，\methodname{}在准确性和效率方面优于现有技术，为机器人和计算机视觉中的下游任务提供了一种可扩展的解决方案。</p>
<h3 id="Interpretable-Decision-Making-for-End-to-End-Autonomous-Driving"><a href="#Interpretable-Decision-Making-for-End-to-End-Autonomous-Driving" class="headerlink" title="Interpretable Decision-Making for End-to-End Autonomous Driving"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.18898v3">Interpretable Decision-Making for End-to-End Autonomous Driving</a></h3><p><strong>Categories:</strong> Robotics, Reinforcement Learning</p>
<p>可信的AI是自动驾驶车辆广泛部署的必要条件。尽管端到端方法直接从原始数据中生成控制指令，但解释这些决策仍然具有挑战性，尤其是在复杂的城区场景中。这主要是由于非常深的神经网络具有非线性的决策边界，使得理解AI驱动决策背后的逻辑变得困难。本文提出了一种方法，在优化自动驾驶控制指令的同时提高模型的可解释性。为此，我们提出了促进模型可解释性的损失函数，通过生成稀疏且局部化的特征图来实现。特征激活使我们能够解释哪些图像区域对预测的控制指令产生了贡献。我们在特征提取步骤上进行了全面的消融研究，并在CARLA基准上验证了我们的方法。我们还证明，我们的方法提高了可解释性，这与减少违规行为相关，从而实现了更安全、高性能的驾驶模型。值得注意的是，我们的单目、非集成模型在保证可解释性的前提下，超越了CARLA排行榜上表现最好的方法，实现了更低的违规评分和最高的路线完成率。</p>
<h3 id="VIKI-R-Coordinating-Embodied-Multi-Agent-Cooperation-via-Reinforcement-Learning"><a href="#VIKI-R-Coordinating-Embodied-Multi-Agent-Cooperation-via-Reinforcement-Learning" class="headerlink" title="VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2506.09049v2">VIKI-R: Coordinating Embodied Multi-Agent Cooperation via Reinforcement Learning</a></h3><p><strong>Categories:</strong> Robotics, Vision-Language Models, Reinforcement Learning</p>
<p>在动态环境中协调多个具身智能体仍然是人工智能领域的一个核心挑战，需要同时具备基于感知的推理能力和可扩展的协作策略。虽然近期的研究已经开始利用大型语言模型（LLMs）进行多智能体规划，但少数研究开始探索视觉语言模型（VLMs）用于视觉推理。然而，这些基于VLM的方法在支持多样化具身类型方面仍存在局限性。在本文中，我们引入了VIKI-Bench，这是首个专门针对具身多智能体协作的分层基准测试平台，包含三个结构化层级：智能体激活、任务规划和轨迹感知。VIKI-Bench包含多样化的机器人具身形式、多视角视觉观测以及结构化的监督信号，以评估基于视觉输入的推理能力。为了展示VIKI-Bench的实用性，我们提出了一种名为VIKI-R的两阶段框架，该框架首先使用思维链（Chain-of-Thought）标注的示范数据对预训练的视觉语言模型（VLM）进行微调，随后在多层级奖励信号下进行强化学习。我们的大量实验表明，VIKI-R在所有任务层级上均显著优于基线方法。此外，我们还表明，强化学习能够促使异构智能体之间形成组合性协作模式。总体而言，VIKI-Bench和VIKI-R为推进具身人工智能系统中的多智能体、视觉驱动协作提供了一个统一的测试平台和方法。</p>
<h3 id="Deep-Learning-Based-Control-Optimization-for-Glass-Bottle-Forming"><a href="#Deep-Learning-Based-Control-Optimization-for-Glass-Bottle-Forming" class="headerlink" title="Deep Learning-Based Control Optimization for Glass Bottle Forming"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18412v1">Deep Learning-Based Control Optimization for Glass Bottle Forming</a></h3><p><strong>Categories:</strong> Robotics</p>
<p>在玻璃瓶制造过程中，对成型机进行精确控制对于确保产品质量和减少缺陷至关重要。本研究提出了一种基于深度学习的控制算法，旨在优化实际生产环境中的成型工艺。通过使用来自活跃制造工厂的真实运行数据，我们的神经网络能够根据当前的生产设置预测参数变化的影响。通过专门设计的反演机制，该算法可以识别出实现所需玻璃坯料特性的最佳机器设置。在多个生产线上历史数据集上的实验结果表明，所提出的方法取得了令人鼓舞的成果，表明其在提高工艺稳定性、减少废品和提升产品一致性方面具有巨大潜力。这些结果凸显了深度学习在玻璃制造工艺控制中的应用潜力。</p>
<h3 id="PGTT-Phase-Guided-Terrain-Traversal-for-Perceptive-Legged-Locomotion"><a href="#PGTT-Phase-Guided-Terrain-Traversal-for-Perceptive-Legged-Locomotion" class="headerlink" title="PGTT: Phase-Guided Terrain Traversal for Perceptive Legged Locomotion"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18348v1">PGTT: Phase-Guided Terrain Traversal for Perceptive Legged Locomotion</a></h3><p><strong>Categories:</strong> Robotics,Reinforcement Learning</p>
<p>当前最先进的感知强化学习控制器用于腿部机器人，要么（i）施加基于振荡器或逆运动学（IK）的步态先验，从而限制动作空间，给策略优化引入偏差，降低其在不同机器人形态间的适应性；要么（ii）以“盲”模式运行，难以预判后腿地形，并且对噪声敏感。在本文中，我们提出了“相位引导地形穿越”（Phase-Guided Terrain Traversal, PGT）方法，这是一种感知感知的深度强化学习（deep-RL）方法，通过仅通过奖励塑造来强制步态结构，从而克服上述限制，相比基于振荡器&#x2F;逆运动学的行动先验，减少了策略学习中的归纳偏差。PGT将每条腿的相位编码为一个三次样条曲线（cubic Hermite spline），根据局部高度图统计信息调整摆动高度，并引入摆动相位接触惩罚，同时策略直接在关节空间中运行，支持形态无关的部署。PGT在MuJoCo（MJX）中通过程序生成的类似楼梯地形进行训练，采用课程学习和领域随机化方法，PGT在受到推力干扰的情况下表现最佳（中位数提升+7.5%，优于次优方法），在离散障碍物上表现也提升+9%，且速度跟踪性能相当，并且比强大的端到端基线方法快约两倍即可收敛至有效策略。我们在Unitree Go2机器人上使用实时LiDAR高度图转换管道验证了PGT，并报告了使用相同超参数在ANYmal-C上获得的初步结果。这些发现表明，地形自适应、相位引导的奖励塑造是一种简单且通用的机制，可用于平台间的稳健感知运动。</p>
<h3 id="Improving-Human-AI-Coordination-through-Online-Adversarial-Training-and-Generative-Models"><a href="#Improving-Human-AI-Coordination-through-Online-Adversarial-Training-and-Generative-Models" class="headerlink" title="Improving Human-AI Coordination through Online Adversarial Training and Generative Models"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2504.15457v4">Improving Human-AI Coordination through Online Adversarial Training and Generative Models</a></h3><p><strong>Categories:</strong> Robotics,Reinforcement Learning</p>
<p>能够与各种人类进行合作是许多具有经济价值的AI任务的重要组成部分，从家庭机器人到自动驾驶。然而，要使AI系统泛化到新型人类，需要使用能够捕捉人类行为多样性的数据进行训练。对抗性训练是一种有前景的方法，它允许动态生成数据并确保智能体具有鲁棒性。这种方法创建了一个反馈循环，智能体的性能会直接影响新的对抗性数据的生成，这些数据可以立即用于训练智能体。然而，对抗性训练在合作任务中难以应用；我们该如何训练一个对抗性合作者呢？我们提出了一种新颖的策略，将预训练的生成模型与对抗性训练结合，以最大化遗憾（regret），从而模拟有效的合作智能体策略。我们将该方法称为GOAT（生成式在线对抗训练）。在这个框架中，GOAT会动态地在生成模型的潜在空间中搜索协调策略，其中学习策略（即合作者智能体）表现不佳。GOAT通过让合作者智能体接触各种具有挑战性的交互场景，从而实现更好的泛化能力。我们通过保持生成模型冻结来维持现实的协调策略，从而避免对抗性利用。我们使用真实的人类伙伴对GOAT进行了评估，实验结果表明在Overcooked基准测试中表现出最先进的性能，突显了其在泛化到多样化人类行为方面的有效性。</p>
<h3 id="MoMaGen-Generating-Demonstrations-under-Soft-and-Hard-Constraints-for-Multi-Step-Bimanual-Mobile-Manipulation"><a href="#MoMaGen-Generating-Demonstrations-under-Soft-and-Hard-Constraints-for-Multi-Step-Bimanual-Mobile-Manipulation" class="headerlink" title="MoMaGen: Generating Demonstrations under Soft and Hard Constraints for Multi-Step Bimanual Mobile Manipulation"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18316v1">MoMaGen: Generating Demonstrations under Soft and Hard Constraints for Multi-Step Bimanual Mobile Manipulation</a></h3><p><strong>Categories:</strong> Robotics,Reinforcement Learning</p>
<p>从大规模、多样化的示例中进行模仿学习已被证明在训练机器人方面非常有效，但收集这类数据成本高且耗时。这一挑战在涉及多步双臂移动操作的任务中尤为突出，因为人类必须远程操控移动底盘和两个高自由度的机械臂。此前的自动化数据生成框架通过在仿真环境中扩充少量人类示例来处理静态双臂操作，但由于两个关键挑战，它们在移动场景中表现不足：(1) 确定底盘的位置以确保可到达性，以及 (2) 安排相机位置以提供足够的视觉信息，从而支持视觉运动策略。为了解决这些问题，我们引入了 MoMaGen，它将数据生成建模为一个带约束的优化问题，强制执行硬约束（如可到达性），同时平衡软约束（如导航过程中的可见性）。这种建模方法扩展了之前的方法，并为未来的方法提供了理论基础。我们在四个多步双臂移动操作任务上评估了 MoMaGen，结果表明它生成的数据集比现有方法更具多样性。借助这种多样性，MoMaGen 可以仅从一个示例演示中训练出成功的模仿学习策略，并且这些策略只需最少的 40 个真实世界演示即可微调，从而实现在物理机器人硬件上的部署。更多细节请访问我们的项目页面：momagen.github.io。</p>
<h3 id="EVER-Edge-Assisted-Auto-Verification-for-Mobile-MR-Aided-Operation"><a href="#EVER-Edge-Assisted-Auto-Verification-for-Mobile-MR-Aided-Operation" class="headerlink" title="EVER: Edge-Assisted Auto-Verification for Mobile MR-Aided Operation"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18224v1">EVER: Edge-Assisted Auto-Verification for Mobile MR-Aided Operation</a></h3><p><strong>Categories:</strong> Robotics</p>
<p>混合现实（MR）辅助操作通过将数字对象叠加在物理世界中，为操作过程提供更加沉浸和直观的体验。一个主要挑战在于，需要精确且快速地自动验证用户是否遵循了MR的指导，这通常通过比较每次操作前后的帧图像来实现。操作前的帧包含虚拟引导对象，而操作后的帧则包含其对应的物理对象。现有的方法由于3D建模不完善或光照估计不准确，难以有效考虑物理对象与虚拟对象之间的差异。本文提出EVER：一个面向移动MR辅助操作的边缘辅助自动验证系统。与传统的基于帧的相似性比较不同，EVER利用分割模型和渲染管线，针对包含物理部件的帧及其对应的虚拟部件帧的特殊属性进行优化；它采用基于交并比（IoU）度量的阈值策略，以实现准确的自动验证。为了确保快速自动验证和低能耗，EVER将计算密集型任务卸载到边缘服务器。通过在公开数据集和实际应用数据集上的全面评估，EVER在100毫秒内实现了超过90%的验证准确率（显著快于人类平均反应时间约273毫秒），同时相比无自动验证的系统，仅消耗极少的额外计算资源和能量。</p>
<h2 id="Vision-Language-Models"><a href="#Vision-Language-Models" class="headerlink" title="Vision-Language Models"></a>Vision-Language Models</h2><h3 id="PoSh-Using-Scene-Graphs-To-Guide-LLMs-as-a-Judge-For-Detailed-Image-Descriptions"><a href="#PoSh-Using-Scene-Graphs-To-Guide-LLMs-as-a-Judge-For-Detailed-Image-Descriptions" class="headerlink" title="PoSh: Using Scene Graphs To Guide LLMs-as-a-Judge For Detailed Image Descriptions"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19060v1">PoSh: Using Scene Graphs To Guide LLMs-as-a-Judge For Detailed Image Descriptions</a></h3><p><strong>Categories:</strong> Vision-Language Models, Large Language Models</p>
<p>尽管视觉-语言模型（VLMs）已经能够进行详细的图像描述，但评估仍然面临挑战。标准指标（例如CIDEr、SPICE）最初是为简短文本设计的，并被调优以识别如今已不常见的错误，例如物体识别错误。相比之下，长文本需要对属性和关系的依赖性具有敏感性，并且需要能够将错误定位到特定文本片段的评分机制。在本文中，我们提出了PoSh，这是一种用于详细图像描述的评估指标，它利用场景图作为结构化的评分标准，引导“作为评委的LLM”，从而产生基于细粒度错误（例如组合理解错误）的综合评分。PoSh具有可复制性和可解释性，比现有的指标（包括“作为评委的GPT4o”）更接近人类评分者的判断。为了验证PoSh，我们引入了一个具有挑战性的新数据集DOCENT。这个新颖的基准数据集包含艺术品，并与专家撰写的参考文本、模型生成的描述以及艺术史学生提供的细致和粗略的质量评估相结合。因此，DOCENT使得在新的挑战性领域中，能够评估详细图像描述指标本身以及详细图像描述能力。我们证明，PoSh在DOCENT数据集上与人类评分的相关性比最佳的开源替代方案高出+0.05（Spearman ρ），并且对图像类型具有鲁棒性（使用CapArena，一个现有的网络图像数据集），并且是一种有效的奖励函数，优于标准的监督微调方法。然后，我们使用PoSh评估了在DOCENT中描述绘画、素描和雕像的开放模型和闭合模型的性能，并发现基础模型在处理具有丰富场景动态的图像时难以实现全面且无错误的覆盖，从而提出了一个具有挑战性的新任务，以衡量VLM的发展。通过PoSh和DOCENT，我们希望推动诸如辅助文本生成等重要领域的发展。</p>
<h3 id="Robust-Driving-QA-through-Metadata-Grounded-Context-and-Task-Specific-Prompts"><a href="#Robust-Driving-QA-through-Metadata-Grounded-Context-and-Task-Specific-Prompts" class="headerlink" title="Robust Driving QA through Metadata-Grounded Context and Task-Specific Prompts"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19001v1">Robust Driving QA through Metadata-Grounded Context and Task-Specific Prompts</a></h3><p><strong>Categories:</strong> Vision-Language Models, Large Language Models</p>
<p>我们提出了一种用于自动驾驶的两阶段视觉-语言问答系统，该系统能够回答高级别的感知、预测和规划问题。在第一阶段，一个大型多模态大语言模型（Qwen2.5-VL-32B）以六路摄像头输入、一个短时间窗口的历史数据以及结合少量示例的链式思维提示作为输入。通过自一致性集成（多个采样的推理链），进一步提高了答案的可靠性。在第二阶段，我们通过添加nuScenes场景元数据（如物体标注、本车状态等）和针对不同任务的类别特定问答指令（如感知、预测、规划任务的独立提示）来增强提示。在驾驶问答基准测试中，我们的方法显著优于基线Qwen2.5模型。例如，在第一阶段使用5帧历史数据和10个示例提示，整体准确率达到65.1%（与零示例提示相比为62.61%）；应用自一致性后，准确率提高到66.85%。第二阶段的整体准确率为67.37%。值得注意的是，系统在严重视觉损坏的情况下仍能保持96%的准确率。这些结果表明，精心设计的提示和上下文锚定能够显著提升预训练视觉-语言模型在高级驾驶问答任务中的表现。</p>
<h3 id="Bee-A-High-Quality-Corpus-and-Full-Stack-Suite-to-Unlock-Advanced-Fully-Open-MLLMs"><a href="#Bee-A-High-Quality-Corpus-and-Full-Stack-Suite-to-Unlock-Advanced-Fully-Open-MLLMs" class="headerlink" title="Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.13795v2">Bee: A High-Quality Corpus and Full-Stack Suite to Unlock Advanced Fully Open MLLMs</a></h3><p><strong>Categories:</strong> Vision-Language Models, Large Language Models</p>
<p>目前，完全开源的多模态大语言模型（MLLMs）在性能上仍落后于专有模型，主要原因是监督微调（SFT）数据质量存在显著差距。现有的开源数据集常常受到广泛噪声的困扰，并且在复杂推理数据（如思维链（Chain-of-Thought, CoT）数据）方面存在严重不足，这阻碍了先进模型能力的发展。为了解决这些挑战，我们的工作做出了以下三个主要贡献。首先，我们提出了Honey-Data-15M，这是一个包含约1500万个问答对的新SFT数据集，经过多种清洗技术处理，并通过一种新颖的双层级（短与长）CoT增强策略进行优化。其次，我们引入了数据整理流程HoneyPipe及其底层框架DataStudio，为社区提供了一种透明且可调整的数据整理方法，超越了静态数据集发布的方式。最后，为了验证我们的数据集和流程，我们在Honey-Data-15M上训练了一个80亿参数的模型Bee-8B。实验表明，Bee-8B在完全开源MLLMs中达到了新的最先进水平（SOTA），其性能在某些情况下甚至超过了最近的半开源模型，如InternVL3.5-8B。我们的工作为社区提供了多种基础资源，包括：Honey-Data-15M语料库；涵盖HoneyPipe和DataStudio的完整工具链；训练指南；评估框架；以及模型权重。这一努力表明，对数据质量的系统关注是开发高性能完全开源MLLMs的关键途径，使其能够与半开源模型展开激烈竞争。</p>
<h3 id="AstroMMBench-A-Benchmark-for-Evaluating-Multimodal-Large-Language-Models-Capabilities-in-Astronomy"><a href="#AstroMMBench-A-Benchmark-for-Evaluating-Multimodal-Large-Language-Models-Capabilities-in-Astronomy" class="headerlink" title="AstroMMBench: A Benchmark for Evaluating Multimodal Large Language Models Capabilities in Astronomy"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.00063v2">AstroMMBench: A Benchmark for Evaluating Multimodal Large Language Models Capabilities in Astronomy</a></h3><p><strong>Categories:</strong> Vision-Language Models, Large Language Models</p>
<p>天文学图像解释在将多模态大语言模型（MLLMs）应用于专门的科学研究任务时提出了重大挑战。现有的基准测试主要关注通用的多模态能力，但未能捕捉天文学数据的复杂性。为弥合这一差距，我们推出了AstroMMBench，这是首个专门用于评估MLLMs在天文学图像理解能力的综合性基准测试。AstroMMBench包含六个天体物理子领域的621道多项选择题，由15位领域专家精心筛选和审阅，以确保其质量和相关性。我们使用AstroMMBench对25个多样化的MLLMs进行了广泛评估，其中包括22个开源模型和3个闭源模型。结果显示，Ovis2-34B模型在整体准确率上达到了最高水平（70.5%），其表现甚至优于一些强大的闭源模型。在六个天体物理子领域中，模型的性能存在差异，其中在宇宙学和高能天体物理等领域的表现尤为困难，而在仪器设备和太阳天体物理等领域则相对表现更好。这些发现突显了像AstroMMBench这样针对特定领域的基准测试在关键评估MLLM性能以及指导其在科学应用中的定向发展中的重要作用。AstroMMBench为AI与天文学交叉领域的进步提供了一个基础资源和动态工具。</p>
<h3 id="Seg-the-HAB-Language-Guided-Geospatial-Algae-Bloom-Reasoning-and-Segmentation"><a href="#Seg-the-HAB-Language-Guided-Geospatial-Algae-Bloom-Reasoning-and-Segmentation" class="headerlink" title="Seg the HAB: Language-Guided Geospatial Algae Bloom Reasoning and Segmentation"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18751v1">Seg the HAB: Language-Guided Geospatial Algae Bloom Reasoning and Segmentation</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>气候变化正在加剧有害藻华（HAB）的发生，特别是蓝藻，它们通过耗氧、释放毒素和破坏海洋生物多样性，威胁水生生态系统和人类健康。传统的监测方法，如人工水样采集，仍然耗费大量人力，且在空间和时间覆盖范围上存在局限。近年来，遥感领域的视觉-语言模型（VLMs）在实现可扩展的AI驱动解决方案方面展现出潜力，但仍在图像推理和藻华严重程度量化方面面临挑战。在本研究中，我们提出了ALGae Observation and Segmentation（ALGOS），这是一种结合遥感图像理解与严重程度评估的藻华监测分割与推理系统。我们的方法通过GeoSAM辅助的人工评估，用于高质量分割掩膜的管理，并利用来自NASA的蓝藻人工标注数据集（Cyanobacteria Aggregated Manual Labels, CAML）对视觉语言模型进行微调，以实现严重程度预测。实验表明，ALGOS在分割和严重程度评估方面均表现出良好的性能，为实现实用化和自动化的蓝藻监测系统铺平了道路。</p>
<h3 id="UniPixel-Unified-Object-Referring-and-Segmentation-for-Pixel-Level-Visual-Reasoning"><a href="#UniPixel-Unified-Object-Referring-and-Segmentation-for-Pixel-Level-Visual-Reasoning" class="headerlink" title="UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2509.18094v2">UniPixel: Unified Object Referring and Segmentation for Pixel-Level Visual Reasoning</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>近年来，大型多模态模型（LMMs）取得了显著进展，展示了其作为通用多模态助手的卓越能力，尤其在全面理解图像和视频与语言之间的关系方面表现突出。然而，相比之下，对模型在像素级细粒度理解能力上的扩展研究却较少。这些模型被期望能够实现视觉信号与语言语义之间的像素级对齐。一些先前的研究已将LMMs应用于相关任务，如区域级描述生成和指代表达分割。然而，这些模型仅能独立完成指代或分割任务，无法将这些细粒度的感知能力整合到视觉推理中。为弥合这一差距，我们提出了UniPixel，这是一种能够灵活理解视觉提示输入并生成基于掩码响应的大型多模态模型。我们的模型通过无缝地将像素级感知能力与通用视觉理解能力相结合而独具特色。具体而言，UniPixel在接收到视觉提示后，按需生成相关掩码，并在推理过程中基于这些中间指针进行后续推理，从而实现细粒度的像素级推理。我们在涵盖多种任务的10个基准测试中验证了该方法的有效性，包括像素级的指代和分割以及图像和视频中的以对象为中心的理解。此外，我们还设计了一个新颖的PixelQA任务，该任务需要同时进行指代、分割和问答，以验证我们方法的灵活性。</p>
<h3 id="Think-with-3D-Geometric-Imagination-Grounded-Spatial-Reasoning-from-Limited-Views"><a href="#Think-with-3D-Geometric-Imagination-Grounded-Spatial-Reasoning-from-Limited-Views" class="headerlink" title="Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18632v1">Think with 3D: Geometric Imagination Grounded Spatial Reasoning from Limited Views</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>尽管视觉-语言模型（VLMs）在多种多模态任务中取得了显著进展，但从有限视角理解三维空间关系仍然是一个重大挑战。以往的推理方法通常依赖纯文本（如拓扑认知地图）或二维视觉线索。然而，其有限的表示能力阻碍了需要三维空间想象力的具体任务的性能。为了解决这一限制，我们提出了3DThinker框架，该框架在推理过程中能够有效地利用图像中蕴含的丰富几何信息，如同人类一样。我们的框架是首个在无需任何三维先验输入的情况下实现推理过程中三维思维的框架，并且不需要依赖显式标注的三维数据进行训练。具体而言，我们的训练分为两个阶段。首先，我们进行监督训练，将VLM在推理过程中生成的三维潜在表示与三维基础模型（如VGGT）的潜在表示对齐。然后，我们仅基于结果信号优化整个推理轨迹，从而优化底层的三维思维过程。在多个基准测试中的广泛实验表明，3DThinker持续优于强大的基线模型，并为将三维表示统一到多模态推理中提供了新的视角。我们的代码可在<a target="_blank" rel="noopener" href="https://github.com/zhangquanchen/3DThinker%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/zhangquanchen/3DThinker获取。</a></p>
<h3 id="VAR-Visual-Attention-Reasoning-via-Structured-Search-and-Backtracking"><a href="#VAR-Visual-Attention-Reasoning-via-Structured-Search-and-Backtracking" class="headerlink" title="VAR: Visual Attention Reasoning via Structured Search and Backtracking"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18619v1">VAR: Visual Attention Reasoning via Structured Search and Backtracking</a></h3><p><strong>Categories:</strong> Vision-Language Models, Large Language Models</p>
<p>尽管多模态大语言模型（MLLMs）在技术上取得了进展，但它们仍受到高幻觉倾向和对脆弱、线性推理过程的严重依赖的限制，导致在复杂任务中出现失败。为了解决这些问题，我们引入了一种名为“视觉注意力推理”（Visual Attention Reasoning，简称VAR）的新框架，将基于上下文的推理重新定义为在推理轨迹空间上的结构化搜索。VAR将推理过程分解为两个关键阶段：可追溯的证据锚定和基于搜索的思维链（Chain-of-Thought，简称CoT）生成，其中包含一种回溯机制用于自我校正。搜索过程由一个多维度奖励函数引导，该函数包含语义和几何自验证组件，对未忠实基于视觉输入的输出进行惩罚。我们对搜索策略进行了理论分析，验证了其以高概率找到正确解的能力。实验结果表明，我们的7B模型VAR-7B在全面的幻觉和安全性基准测试中达到了新的最先进水平，显著优于现有的开源模型，并在与领先的专有系统竞争时表现出强劲的性能。</p>
<h3 id="PICABench-How-Far-Are-We-from-Physically-Realistic-Image-Editing"><a href="#PICABench-How-Far-Are-We-from-Physically-Realistic-Image-Editing" class="headerlink" title="PICABench: How Far Are We from Physically Realistic Image Editing?"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.17681v2">PICABench: How Far Are We from Physically Realistic Image Editing?</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>图像编辑最近取得了显著进展。现代编辑模型已经能够遵循复杂的指令来对原始内容进行操作。然而，除了完成编辑指令之外，伴随的物理效果是生成真实感的关键。例如，删除一个物体的同时，也应删除其阴影、反射以及与周围物体的相互作用。不幸的是，现有的模型和基准测试主要关注指令完成，却忽视了这些物理效果。因此，目前我们距离实现物理真实的图像编辑还有多远？为了解答这个问题，我们引入了PICABench，该基准系统性地评估了大多数常见编辑操作（如添加、删除、属性更改等）在八个子维度（涵盖光学、力学和状态转换）上的物理真实感。我们进一步提出了PICAEval，这是一种可靠的评估协议，采用VLM作为裁判，并结合针对每个案例、区域级别的标注和问题。除了基准测试之外，我们还通过从视频中学习物理知识，探索有效的解决方案，并构建了一个训练数据集PICA-100K。在评估了大多数主流模型后，我们发现物理真实感仍然是一个具有大量研究空间的挑战性问题。我们希望我们的基准测试和提出的解决方案能够成为未来工作从简单的内容编辑向物理一致的真实感发展的一个基础。</p>
<h3 id="Zero-Shot-Vehicle-Model-Recognition-via-Text-Based-Retrieval-Augmented-Generation"><a href="#Zero-Shot-Vehicle-Model-Recognition-via-Text-Based-Retrieval-Augmented-Generation" class="headerlink" title="Zero-Shot Vehicle Model Recognition via Text-Based Retrieval-Augmented Generation"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18502v1">Zero-Shot Vehicle Model Recognition via Text-Based Retrieval-Augmented Generation</a></h3><p><strong>Categories:</strong> Vision-Language Models, Large Language Models</p>
<p>车辆品牌与型号识别（VMMR）是智能交通系统中的重要任务，但现有方法难以适应新发布的车型。对比语言-图像预训练（CLIP）提供了强大的视觉-文本对齐能力，但其固定的预训练权重在没有昂贵的图像特定微调的情况下会限制性能。我们提出了一种将视觉语言模型（VLM）与检索增强生成（RAG）相结合的流程，以支持基于文本推理的零样本识别。VLM将车辆图像转换为描述性属性，这些属性与文本特征数据库进行比对。相关条目被检索并结合描述生成提示，然后由语言模型（LM）推断出品牌和型号。这种设计避免了大规模重新训练，并通过添加新车型的文本描述实现快速更新。实验表明，所提出的方法在CLIP基线基础上识别性能提升了近20%，展示了RAG增强的语言模型推理在智慧城市应用中可扩展VMMR的潜力。</p>
<h3 id="MATRIX-Multimodal-Agent-Tuning-for-Robust-Tool-Use-Reasoning"><a href="#MATRIX-Multimodal-Agent-Tuning-for-Robust-Tool-Use-Reasoning" class="headerlink" title="MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.08567v3">MATRIX: Multimodal Agent Tuning for Robust Tool-Use Reasoning</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>视觉语言模型（VLMs）正越来越多地被用作控制器，通过访问外部工具进行复杂推理和决策，但其效果仍受到高质量多模态轨迹稀缺以及手动标注成本的限制。我们提出了一种以视觉为中心的代理调优框架，该框架能够自动合成多模态轨迹、生成分步偏好对，并训练一个VLM控制器以实现稳健的工具使用推理。我们的流程首先构建了M-TRACE，这是一个包含28.5万个多模态任务和177万个验证轨迹的大规模数据集，从而实现基于模仿的轨迹调优。在此基础上，我们开发了MATRIX Agent，这是在M-TRACE上微调的控制器，用于分步工具推理。为了实现更精细的对齐，我们进一步引入了Pref-X，这是一个包含11,000个自动生成功率对的集合，并通过分步偏好学习在Pref-X上优化MATRIX。在三个基准测试（Agent-X、GTA和GAIA）中，MATRIX始终优于开源和闭源的VLMs，展示了可扩展且有效的多模态工具使用能力。我们的数据和代码可在<a target="_blank" rel="noopener" href="https://github.com/mbzuai-oryx/MATRIX%E8%8E%B7%E5%8F%96%E3%80%82">https://github.com/mbzuai-oryx/MATRIX获取。</a></p>
<h3 id="StarBench-A-Turn-Based-RPG-Benchmark-for-Agentic-Multimodal-Decision-Making-and-Information-Seeking"><a href="#StarBench-A-Turn-Based-RPG-Benchmark-for-Agentic-Multimodal-Decision-Making-and-Information-Seeking" class="headerlink" title="StarBench: A Turn-Based RPG Benchmark for Agentic Multimodal Decision-Making and Information Seeking"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18483v1">StarBench: A Turn-Based RPG Benchmark for Agentic Multimodal Decision-Making and Information Seeking</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>人类玩家所做的不仅仅是按按钮：他们将屏幕上的视觉信息转化为精确的键盘鼠标操作，并且在遇到困难时会先寻求信息再尝试重新操作。我们问当前的视觉-语言模型（VLM）是否能做到这一点。尽管在简化控制或工具辅助框架下取得了令人鼓舞的结果，但将人类般的操作应用于真实客户端——即将原始屏幕截图映射为时间上连贯的底层操作，同时决定何时寻求指导——仍然是一个开放的挑战。我们引入了StarBench，这是一个基于《崩坏：星穹铁道》的回合制RPG基准测试，旨在评估这两种人类般的能力：从像素到操作的多模态决策能力，以及主动的信息获取能力。StarBench在八个战斗任务和两个具有共享任务和指标的模式下标准化评估：（i）直接控制模式，其中智能体仅接收屏幕截图，并必须输出底层操作（点击和按键），且没有任何语义提示；（ii）工具辅助控制模式，其中可以通过检测器将更高层次的意图映射为底层操作，OCR输出提供可选的文本化观察信息以辅助用户界面的定位。为了模拟人类的实践，StarBench还包含了一个“询问或行动”诊断机制，用于衡量智能体在继续操作前是否以及何时选择请求简短的指导，以及这一选择如何影响后续的表现。我们报告了当前VLM的参考基线和人类参考结果。结果揭示了在直接控制模式下感知到控制的显著差距，同时表明谨慎的信息获取与成功度的提高相关，从而确立了StarBench作为评估主动信息获取和多模态决策能力的可复现基准。</p>
<h3 id="ImageGem-In-the-wild-Generative-Image-Interaction-Dataset-for-Generative-Model-Personalization"><a href="#ImageGem-In-the-wild-Generative-Image-Interaction-Dataset-for-Generative-Model-Personalization" class="headerlink" title="ImageGem: In-the-wild Generative Image Interaction Dataset for Generative Model Personalization"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18433v1">ImageGem: In-the-wild Generative Image Interaction Dataset for Generative Model Personalization</a></h3><p><strong>Categories:</strong> Vision-Language Models, Large Language Models</p>
<p>我们引入了ImageGem数据集，这是一个用于研究能够理解细粒度个体偏好的生成模型的数据集。我们认为，阻碍此类生成模型发展的关键挑战之一是缺乏真实场景中细粒度的用户偏好标注。我们的数据集包含来自57,000名用户的现实世界交互数据，这些用户共同创建了242,000个定制的LoRAs，撰写了300万个文本提示，并生成了500万张图像。借助我们数据集中的用户偏好标注，我们成功训练了更优的偏好对齐模型。此外，利用个体用户的偏好，我们研究了检索模型和视觉-语言模型在个性化图像检索和生成模型推荐中的表现。最后，我们提出了一种端到端框架，用于在潜在权重空间中编辑定制的扩散模型，以与个体用户偏好对齐。我们的实验结果表明，ImageGem数据集首次实现了生成模型个性化的新范式。</p>
<h3 id="Automated-urban-waterlogging-assessment-and-early-warning-through-a-mixture-of-foundation-models"><a href="#Automated-urban-waterlogging-assessment-and-early-warning-through-a-mixture-of-foundation-models" class="headerlink" title="Automated urban waterlogging assessment and early warning through a mixture of foundation models"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18425v1">Automated urban waterlogging assessment and early warning through a mixture of foundation models</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>随着气候变化的加剧，城市内涝对全球公共安全和基础设施构成了日益严重的威胁。然而，现有的监测方法严重依赖人工报告，无法提供及时且全面的评估。在本研究中，我们提出了“城市内涝评估”（Urban Waterlogging Assessment，简称UWAssess），这是一种基于基础模型的框架，能够自动识别监控图像中的内涝区域，并生成结构化的评估报告。为了解决标注数据稀缺的问题，我们设计了一种半监督微调策略和一种“思维链”（Chain-of-thought，简称CoT）提示策略，以释放基础模型在数据匮乏的下游任务中的潜力。在具有挑战性的视觉基准测试中，评估结果表明感知性能有了显著提升。基于GPT的评估进一步证实了UWAssess生成可靠文本报告的能力，这些报告能够准确描述内涝的范围、深度、风险和影响。这种双重能力使内涝监测从感知向生成转变，而多个基础模型的协作框架为构建智能化、可扩展的系统奠定了基础，支持城市治理、灾害响应和气候韧性建设。</p>
<h3 id="Med-VRAgent-A-Framework-for-Medical-Visual-Reasoning-Enhanced-Agents"><a href="#Med-VRAgent-A-Framework-for-Medical-Visual-Reasoning-Enhanced-Agents" class="headerlink" title="Med-VRAgent: A Framework for Medical Visual Reasoning-Enhanced Agents"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18424v1">Med-VRAgent: A Framework for Medical Visual Reasoning-Enhanced Agents</a></h3><p><strong>Categories:</strong> Vision-Language Models,Reinforcement Learning</p>
<p>视觉语言模型（VLMs）在医学推理方面取得了有希望的成果，但在幻觉、描述模糊、逻辑不一致和定位不准确等方面存在困难。为了解决这些问题，我们提出了一种名为医学视觉推理代理（Medical Visual Reasoning Agent，简称Med-VRAgent）的代理框架。该方法基于视觉引导和自我奖励范式以及蒙特卡洛树搜索（MCTS）。通过将视觉引导与树搜索相结合，Med-VRAgent 提升了 VLMs 的医学视觉推理能力。我们利用 Med-VRAgent 收集的轨迹作为反馈，通过使用近端策略优化（PPO）目标对 VLMs 进行微调，以进一步提升其性能。在多个医学视觉问答（VQA）基准数据集上的实验表明，我们的方法在现有方法之上表现更优。</p>
<h3 id="Visible-Yet-Unreadable-A-Systematic-Blind-Spot-of-Vision-Language-Models-Across-Writing-Systems"><a href="#Visible-Yet-Unreadable-A-Systematic-Blind-Spot-of-Vision-Language-Models-Across-Writing-Systems" class="headerlink" title="Visible Yet Unreadable: A Systematic Blind Spot of Vision Language Models Across Writing Systems"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2509.06996v4">Visible Yet Unreadable: A Systematic Blind Spot of Vision Language Models Across Writing Systems</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>写作是一种普遍的文化技术，它利用视觉进行符号交流。人类展现出惊人的韧性：即使字符被碎片化、融合或部分遮挡，我们仍然能够轻松识别单词。本文探讨先进的视觉语言模型（VLMs）是否也具有这种韧性。我们通过拼接、重组和叠加字符，构建了两个受心理物理学启发的基准测试，分别应用于中文表意文字和英文字母词，从而生成对模型来说“可见但不可读”，但对人类来说仍可辨识的刺激。尽管在干净文本上表现优异，但当前的VLMs在这些扰动下表现出严重性能下降，经常产生无关或不连贯的输出。这种模式表明存在结构性限制：模型严重依赖于通用的视觉不变性，但对用于稳健识字所需的组合先验知识依赖不足。我们发布了刺激生成代码、提示和评估协议，以促进透明的复现和后续研究。我们的发现激励了能够编码跨书写系统的符号分割、组合和绑定的架构和训练策略，并明确了在教育、无障碍、文化遗产和安全领域部署多模态系统的具体挑战。</p>
<h3 id="StreamingTOM-Streaming-Token-Compression-for-Efficient-Video-Understanding"><a href="#StreamingTOM-Streaming-Token-Compression-for-Efficient-Video-Understanding" class="headerlink" title="StreamingTOM: Streaming Token Compression for Efficient Video Understanding"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18269v1">StreamingTOM: Streaming Token Compression for Efficient Video Understanding</a></h3><p><strong>Categories:</strong> Vision-Language Models, Large Language Models</p>
<p>与离线处理不同，流式视频视觉-语言模型面临两个基本限制：因果性与累积性。因果性限制了模型无法访问未来帧，而离线方法可以利用这一点；累积性则导致令牌数量无限制增长，从而造成效率瓶颈。然而，现有方法仅对后语言模型（LLM）的KV缓存进行管理，而未对前语言模型（LLM）的预填充阶段进行优化。我们引入了StreamingTOM，这是一个无需训练、即插即用的两阶段框架，能够以可预测的延迟解决前LLM和后LLM阶段的瓶颈问题。</p>
<p>因果时间缩减（Causal Temporal Reduction）对每帧施加固定的预算限制，并根据相邻帧的变化和令牌显著性选择令牌，从而大幅降低每帧的预填充成本。该方法仅处理每帧中一小部分视觉令牌，而非所有视觉令牌。在线量化内存（Online Quantized Memory）以4位格式存储令牌，按需检索相关组并进行反量化，无论流的长度如何，都能保持活跃的KV缓存有限。</p>
<p>实验表明，我们的方法相比现有最先进的方法（SOTA）实现了15.7倍的KV缓存压缩、1.2倍的峰值内存降低以及2倍的TTFT（首次令牌延迟）。在离线基准测试中，StreamingTOM的平均准确率为63.8%，在RVS测试中为55.8%&#x2F;3.7，同时保持了训练无关方法的最先进水平。这些结果突显了我们的两阶段方法在实现具有增长限制的高效流式视频理解方面的实际优势。</p>
<h3 id="VisuRiddles-Fine-grained-Perception-is-a-Primary-Bottleneck-for-Multimodal-Large-Language-Models-in-Abstract-Visual-Reasoning"><a href="#VisuRiddles-Fine-grained-Perception-is-a-Primary-Bottleneck-for-Multimodal-Large-Language-Models-in-Abstract-Visual-Reasoning" class="headerlink" title="VisuRiddles: Fine-grained Perception is a Primary Bottleneck for Multimodal Large Language Models in Abstract Visual Reasoning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2506.02537v3">VisuRiddles: Fine-grained Perception is a Primary Bottleneck for Multimodal Large Language Models in Abstract Visual Reasoning</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>近年来，多模态大语言模型（MLLMs）在许多推理任务中的性能取得了显著进展。然而，抽象视觉推理（Abstract Visual Reasoning，AVR）仍然是一个关键挑战，主要由于其在感知抽象图形方面的局限性。为了解决这一问题，我们研究了当前MLLMs中的瓶颈，并合成训练数据以增强其对抽象视觉信息的感知能力。首先，我们提出了VisuRiddles，这是一个用于AVR的基准测试，包含精心设计的任务，以评估模型在五个核心维度和两个高级推理类别中的推理能力。其次，我们引入了感知谜题合成器（Perceptual Riddle Synthesizer，PRS），这是一种自动框架，用于生成带有细粒度感知描述的谜题。PRS不仅能够为抽象图形生成有价值的训练数据，还提供了细粒度的感知描述，从而对中间推理阶段进行监督，进而提升训练效果和模型的可解释性。我们在VisuRiddles上的大量实验结果验证了细粒度的视觉感知是主要瓶颈，而我们的合成框架显著提升了当前MLLMs在这些挑战性任务上的性能。我们的代码和数据集将在<a target="_blank" rel="noopener" href="https://github.com/yh-hust/VisuRiddles%E4%B8%8A%E5%8F%91%E5%B8%83%E3%80%82">https://github.com/yh-hust/VisuRiddles上发布。</a></p>
<h3 id="LegiScout-A-Visual-Tool-for-Understanding-Complex-Legislation"><a href="#LegiScout-A-Visual-Tool-for-Understanding-Complex-Legislation" class="headerlink" title="LegiScout: A Visual Tool for Understanding Complex Legislation"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.01195v2">LegiScout: A Visual Tool for Understanding Complex Legislation</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>现代立法框架，如《平价医疗法案》（ACA），通常涉及复杂的机构网络、指令和相互依赖关系。政府发布的图表试图描绘这些结构，但这些图表往往是静态的、密集的，且难以理解，即使对专家来说也是如此。我们推出了LegiScout，这是一种交互式可视化系统，它将静态的政策图表转换为动态的力导向图，从而在保持关键关系的同时增强理解。通过整合数据提取、自然语言处理和计算机视觉技术，LegiScout不仅支持对ACA的深入探索，还支持对各种立法和监管框架的广泛研究。我们的方法使利益相关者——包括政策制定者、分析师和公众——能够更好地导航和理解现代法律中固有的复杂性。</p>
<h3 id="VLSU-Mapping-the-Limits-of-Joint-Multimodal-Understanding-for-AI-Safety"><a href="#VLSU-Mapping-the-Limits-of-Joint-Multimodal-Understanding-for-AI-Safety" class="headerlink" title="VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18214v1">VLSU: Mapping the Limits of Joint Multimodal Understanding for AI Safety</a></h3><p><strong>Categories:</strong> Vision-Language Models, Large Language Models</p>
<p>多模态基础模型的安全评估通常将视觉和语言输入分别处理，忽略了在联合解释中良性内容可能组合成有害内容所带来的风险。现有的方法也难以明确区分不安全内容与边缘案例，导致对真正有害内容的过度屏蔽或拒绝不足。我们提出了视觉语言安全理解（Vision Language Safety Understanding, VLSU），这是一个全面的框架，通过细粒度严重程度分类和跨17种安全模式的组合分析，系统评估多模态安全。我们采用多阶段流程，结合现实世界的图像和人工标注，构建了一个包含8,187个样本、涵盖15类危害的大型基准数据集。对11个最先进的模型的评估揭示了系统性的联合理解失败：虽然模型在清晰的单模态安全信号上能达到90%以上的准确率，但当需要联合图像-文本推理以确定安全标签时，其性能显著下降至20%-55%。最关键的是，联合图像-文本安全分类中34%的错误发生在单个模态分类正确的前提下，进一步证明了模型缺乏组合推理能力。此外，我们发现模型在拒绝不安全内容的同时，难以平衡对值得参与的边缘案例的响应。例如，我们发现指令框架的使用可以将Gemini-1.5在边缘内容上的过度屏蔽率从62.4%降至10.4%，但这以对不安全内容拒绝率的下降为代价（从90.8%降至53.9%）。总体而言，我们的框架揭示了联合图像-文本理解中的不足以及当前模型中的对齐差距，并为推动稳健视觉-语言安全研究的下一步里程碑提供了关键的测试平台。</p>
<h3 id="LLM-RG-Referential-Grounding-in-Outdoor-Scenarios-using-Large-Language-Models"><a href="#LLM-RG-Referential-Grounding-in-Outdoor-Scenarios-using-Large-Language-Models" class="headerlink" title="LLM-RG: Referential Grounding in Outdoor Scenarios using Large Language Models"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2509.25528v2">LLM-RG: Referential Grounding in Outdoor Scenarios using Large Language Models</a></h3><p><strong>Categories:</strong> Vision-Language Models, Large Language Models</p>
<p>在户外驾驶场景中实现指称锚定（referential grounding）具有挑战性，原因是场景变化大、视觉上相似的物体众多，以及动态元素使得解析自然语言指称（例如“右边的黑色汽车”）变得复杂。我们提出了LLM-RG，这是一种混合型流程，结合了现成的视觉-语言模型（VLM）进行细粒度属性提取，以及大型语言模型（LLM）进行符号推理。LLM-RG通过使用LLM来提取相关物体类型和属性，检测候选区域，借助VLM生成丰富的视觉描述符，然后将这些描述符与空间元数据结合，生成自然语言提示，输入到LLM中进行链式推理，以识别所指对象的边界框。在Talk2Car基准数据集上的评估表明，LLM-RG在基于LLM和VLM的基线方法上取得了显著提升。此外，我们的消融实验表明，加入三维空间线索进一步提高了锚定效果。我们的结果展示了VLM和LLM在零样本（zero-shot）方式下互补优势，用于实现稳健的户外指称锚定。</p>
<h3 id="NEBULA-Do-We-Evaluate-Vision-Language-Action-Agents-Correctly"><a href="#NEBULA-Do-We-Evaluate-Vision-Language-Action-Agents-Correctly" class="headerlink" title="NEBULA: Do We Evaluate Vision-Language-Action Agents Correctly?"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.16263v2">NEBULA: Do We Evaluate Vision-Language-Action Agents Correctly?</a></h3><p><strong>Categories:</strong> Vision-Language Models, Robotics</p>
<p>视觉-语言-动作（VLA）代理的评估受到粗略的、以最终任务成功为指标的评估方式的阻碍，这种指标无法提供精确的技能诊断或衡量其对现实世界扰动的鲁棒性。这一挑战因数据景观的碎片化而加剧，阻碍了可重复研究和通用模型的开发。为了解决这些限制，我们引入了NEBULA，这是一个用于单臂操作的统一生态系统，能够实现诊断性和可重复的评估。NEBULA具有创新的双轴评估协议，将精细粒度的能力测试用于精确的技能诊断，并结合系统性的压力测试来衡量鲁棒性。我们提供了标准化的API和大规模聚合数据集，以减少数据碎片化，并支持跨数据集训练和公平比较。通过NEBULA，我们证明了表现最好的VLA在关键能力（如空间推理和动态适应）方面存在困难，而这些能力常被传统的最终任务成功指标所掩盖。通过同时衡量代理能做什么以及在何时能可靠地执行，NEBULA为稳健且通用的具身代理提供了实用的基础。</p>
<h3 id="RadDiagSeg-M-A-Vision-Language-Model-for-Joint-Diagnosis-and-Multi-Target-Segmentation-in-Radiology"><a href="#RadDiagSeg-M-A-Vision-Language-Model-for-Joint-Diagnosis-and-Multi-Target-Segmentation-in-Radiology" class="headerlink" title="RadDiagSeg-M: A Vision Language Model for Joint Diagnosis and Multi-Target Segmentation in Radiology"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18188v1">RadDiagSeg-M: A Vision Language Model for Joint Diagnosis and Multi-Target Segmentation in Radiology</a></h3><p><strong>Categories:</strong> Vision-Language Models</p>
<p>目前大多数医疗视觉语言模型在面对复杂视觉问题时，难以同时生成诊断文本和像素级分割掩码。这在临床应用中是一个重大限制，因为无法同时提供这两种模态的辅助系统对医疗从业者的价值有限。为了解决这一限制，我们首先引入了一个名为RadDiagSeg-D的数据集，将异常检测、诊断和多目标分割统一并分层地整合为一个任务。RadDiagSeg-D涵盖了多种成像模态，并经过精确设计，支持开发能够同时生成描述性文本和相应分割掩码的模型。随后，我们利用该数据集提出了一种新的视觉语言模型RadDiagSeg-M，该模型能够实现联合异常检测、诊断和灵活分割。RadDiagSeg-M提供了高度信息丰富且具有临床价值的输出，有效满足了辅助诊断中丰富上下文信息的需求。最后，我们对RadDiagSeg-M进行了基准测试，并展示了其在多目标文本和掩码生成任务中所有相关组件上的强大性能，建立了稳健且具有竞争力的基线。</p>
<h2 id="World-Models"><a href="#World-Models" class="headerlink" title="World Models"></a>World Models</h2><h3 id="CaMiT-A-Time-Aware-Car-Model-Dataset-for-Classification-and-Generation"><a href="#CaMiT-A-Time-Aware-Car-Model-Dataset-for-Classification-and-Generation" class="headerlink" title="CaMiT: A Time-Aware Car Model Dataset for Classification and Generation"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.17626v2">CaMiT: A Time-Aware Car Model Dataset for Classification and Generation</a></h3><p><strong>Categories:</strong> World Models</p>
<p>人工智能系统必须适应不断变化的视觉环境，特别是在对象外观随时间变化的领域。我们引入了“时间中的汽车模型”（CaMiT）数据集，该数据集详细记录了汽车模型随时间的演变过程，这是技术制品的一个代表性类别。CaMiT包含190个汽车模型（2007-2023年）的787,000个标注样本和510万个未标注样本（2005-2023年），支持监督学习和自监督学习。在领域内数据上进行静态预训练可以实现与大规模通用模型相当的性能，同时更节省资源；然而，当模型跨年测试时，准确率会下降。为了解决这一问题，我们提出了一种时间增量分类设置，这是一种现实的持续学习场景，包含新兴、演变和消失的类别。我们评估了两种策略：时间增量预训练，即更新主干网络；以及时间增量分类器学习，即仅更新最后一层，这两种方法均提升了时间上的鲁棒性。最后，我们探索了时间感知的图像生成方法，在训练过程中利用时间元数据，从而产生更逼真的输出。CaMiT为研究细粒度视觉识别和生成中的时间适应性提供了丰富的基准数据集。</p>
<h3 id="SOCIA-Joint-Structure-Parameter-Co-Optimization-for-Automated-Simulator-Construction"><a href="#SOCIA-Joint-Structure-Parameter-Co-Optimization-for-Automated-Simulator-Construction" class="headerlink" title="SOCIA: Joint Structure-Parameter Co-Optimization for Automated Simulator Construction"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.12006v3">SOCIA: Joint Structure-Parameter Co-Optimization for Automated Simulator Construction</a></h3><p><strong>Categories:</strong> World Models</p>
<p>从数据构建可信的模拟器具有挑战性，因为结构设计、参数校准和分布外（OOD）鲁棒性紧密耦合。我们引入SOCIA（面向计算智能代理的模拟协调框架），该框架将模拟器构建视为结构-参数联合优化问题：它提取机制丰富的蓝图，暴露显式的可调参数，并实例化校准方案，从而生成一个内置校准钩子的可执行模拟器。SOCIA结合了贝叶斯优化用于样本高效的点校准，并结合基于模拟的推理以实现对不确定性的感知拟合；诊断机制会触发外部优化循环中的定向结构修改，从而在预算受限的情况下联合优化设计与参数。在三个不同的任务中，SOCIA始终优于强大的基线方法，在分布内（ID）拟合和分布外（OOD）偏移方面表现尤为出色。削弱结构、校准设计或调参的消融实验导致性能近似单调下降，突显了统一结构-参数优化的必要性。我们将很快发布代码。</p>
<h3 id="Regression-is-all-you-need-for-medical-image-translation"><a href="#Regression-is-all-you-need-for-medical-image-translation" class="headerlink" title="Regression is all you need for medical image translation"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.02048v3">Regression is all you need for medical image translation</a></h3><p><strong>Categories:</strong> World Models</p>
<p>尽管生成对抗网络（GANs）和扩散模型（DMs）在自然图像合成方面取得了显著成果，但它们的核心优势——创造力和真实感——在医疗应用中可能成为不利因素，因为在医疗领域，准确性和保真度至关重要。这些模型可能会引入幻觉并复制不需要的采集噪声。为此，我们提出了一种基于2.5D扩散的医学图像翻译（MIT）框架YODA（You Only Denoise once - or Average，只需一次去噪——或平均）。与扩散模型理论一致，我们发现传统的扩散采样会随机复制噪声。为了解决这一问题，我们通过绘制并平均多个样本，类似于物理信号平均。由于这种方法有效逼近了扩散模型的期望值，我们将这种采样方法称为期望近似（ExpA）采样。此外，我们还提出了回归采样方法YODA，该方法保留了初始的扩散模型预测，并省略了迭代优化步骤，从而在单一步骤中生成无噪声图像。在五个不同的多模态数据集上（包括多对比度脑部MRI和盆腔MRI-CT），我们证明回归采样不仅显著提高了效率，而且即使结合ExpA，其图像质量也与完整扩散采样相当甚至更优。我们的实验结果表明，迭代优化仅能提升感知真实感，而不会提升信息翻译效果，我们在相关下游任务中对此进行了验证。YODA在八种最先进的扩散模型和GANs上表现更优，并对扩散模型和GANs在计算成本低廉的回归模型上被认为具有优势的高质量MIT领域提出了挑战。此外，我们还表明，YODA翻译生成的图像在多个医疗应用中可以与物理采集图像互换，甚至更优。</p>
<h3 id="Application-of-Reduced-Order-Models-for-Temporal-Multiscale-Representations-in-the-Prediction-of-Dynamical-Systems"><a href="#Application-of-Reduced-Order-Models-for-Temporal-Multiscale-Representations-in-the-Prediction-of-Dynamical-Systems" class="headerlink" title="Application of Reduced-Order Models for Temporal Multiscale Representations in the Prediction of Dynamical Systems"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18925v1">Application of Reduced-Order Models for Temporal Multiscale Representations in the Prediction of Dynamical Systems</a></h3><p><strong>Categories:</strong> World Models</p>
<p>对复杂多尺度系统的动态建模和预测仍然是一个重大挑战，原因在于其内在的非线性特征和对初始条件的敏感性，以及传统机器学习方法在捕捉高频行为方面的局限性。为克服这些困难，我们提出了三种多尺度学习方法。第一种方法利用统一划分（Partition of Unity, PU）方法结合神经网络，将动态分解为局部成分，并直接预测宏观和微观尺度的行为。第二种方法采用奇异值分解（Singular Value Decomposition, SVD）提取主导模式，以明确区分宏观和微观尺度的动力学行为。由于在实际应用中通常难以获得完整的数据矩阵，我们进一步采用稀疏高阶SVD，从有限的测量数据中重构多尺度动态。这些方法共同确保了粗粒度和细粒度动态都能被准确捕捉，从而使该框架在涉及复杂多尺度现象的现实应用中具有有效性，并能够适应具有不完全观测的高维系统，通过在所研究现象中所有时间尺度上提供近似和解释，实现这一目标。</p>
<h3 id="Earth-AI-Unlocking-Geospatial-Insights-with-Foundation-Models-and-Cross-Modal-Reasoning"><a href="#Earth-AI-Unlocking-Geospatial-Insights-with-Foundation-Models-and-Cross-Modal-Reasoning" class="headerlink" title="Earth AI: Unlocking Geospatial Insights with Foundation Models and Cross-Modal Reasoning"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18318v1">Earth AI: Unlocking Geospatial Insights with Foundation Models and Cross-Modal Reasoning</a></h3><p><strong>Categories:</strong> World Models</p>
<p>地理空间数据为理解我们的星球提供了巨大的潜力。然而，这种数据的庞大体量和多样性，以及其不同的分辨率、时间尺度和稀疏性，给深入分析和解释带来了重大挑战。本文介绍了Earth AI，这是一套地理空间人工智能模型和代理推理方法，能够显著提升我们获取关于星球的新颖而深刻洞察的能力。该方法建立在三个关键领域——全球尺度的影像、人口和环境——的基础模型之上，并结合了智能的Gemini驱动推理引擎。我们展示了严谨的基准测试，突显了我们基础模型的强大功能和新颖能力，并验证了当这些模型协同使用时，它们在地理空间推理中提供了互补的价值，其协同效应能够解锁更优越的预测能力。为了处理复杂且多步骤的查询，我们开发了一个Gemini驱动的代理，它能够联合推理多个基础模型以及大规模的地理空间数据源和工具。在新的现实危机场景基准测试中，我们的代理展示了其提供关键且及时洞察的能力，有效弥合了原始地理空间数据与可操作理解之间的鸿沟。</p>
<h3 id="Higher-Embedding-Dimension-Creates-a-Stronger-World-Model-for-a-Simple-Sorting-Task"><a href="#Higher-Embedding-Dimension-Creates-a-Stronger-World-Model-for-a-Simple-Sorting-Task" class="headerlink" title="Higher Embedding Dimension Creates a Stronger World Model for a Simple Sorting Task"></a><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18315v1">Higher Embedding Dimension Creates a Stronger World Model for a Simple Sorting Task</a></h3><p><strong>Categories:</strong> World Models,Reinforcement Learning</p>
<p>我们研究了嵌入维度对通过强化学习训练的变压器模型中内部“世界模型”形成的影响，这些模型被用于执行类似冒泡排序的相邻交换操作。模型即使在非常小的嵌入维度下也能实现高准确率，但更大的嵌入维度会产生更忠实、一致且鲁棒的内部表示。特别是，更高的嵌入维度有助于结构化内部表示的形成，从而提高模型的可解释性。经过数百次实验，我们观察到两种一致的机制：(1) 注意力权重矩阵的最后一行单调地编码了标记的全局顺序；(2) 所选的交换操作与这些编码值的最大相邻差异相一致。我们的结果提供了定量证据，表明变压器构建了结构化的内部世界模型，并且模型规模的增加不仅提升了表示质量，也提升了最终性能。我们发布了相关指标和分析结果，可用于研究类似的算法任务。</p>
<h2 id="Rejected-Papers"><a href="#Rejected-Papers" class="headerlink" title="Rejected Papers"></a>Rejected Papers</h2><ul>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19127v1">Steering Autoregressive Music Generation with Recursive Feature Machines</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2510.19118v1">A Novel Approach to Breast Cancer Segmentation using U-Net Model with Attention Mechanisms and FedProx</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2411.13022v3">Fast MRI for All: Bridging Access Gaps by Training without Raw Data</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19033v1">“Over-the-Hood” AI Inclusivity Bugs and How 3 AI Product Teams Found and Fixed Them</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19008v1">Plural Voices, Single Agent: Towards Inclusive AI in Multi-User Domestic Spaces</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.19003v1">$Δ$t-Mamba3D: A Time-Aware Spatio-Temporal State-Space Model for Breast Cancer Risk Prediction</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.20222v1">QKCV Attention: Enhancing Time Series Forecasting with Static Categorical Embeddings for Both Lightweight and Pre-trained Foundation Models</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18938v1">StutterZero and StutterFormer: End-to-End Speech Conversion for Stuttering Transcription and Correction</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2508.17094v3">PowerChain: A Verifiable Agentic AI System for Automating Distribution Grid Analyses</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.16028v2">Nondeterminism-Aware Optimistic Verification for Floating-Point Neural Networks</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18819v1">An Explainable Hybrid AI Framework for Enhanced Tuberculosis and Symptom Detection</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2505.03835v2">The Shift Towards Preprints in AI Policy Research: A Comparative Study of Preprint Trends in the U.S., Europe, and South Korea</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18803v1">Decoding Funded Research: Comparative Analysis of Topic Models and Uncovering the Effect of Gender and Geographic Location</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18802v1">Computational Foundations for Strategic Coopetition: Formalizing Interdependence and Complementarity</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2509.20409v2">A Unified Formal Theory on the Logical Limits of Symbol Grounding</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18719v1">Causally Perturbed Fairness Testing</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2408.00166v2">Review of Explainable Graph-Based Recommender Systems</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2406.04082v2">Discovering the curriculum with AI: A proof-of-concept demonstration with an intelligent tutoring system for teaching project selection</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18699v1">Fetch.ai: An Architecture for Modern Multi-Agent Systems</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2306.07032v2">Mitigating Prior Errors in Causal Structure Learning: A Resilient Approach via Bayesian Networks</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18650v1">Binary Quadratic Quantization: Beyond First-Order Quantization for Real-Valued Matrix Compression</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2501.01166v2">Deep Learning in Palmprint Recognition-A Comprehensive Survey</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18637v1">ε-Seg: Sparsely Supervised Semantic Segmentation of Microscopy Data</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18636v1">C-SWAP: Explainability-Aware Structured Pruning for Efficient Neural Networks Compression</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18631v1">Comparative Expressivity for Structured Argumentation Frameworks with Uncertain Rules and Premises</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18628v1">Leveraging Association Rules for Better Predictions and Better Explanations</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18615v1">A Rectification-Based Approach for Distilling Boosted Trees into Decision Trees</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2408.10111v3">LENS: Large Pre-trained Transformer for Exploring Financial Time Series Regularities</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18581v1">The Cost-Benefit of Interdisciplinarity in AI for Mental Health</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18573v1">Kaleido: Open-Sourced Multi-Subject Reference Video Generation Model</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18569v1">QuantEvolve: Automating Quantitative Strategy Discovery through Multi-Agent Evolutionary Framework</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2506.07578v3">Denoising the Future: Top-p Distributions for Moving Through Time</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18559v1">RAISE: A Unified Framework for Responsible AI Scoring and Evaluation</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2507.22539v2">A surrogate model for topology optimisation of elastic structures via parametric autoencoders</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2404.18134v3">Learning Fairer Representations with FairVIC</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18535v1">Physics-guided Emulators Reveal Resilience and Fragility under Operational Latencies and Outages</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/abs/2504.18400v4">A Multimodal Deep Learning Approach for White Matter Shape Prediction in Diffusion MRI Tractography</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18473v1">Benchmarking Fairness-aware Graph Neural Networks in Knowledge Graphs</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.15985v2">MEET-Sepsis: Multi-Endogenous-View Enhanced Time-Series Representation Learning for Early Sepsis Prediction</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.13865v3">Deep Edge Filter: Return of the Human-Crafted Layer in Deep Learning</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.16601v2">Uncertain Knowledge Graph Completion via Semi-Supervised Confidence Distribution Learning</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18429v1">Optimistic Higher-Order Superposition</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2506.10821v5">Think With Videos For Agentic Long-Video Understanding</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2506.06387v2">Model-based Implicit Neural Representation for sub-wavelength Radio Localization</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18406v1">Learning from N-Tuple Data with M Positive Instances: Unbiased Risk Estimation and Theoretical Guarantees</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18405v1">Automated Wicket-Taking Delivery Segmentation and Weakness Detection in Cricket Videos Using OCR-Guided YOLOv8 and Trajectory Modeling</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2503.12211v3">Changing Base Without Losing Pace: A GPU-Efficient Alternative to MatMul in DNNs</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2506.19461v2">Iterative Quantum Feature Maps</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18381v1">S2AP: Score-space Sharpness Minimization for Adversarial Pruning</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18342v1">ShortcutBreaker: Low-Rank Noisy Bottleneck with Global Perturbation Attention for Multi-Class Unsupervised Anomaly Detection</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18328v1">Scalable, Explainable and Provably Robust Anomaly Detection with One-Step Flow Matching</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.13887v2">Incomplete Multi-view Clustering via Hierarchical Semantic Alignment and Cooperative Completion</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2507.15886v4">Combining Cost-Constrained Runtime Monitors for AI Safety</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2503.21322v3">HyperGraphRAG: Retrieval-Augmented Generation via Hypergraph-Structured Knowledge Representation</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18911v1">Prospects for Using Artificial Intelligence to Understand Intrinsic Kinetics of Heterogeneous Catalytic Reactions</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2509.04622v4">Measuring the Measures: Discriminative Capacity of Representational Similarity Metrics Across Model Families</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18910v1">Large Connectome Model: An fMRI Foundation Model of Brain Connectomes Empowered by Brain-Environment Interaction in Multitask Learning Landscape</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18267v1">Latent-Info and Low-Dimensional Learning for Human Mesh Recovery and Parallel Optimization</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18266v1">SPIKE: Stable Physics-Informed Kernel Evolution Method for Solving Hyperbolic Conservation Laws</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18259v1">Learning under Quantization for High-Dimensional Linear Regression</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18256v1">Hyperbolic Space Learning Method Leveraging Temporal Motion Priors for Human Mesh Recovery</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18252v1">Finding the Sweet Spot: Optimal Data Augmentation Ratio for Imbalanced Credit Scoring Using ADASYN</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.15301v3">Latent Diffusion Model without Variational Autoencoder</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.16034v2">Disaster Management in the Era of Agentic AI Systems: A Vision for Collective Human-Machine Intelligence for Augmented Resilience</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.17753v2">Human-AI Interactions: Cognitive, Behavioral, and Emotional Impacts</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2509.00398v3">A Study on the Framework for Evaluating the Ethics and Trustworthiness of Generative AI</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18212v1">A Definition of AGI</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2506.22498v4">ViFusionTST: Deep Fusion of Time-Series Image Representations from Load Signals for Early Bed-Exit Prediction</a></li>
<li><a target="_blank" rel="noopener" href="http://arxiv.org/pdf/2510.18187v1">VelocityNet: Real-Time Crowd Anomaly Detection via Person-Specific Velocity Analysis</a></li>
</ul>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
<div class="share-btn">
      <span class="share-sns share-outer">
        <i class="ri-share-forward-line"></i>
        分享
      </span>
      <div class="share-wrap">
        <i class="arrow"></i>
        <div class="share-icons">
          
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="ri-weibo-fill"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="ri-wechat-fill"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="ri-qq-fill"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="ri-douban-line"></i>
          </a>
          <!-- <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a> -->
          
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="ri-facebook-circle-fill"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="ri-twitter-fill"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="ri-google-fill"></i>
          </a>
        </div>
      </div>
</div>

<div class="wx-share-modal">
    <a class="modal-close" href="javascript:;"><i class="ri-close-circle-line"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="//api.qrserver.com/v1/create-qr-code/?size=150x150&data=https://fan-jj24.github.io/2025/10/25/2025-10-21/" alt="微信分享二维码">
    </div>
</div>

<div id="share-mask"></div>  
    </footer>
  </div>

   
  
   
    
    <script src="https://cdn.staticfile.org/twikoo/1.4.18/twikoo.all.min.js"></script>
    <div id="twikoo" class="twikoo"></div>
    <script>
        twikoo.init({
            envId: ""
        })
    </script>
 
</article>

</section>
      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2025
        <i class="ri-heart-fill heart_icon"></i> Pop Fan
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>Visitors:<span id="busuanzi_value_site_uv"></span></span>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>Views:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
      </li>
    </ul>
  </div>
</footer>    
    </main>
    <div class="float_btns">
      <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

    </div>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="Instant Papers"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="Search">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-3.6.0.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->
 
<script src="/js/tocbot.min.js"></script>

<script>
  tocbot.init({
    tocSelector: ".tocbot",
    contentSelector: ".article-entry",
    headingSelector: "h1, h2, h3, h4, h5, h6",
    hasInnerContainers: true,
    scrollSmooth: true,
    scrollContainer: "main",
    positionFixedSelector: ".tocbot",
    positionFixedClass: "is-position-fixed",
    fixedSidebarOffset: "auto",
  });
</script>

<script src="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.staticfile.org/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/default-skin/default-skin.min.css">
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.js"></script>
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->

<!-- Katex -->

<!-- busuanzi  -->
 
<script src="/js/busuanzi-2.3.pure.min.js"></script>
 
<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.staticfile.org/clipboard.js/2.0.10/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->

<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
    

  </div>
</body>

</html>